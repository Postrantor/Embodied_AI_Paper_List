---
tip: translate by baidu@2024-08-12 12:55:55
---

# Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI

> Yang Liu, Weixing Chen, Yongjie Bai, Guanbin Li, Wen Gao, _Fellow, IEEE, Liang Lin, Fellow, IEEE_

_Abstract_---Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. The associated project can be found at [https://github.](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List) [com/HCPLab-SYSU/Embodied AI Paper List.](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)

> 嵌入式人工智能(Embedded AI)是实现通用人工智能(Artificial General Intelligence，AGI)的关键，也是连接网络空间和物理世界的各种应用的基础。最近，多模态大模型(MLM)和世界模型(WMs)的出现因其卓越的感知、交互和推理能力而引起了人们的广泛关注，使其成为具身智能体大脑的一种有前景的架构。然而，在传销时代，还没有对嵌入式 AI 进行全面的调查。
> 在本次调查中，我们全面探讨了物化人工智能的最新进展。
>
> - 我们的分析首先浏览了物化机器人和模拟器的代表性作品的前沿，以充分了解研究重点及其局限性。
> - 然后，我们分析了四个主要的研究目标：1)具身感知，2)具身交互，3)具身代理(agents)，4)模拟到真实的适应，涵盖了最先进的方法、基本范式和全面的数据集。
> - 此外，我们还探讨了虚拟和真实化身代理(agents)中 MLM 的复杂性，强调了它们在促进动态数字和物理环境中的交互方面的重要性。
> - 最后，我们总结了具身人工智能的挑战和局限性，并讨论了它们未来的潜在发展方向。我们希望这项调查能够为研究界提供基础参考，并激发持续创新。
>   相关项目可以在以下网址找到[https://github.](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)[com/HCPLab SYSU/人工智能论文列表](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)

**_Index Terms_---Embodied AI, Cyber Space, Physical World, Multi-modal Large Models, World Models, Agents, Robotics**

## I. INTRODUCTION

MBODIED AI was initially proposed from the Embod­ied Turing Test by Alan Turing in 1950 [[1]](#bookmark25) which is designed to determine whether agents can display intelligence that is not just limited to solving abstract problems in a virtual environment (cyber space[^1^](#_bookmark0)), but that is also capable of navigating the complexity and unpredictability of the physical world. The agents in the cyber space are generally referred to as disembodied AI, while those in the physical space are embodied AI (Table [I).](#_bookmark2) Recent advances in Multi-modal Large Models (MLMs) have injected strong perception, interaction and planning capabilities to embodied models, to develop general-purpose embodied agents and robots that actively interact with virtual and physical environments [[2]](#bookmark26) Therefore, the embodied agents are widely considered as the best carriers for MLMs. The recent representative embodied models are RT-2 [[3]](#bookmark27) and RT-H [[4]](#bookmark28) Nevertheless, the capabilities of long-term memory, understanding complex intentions, and the decomposition of complex tasks are limited for current MLMs. To achieve Artificial General Intelligence (AGI), the de­velopment of embodied AI stands as a fundamental avenue. Different from conversational agents like ChatGPT [[5]](#bookmark29) em­bodied AI believes that the true AGI can be achieved by controlling physical embodiments and interacting with both simulated and physical environments [[6]--[8]](#bookmark31) As we stand at the forefront of AGI-driven innovation, it is crucial to delve deeper into the realm of embodied AI, unraveling their complexities, evaluating their current developmental stage, and contemplating the potential trajectories they may follow in the future. Nowadays, embodied AI contains various key techniques across Computer Vision (CV), Natural Language Processing (NLP), and robotics, with the most representative being embodied perception, embodied interaction, embodied agents, and sim-to-real robotic control. Therefore, it is imper­ative to capture the evolving landscape of embodied AI in the pursuit of AGI through a comprehensive survey.

> MBODIED AI 最初是由 Alan Turing 在 1950 年的 Embodied Turing Test 提出的，旨在确定代理(agents)是否可以显示智能，这些智能不仅限于解决虚拟环境(网络空间)中的抽象问题，而且还能够驾驭物理世界的复杂性和不可预测性。网络空间中的代理(agents)通常被称为无实体 AI，而物理空间中的那些代理(agents)是实体 AI(表[I])多模态大模型(MLM)的最新进展为实体模型注入了强大的感知、交互和规划能力，以开发与虚拟和物理环境积极交互的通用实体代理(agents)和机器人。因此，实体代理(agents)被广泛认为是 MLM 的最佳载体。最近的代表性具体化模型是 RT-2 和 RT-H。然而，对于当前的传销来说，长期记忆、理解复杂意图和分解复杂任务的能力是有限的。为了实现通用人工智能(AGI)，开发嵌入式人工智能是一条基本途径。与 ChatGPT 等会话代理(agents)不同，实体 AI 认为，真正的 AGI 可以通过控制物理实施例并与模拟和物理环境交互来实现。由于我们站在 AGI 驱动创新的最前沿，深入研究实体 AI 的领域，揭示其复杂性，评估其当前的发展阶段，并考虑其未来可能遵循的潜在轨迹，这一点至关重要。如今，具身人工智能包含计算机视觉(CV)、自然语言处理(NLP)和机器人技术的各种关键技术，其中最具代表性的是具身感知、具身交互、具身代理(agents)和模拟真实机器人控制。因此，在追求 AGI 的过程中，有必要通过全面的调查来捕捉具身人工智能的发展趋势。

Embodied agent is the most prominent basis of embodied AI. For an embodied task, the embodied agent must fully un­derstand the human intention in language instructions, actively explore the surrounding environments, comprehensively per­ceive the multi-modal elements from both virtual and physical environments, and execute appropriate actions for complex tasks [[12]](#bookmark35) [[13]](#bookmark36) as shown in Fig. [2.](#_bookmark3) The rapid progress in multi-modal models exhibits superior versatility, dexterity, and generalizability in complex environments compared to tra­ditional deep reinforcement learning approaches. Pre-trained visual representations from state-of-the-art vision encoders [[14]](#bookmark37) [[15]](#bookmark38) provide precise estimations of object class, pose, and geometry, which makes the embodied models thoroughly perceive complex and dynamic environments. Powerful Large Language Models (LLMs) make robots better understand the linguistic instructions from humans. Promising MLMs gives feasible approach for aligning the visual and linguistic representations from embodied robots. The world models [[16]](#bookmark39) [[17]](#bookmark40) exhibit remarkable simulation capabilities and promis­ing comprehension of physical laws, which makes embodied models comprehensively understand both the physical and real environments. These innovations empower embodied agents to comprehensively perceive complex environment, interact with humans naturally, and execute tasks reliably.

> 具身代理(agents)是具身人工智能最突出的基础。对于具身任务，具身代理(agents)必须充分理解语言指令中的人类意图，积极探索周围的环境，从虚拟和物理环境中全面接收多模态元素，并对复杂任务执行适当的操作，如图[2]所示。来自最先进的视觉编码器的预训练视觉表示提供了对对象类别、姿势和几何形状的精确估计，这使得所体现的模型能够彻底感知复杂和动态的环境。强大的大型语言模型(LLM)使机器人能够更好地理解人类的语言指令。有前景的 MLM 为对齐具身机器人的视觉和语言表示提供了可行的方法。世界模型展现出卓越的模拟能力和对物理定律的深入理解，这使得实体模型能够全面理解物理和真实环境。这些创新使具身代理(agents)能够全面感知复杂的环境，与人类自然互动，并可靠地执行任务。

```
[]{#bookmark2 .anchor}TABLE I
[Comparison between disembodied AI and embodied AI.]{.smallcaps}
**Brain: Embodied World Model**
```

Fig. 1. []{#bookmark1 .anchor}Google Scholar results for topics of Embodied AI. The vertical and horizontal axes denote the number of publications and the year, respectively. The publications grow exponentially since the breakthrough of MLMs in 2023.

> 图 1. Embodied AI 主题的谷歌学术搜索结果。纵轴和横轴分别表示发表次数和年份。自 2023 年传销突破以来，出版物呈指数级增长。

Fig. 2. []{#bookmark3 .anchor}The overall framework of the embodied agent based on MLMs and WMs. The embodied agent has a embodied world model as its "brain". It has the capability to understand the virtual-physical environment and actively perceive multi-modal elements. It can fully understand human intention, align with human value and event causality, decompose complex tasks, and execute reliable actions, as well as interact with humans and utilize knowledge and tools.

> 图 2. 基于传销和 WMs 的实体代理(agents)的总体框架。具身代理(agents)有一个具身世界模型作为其“大脑”。它能够理解虚拟物理环境并主动感知多模态元素。它可以充分理解人类的意图，与人类价值和事件因果关系保持一致，分解复杂的任务，执行可靠的行动，以及与人类互动并利用知识和工具。

The advancement of embodied AI has exhibited rapid progress, capturing significant attention within the research community (Fig. [1),](#_bookmark1) and it is recognized as the most feasible path for achieving AGI. Google Scholar reports a substantial volume of embodied AI publications, with approximately 10,700 papers published in 2023 alone. This accounts for an average of 29 papers per day or more than one paper per hour. Despite the intensive interest in harvesting the powerful perception and reasoning ability from MLMs, the research community is short of a comprehensive survey that can help sort out existing embodied AI studies, the facing challenges, as well as future research directions. In the era of MLMs, we aim to fill up this gap by performing a systematic survey of embodied AI across cyber space to physical world. We conduct the survey from different perspectives including embodied robots, simulators, four representative embodied tasks (visual active perception, embodied interaction, multi-modal agents and sim-to-real robotic controlling), and future research directions. We believe that this survey will provide a clear big picture of what we have achieved, and we could further achieve along this emerging yet very prospective research direction.

> 具身人工智能的发展取得了快速进展，在研究界引起了极大的关注(图[1]，它被认为是实现 AGI 的最可行途径。Google Scholar 报告了大量的人工智能出版物，仅 2023 年就发表了大约 10700 篇论文。这意味着平均每天有 29 篇论文，或者每小时有一篇以上的论文。尽管人们对从 MLM 中获得强大的感知和推理能力非常感兴趣，但研究界缺乏一项全面的调查来帮助梳理现有的具身人工智能研究、面临的挑战以及未来的研究方向。在传销时代，我们的目标是通过对从网络空间到物理世界的具身人工智能进行系统调查来填补这一空白。我们从不同的角度进行了调查，包括**具身机器人、模拟器、四种代表性的具身任务(视觉主动感知、具身交互、多模态代理(agents)和模拟到真实机器人控制)以及未来的研究方向**。我们相信，这项调查将为我们所取得的成就提供一个清晰的大局，我们可以沿着这个新兴但非常有前景的研究方向进一步取得成就。

**Differences from previous works**: Although there have been several survey papers [[6]](#bookmark30) [[18]--[20]](#bookmark42) for embodied AI, most of them are outdated as they were published before the era of MLMs, which started around 2023. To the best of our knowledge, there is only one survey paper [[8]](#bookmark31) after 2023, which only focused on vision-language-action embodied AI models. However, the MLMs, WMs and embodied agents are not fully considered. Additionally, recent developments in embodied robots and simulators are also overlooked. To address the scarcity of comprehensive survey papers in this rapidly developing field, we propose this comprehensive sur­vey that covers representative embodied robots, simulators, and four main research tasks: embodied perception, embodied interaction, embodied agents, and sim-to-real robotic control.

> **与以往作品的不同之处**：尽管已经有几篇关于嵌入式 AI 的调查论文，但其中大多数已经过时，因为它们是在 2023 年左右开始的传销时代之前发表的。据我们所知，2023 年之后只有一篇调查论文，它只关注视觉语言动作体现的人工智能模型。然而，**MLM、WMs 和具身代理(agents)**并没有得到充分考虑。此外，体现机器人和模拟器的最新发展也被忽视了。为了解决这个快速发展的领域缺乏全面调查论文的问题，我们提出了这项全面的调查，涵盖了代表性的**具身机器人、模拟器和四个主要研究任务：具身感知、具身交互、具身代理(agents)和模拟真实机器人控制**。

In summary, the main contributions of this work are three­fold. First, it presents a systematic review of embodied AI including embodied robots, simulators, and four main research tasks: visual active perception, embodied interaction,embodied agents and sim-to-real robotic control. To the best of our knowledge, this is the first comprehensive survey of embodied AI from the perspective of the alignment of cyber and physical spaces based on MLMs and WMs, offering a broad overview with a thorough summary and categorization of existing studies. Second, it examines the latest progress in embodied AI, providing comprehensive benchmarking and discussion of current work across multiple simulators and datasets. Third, it

> 总之，这项工作的主要贡献有三方面。首先，它对具身人工智能进行了系统回顾，包括具身机器人、模拟器和四个主要研究任务：视觉主动感知、具身交互、具身代理(agents)和模拟真实机器人控制。据我们所知，这是第一次从基于 MLM 和 WMs 的网络和物理空间对齐的角度对具身人工智能进行全面调查，对现有研究进行了全面的总结和分类。其次，它考察了嵌入式人工智能的最新进展，提供了跨多个模拟器和数据集的当前工作的全面基准测试和讨论。第三，它

Fig. 3. This survey focuses on comprehensive analysis of the latest advancements in embodied AI.
Fig. 4. []{#bookmark4 .anchor}The Embodied Robots include Fixed-base Robots, Quadruped Robots, Humanoid Robots, Wheeled Robots, Tracked Robots, and Biomimetic Robots.

A. _Fixed-base Robots_

Fixed-base robots, as shown in Fig. [4](#_bookmark4) (a), are extensively employed in laboratory automation, educational training, and industrial manufacturing due to their compactness and high-precision operations. These robots feature robust bases and structures that ensure stability and high accuracy during opera­tion. Equipped with high-precision sensors and actuators, they achieve micron-level precision, making them suitable for tasks that require high accuracy and repeatability [[21]](#bookmark43) Moreover, fixed-base robots are highly programmable, allowing users to identifies several research challenges and potential directions for future research in AGI for embodied AI.

> 如图[4](a)所示，固定基座机器人因其紧凑性和高精度操作而广泛应用于实验室自动化、教育培训和工业制造。这些机器人具有坚固的底座和结构，可确保操作过程中的稳定性和高精度。它们配备了高精度传感器和执行器，实现了微米级的精度，使其适用于需要高精度和可重复性的任务。此外，固定基座机器人具有高度可编程性，使用户能够识别多个研究挑战和未来 AGI 研究的潜在方向。

The rest of this survey is organized as follows. Section 2 introduces various embodied robots. Section 3 describes gen­eral and real-scene embodied simulators. Section 4 introduces embodied perception, including active visual perception, 3D visual grounding, visual language navigation and non-visual perception. Section 5 introduces embodied interaction. Section 6 introduces embodied agents including the embodied multi­modal foundation model and embodied task planning. Section 7 introduces sim-to-real adaptation including embodied world model, data collection and training, and embodied control. In Section 8, we discuss promising research directions.

> 本次调查的其余部分组织如下：
> 第 2 节介绍了各种机器人；
> 第 3 节描述了通用和真实场景体现模拟器；
> 第 4 节介绍了具身感知，包括主动视觉感知、3D 视觉基础、视觉语言导航和非视觉感知；
> 第 5 节介绍了具身交互；
> 第 6 节介绍了具身代理(agents)，包括具身多模式基础模型和具身任务规划；
> 第 7 节介绍了 sim 到实际适应，包括具身世界模型、数据收集和训练以及具身控制；
> 在第 8 节中，我们讨论了有前景的研究方向。

## II. EMBODIED ROBOTS

Embodied agent actively interacts with the physical envi­ronment and encompasses a broad spectrum of embodiments, including robots, smart appliances, smart glasses, autonomous vehicles, etc. Among them, robots stand out as one of the most prominent embodiments. Depending on the applications, robots are designed in various forms to leverage their hardware characteristics for specific tasks, as shown in Fig. [4.](#_bookmark4)

> 体现代理(agents)积极与物理环境交互，涵盖了广泛的实施例，包括机器人、智能电器、智能眼镜、自动驾驶汽车等。其中，机器人是最突出的实施例之一。根据应用，机器人被设计成各种形式，以利用其硬件特性完成特定任务，如图[4]所示。

adapt them for various task scenarios, such as Franka (Franka Emika panda) [[22]](#bookmark44) Kuka iiwa (KUKA) [[23]](#bookmark45) and Sawyer (Rethink Robotics) [[24]](#bookmark46) Nevertheless, fixed-base robots have certain disadvantages. Their fixed-base design limits their operational range and flexibility, preventing them from moving or adjusting positions over large areas and leading to their collaboration with humans and other robots. [[21]](#bookmark43)

> 使它们适应各种任务场景，如 Franka(Franka Emika panda) Kuka iiwa(Kuka) 和 Sawyer(Rethink Robotics)。然而，固定基座机器人有一定的缺点。它们的固定基座设计限制了它们的操作范围和灵活性，阻止了它们在大面积上移动或调整位置，并导致它们与人类和其他机器人的协作。

B. _Wheeled Robots and Tracked Robots_

For mobile robots, they can face more complex and diverse application scenarios. Wheeled robots, as shown in Fig. [4](#_bookmark4) (b), known for their efficient mobility, are widely employed in lo­gistics, warehousing, and security inspections. The advantages of wheeled robots include their simple structure, relatively low cost, high energy efficiency, and rapid movement capabilities on flat surfaces [[21]](#bookmark43) These robots are typically equipped with high-precision sensors such as LiDAR and cameras, enabling autonomous navigation and environmental perception, making them highly effective in automated warehouse management and inspection tasks, e.g., Kiva robots (Kiva Systems) [[25]](#bookmark47) and Jackal robot (Clearpath Robotics) [[26]](#bookmark48) However, wheeled robots have limited mobility in complex terrains and harsh en­vironments, particularly on uneven ground. Additionally, their load capacity and maneuverability are somewhat restricted.

> 对于移动机器人来说，它们可以面对更复杂和多样化的应用场景。如图[4](b)所示，轮式机器人以其高效的移动性而闻名，广泛应用于物流、仓储和安全检查。轮式机器人的优点包括结构简单、成本相对较低、能效高、在平面上的快速移动能力[21]。这些机器人通常配备有激光雷达和摄像头等高精度传感器，能够实现自主导航和环境感知，使其在自动化仓库管理和检查任务中非常有效，例如 Kiva 机器人(Kiva Systems)和 Jackal 机器人(Clearpath Robotics)。然而，轮式机器人在复杂地形和恶劣环境中的机动性有限，特别是在不平坦的地面上。此外，它们的承载能力和机动性也受到一定限制。

Differently, tracked robots have powerful off-road capabil­ities and maneuverability, showing potential in agriculture, construction, and disaster recovery, as shown in Fig. [4](#_bookmark4) (c). The track system provides a larger ground contact area, distributing the robot's weight and reducing the risk of sinking in soft terrain such as mud and sand. Moreover, tracked robots are equipped with robust power and suspension systems, to maintain stability and traction on complex terrains [[27]](#bookmark49) Con­sequently, tracked robots are also used in sensitive areas such as the military. The iRobot's PackBot is a versatile military-tracked robot capable of performing tasks such as reconnais­sance, explosive ordnance disposal, and rescue missions [[28]](#bookmark50) However, due to the high friction of the track system, tracked robots often suffer from low energy efficiency. Additionally, their movement speed on flat surfaces is slower than wheeled robots, as well as their flexibility and maneuverability.

> 不同的是，履带式机器人具有强大的越野能力和机动性，在农业、建筑和灾难恢复方面显示出潜力，如图[4](c)所示。轨道系统提供了更大的地面接触面积，分散了机器人的重量，降低了在泥泞和沙地等软地形中沉没的风险。此外，履带式机器人配备了强大的动力和悬架系统，以在复杂的地形上保持稳定性和牵引力[[27]]。iRobot 的 PackBot 是一种多功能的军用履带式机器人，能够执行侦察、爆炸物处理和救援任务。然而，由于履带系统的高摩擦，履带式机器人的能源效率往往较低。此外，它们在平面上的移动速度比轮式机器人慢，灵活性和机动性也较差。

C. _Quadruped Robots_

Quadruped robots, known for their stability and adaptabil­ity, are well-suited for complex terrain exploration, rescue missions, and military applications. Inspired by quadrupedal animals, these robots can maintain balance and mobility on uneven surfaces, as shown in Fig. [4](#_bookmark4) (d). The multi-jointed design allows them to mimic biological movements, achieving complex gaits and posture adjustments. High adjustability enables the robots to automatically adapt their stance to chang­ing terrain, enhancing maneuverability and stability. Sensing systems, such as LiDAR and cameras, provide environmental awareness, allowing the robots to navigate autonomously and avoid obstacles [[29]](#bookmark51) Several types of quadruped robots are widely used: Unitree Robotics, Boston Dynamics Spot, and ANYmal C. Unitree Robotics' Unitree A1 and Go1 are noted for their cost-effectiveness and flexibility. The A1 [[30]](#bookmark52) and Go1 [[31]](#bookmark53) possess strong mobility and intelligent obsta­cle avoidance capabilities, suitable for various applications. Boston Dynamics' Spot is renowned for its superior stability and operational flexibility, which are commonly used in in­dustrial inspections and rescue missions. It features powerful load-carrying capacity and adaptability, capable of perform­ing complex tasks in harsh environments [[32]](#bookmark54) ANYbotics' ANYmal C, with its modular design and high durability, is widely employed in industrial inspection and maintenance. The ANYmal C is equipped with autonomous navigation and remote operation capabilities, suitable for prolonged outdoor tasks and even extreme lunar missions [[33]](#bookmark55) The complex design and high manufacturing costs of quadruped robots result in substantial initial investments, limiting their use in cost-sensitive areas. Additionally, they have limited battery en­durance in complex environments, requiring frequent recharg­ing or battery replacement for prolonged operation [[34]](#bookmark56)

> 四足机器人以其稳定性和适应性而闻名，非常适合复杂的地形探索、救援任务和军事应用。受四足动物的启发，这些机器人可以在不平坦的表面上保持平衡和机动性，如图[4](d)所示。多关节设计使它们能够模仿生物运动，实现复杂的步态和姿势调整。高度的可调性使机器人能够自动调整姿态以适应不断变化的地形，从而增强机动性和稳定性。传感系统，如激光雷达和摄像头，提供环境意识，使机器人能够自主导航并避开障碍物几种类型的四足机器人被广泛使用：Unitree Robotics、Boston Dynamics Spot 和 ANYmal C.Unitree robots 的 Unitree A1 和 Go1 以其成本效益和灵活性而闻名。A1 和 Go1 具有强大的移动性和智能避障能力，适用于各种应用。波士顿动力公司的 Spot 以其卓越的稳定性和操作灵活性而闻名，通常用于工业检查和救援任务。它具有强大的承载能力和适应性，能够在恶劣的环境中执行复杂的任务 ANYbotics 的 ANYmal C 具有模块化设计和高耐用性，广泛应用于工业检测和维护。ANYmal C 配备了自主导航和远程操作功能，适用于长时间的户外任务，甚至极端的月球任务四足机器人的复杂设计和高制造成本导致了大量的初始投资，限制了它们在成本敏感区域的使用。此外，它们在复杂环境中的电池续航能力有限，需要频繁充电或更换电池才能长时间运行

D. _Humanoid Robots_

Humanoid robots are distinguished by their human-like form and are increasingly prevalent in sectors such as the service industry, healthcare, and collaborative environments. These robots can mimic human movements and behavioral patterns, providing personalized services and support. Their dexterous hand designs enable them to perform intricate and complex tasks, distinguishing them from other types of robots, as shown in Fig. [4](#_bookmark4) (e). These hands typically have multiple de­grees of freedom and high-precision sensors, allowing them to emulate the grasping and manipulation capabilities of human hands, which is particularly crucial in fields such as medical surgery and precision manufacturing [[35]](#bookmark57) Among current humanoid robots, Atlas (Boston Dynamics) is renowned for its exceptional mobility and stability. Atlas can perform com­plex dynamic actions such as running, jumping, and rolling, demonstrating the potential of humanoid robots in highly dynamic environments [[36]](#bookmark58) The HRP series (AIST) is utilized in various research and industrial applications, with a design focus on high stability and flexibility, making it effective in complex environments, particularly for collaborative tasks with humans [[37]](#bookmark59) ASIMO (Honda), one of the most famous humanoid robots, can walk, run, climb stairs, and recognize faces and gestures, making it suitable for reception and guide services [[38]](#bookmark60) Additionally, a small social robot, Pepper (Soft-bank Robotics) can recognize emotions and engage in natural language communication and is widely used in customer service and educational settings [[39]](#bookmark61)

> 人形机器人以其类似人类的形态而闻名，在服务业、医疗保健和协作环境等领域越来越普遍。这些机器人可以模仿人类的动作和行为模式，提供个性化的服务和支持。如图[4](e)所示，它们灵巧的手设计使它们能够执行复杂而复杂的任务，使其有别于其他类型的机器人。这些手通常具有多种自由度和高精度传感器，使其能够模拟人手的抓握和操纵能力，这在医疗手术和精密制造等领域尤为重要。在目前的人形机器人中，Atlas(波士顿动力公司)以其出色的机动性和稳定性而闻名。Atlas 可以执行复杂的动态动作，如跑步、跳跃和滚动，展示了人形机器人在高度动态环境中的潜力 HRP 系列(AIST)用于各种研究和工业应用，其设计侧重于高稳定性和灵活性，使其在复杂环境中有效，特别是在与人类的协作任务中 ASIMO(本田)是最著名的人形机器人之一，可以行走、跑步、爬楼梯和识别面部和手势，使其适用于接待和引导服务此外，小型社交机器人 Pepper(软银机器人)可以识别情绪从事自然语言交流，广泛应用于客户服务和教育环境

Nevertheless, humanoid robots face challenges in main­taining operational stability and reliability in complex envi­ronments due to their sophisticated control systems. These challenges include robust bipedal walking control and dex­terous hand grasping [[40].Furthermore,](#_bookmark62) traditional humanoid robots based on hydraulic systems, characterized by their bulky structures and high maintenance costs, are increasingly being replaced by motor-driven systems. Recently, Tesla and Unitree Robotics have introduced their humanoid robots based on motor systems. With the integration of LLMs, humanoid robots are expected to handle various complex tasks intelli­gently, filling labor gaps in manufacturing, healthcare, and the service industry, thereby improving efficiency and safety [[41]](#bookmark63)

> 然而，由于其复杂的控制系统，人形机器人在复杂环境中保持操作稳定性和可靠性方面面临挑战。这些挑战包括强健的双足行走控制和灵巧的手抓握。此外，基于液压系统的传统人形机器人，以其庞大的结构和高昂的维护成本为特征，正越来越多地被电机驱动系统所取代。最近，特斯拉和 Unitree Robotics 推出了基于电机系统的人形机器人。随着 LLM 的集成，人形机器人有望智能地处理各种复杂的任务，填补制造业、医疗保健和服务业的劳动力缺口，从而提高效率和安全性

E. _Biomimetic Robots_

Differently, biomimetic robots perform tasks in complex and dynamic environments by simulating efficient movements and functions of natural organisms. By emulating biologi­cal entities' forms and movement mechanisms, these robots demonstrate significant potential in fields such as healthcare, environmental monitoring, and biological research [[21]](#bookmark43) Typ­ically, they utilize flexible materials and structures to achieve lifelike, agile movements and minimize environmental impact. Importantly, biomimetic designs can significantly improve the robots' energy efficiency by mimicking the efficient move­ment mechanisms of biological organisms, making them more economical regarding energy consumption [[42]](#bookmark64) [[43]](#bookmark65) These biomimetic robots include fish-like robots [[44]](#bookmark66) [[45]](#bookmark67) insect-like robots [[46]](#bookmark68) [[47]](#bookmark69) and soft-bodied robots [[48]](#bookmark70) as shown in Fig. [4](#_bookmark4) (f). However, biomimetic robots face several challenges. First, their design and manufacturing processes are complex and costly, limiting large-scale production and widespread application. Second, due to their use of flexible materials and complex movement mechanisms, the durability and reliability of biomimetic robots are limited in extreme environments.

> 不同的是，仿生机器人通过模拟自然生物的有效运动和功能，在复杂和动态的环境中执行任务。通过模拟生物实体的形态和运动机制，这些机器人在医疗保健、环境监测和生物研究等领域显示出巨大的潜力典型地，它们利用柔性材料和结构来实现逼真、敏捷的运动，并最大限度地减少对环境的影响。重要的是，仿生设计可以通过模仿生物有机体的高效运动机制来显著提高机器人的能源效率，使其在能源消耗方面更加经济。这些仿生机器人包括鱼状机器人，如图[4](f)所示。然而，仿生机器人面临着几个挑战。首先，它们的设计和制造过程复杂且成本高昂，限制了大规模生产和广泛应用。其次，由于使用柔性材料和复杂的运动机制，仿生机器人在极端环境中的耐用性和可靠性受到限制。

## III. EMBODIED SIMULATORS

Embodied simulators are vital for embodied AI as they offer cost-effective experimentation, ensuring safety by simulating potentially hazardous scenarios, scalability for testing in di­verse environments, rapid prototyping capabilities, accessibil­ity to a wider research community, controlled environments for precise studies, data generation for training and evaluation, and standardized benchmarks for algorithm comparison. To enable agents to interact with the environment, it is necessary to construct a realistic simulated environment. This requires consideration of the physical characteristics of the environ­ment, the properties of objects, and their interactions.

> 具身模拟器对于具身人工智能至关重要，因为它们提供了具有成本效益的实验，通过模拟潜在的危险场景来确保安全，在逆向环境中进行测试的可扩展性，快速原型制作能力，可访问更广泛的研究社区，用于精确研究的受控环境，用于训练和评估的数据生成，以及用于算法比较的标准化基准。为了使代理(agents)能够与环境交互，有必要构建一个逼真的模拟环境。这需要考虑环境的物理特性、物体的属性及其相互作用。

This section will introduce the commonly used simulation platforms in two parts: the general simulator based on under­lying simulation and the simulator based on real scenes.

> 本节将分两部分介绍常用的仿真平台：基于底层仿真的通用模拟器和基于真实场景的模拟器。

```
[]{#bookmark5 .anchor}TABLE II
Fig. 5. Examples of General Simulators. The MuJoCo's figure is from
```

A. _General Simulator_

The physical interactions and dynamic changes present in real environments are irreplaceable. However, deploying embodied models in the physical world often incurs high costs and faces numerous challenges. General-purpose simulators provide a virtual environment that closely mimics the physical world, allowing for algorithm development and model training, which offers significant cost, time, and safety advantages.

> 真实环境中存在的物理相互作用和动态变化是不可替代的。然而，在物理世界中部署实体模型通常会带来高昂的成本，并面临诸多挑战。**通用模拟器**提供了一个紧密模拟物理世界的虚拟环境，允许算法开发和模型训练，这提供了显著的成本、时间和安全优势。

**Isaac Sim** [[49]](#bookmark71) is an advanced simulation platform for robotics and AI research. It has high-fidelity physical sim­ulation, real-time ray tracing, an extensive library of robotic models, and deep learning support. Its application scenarios in­clude autonomous driving, industrial automation, and human-robot interaction.**Gazebo** [[60]](#bookmark82) is an open-source simulator for robotics research. It has extensive robot libraries, and tight integration with ROS. It supports the simulation of various sensors and offers numerous pre-built robot models and envi­ronments. It is mainly used for robot navigation and control and multi-robot systems.**PyBullet** [[52]](#bookmark74) is the python interface for the Bullet physics engine. It is easy to be used and has di­verse sensor simulation and deep learning integration. PyBullet supports real-time physical simulation, including rigid body dynamics, collision detection, and constraint solving. Table. [II](#_bookmark5)

> **Isaac Sim**是一个用于机器人和人工智能研究的高级仿真平台。它具有高保真物理模拟、实时光线追踪、广泛的机器人模型库和深度学习支持。其应用场景包括自动驾驶、工业自动化和人机交互。
> **Gazebo**是一个用于机器人研究的开源模拟器。它拥有广泛的机器人库，并与 ROS 紧密集成。它支持各种传感器的模拟，并提供了许多预先构建的机器人模型和环境。它主要用于机器人导航和控制以及多机器人系统。
> **PyBullet**是 Bullet 物理引擎的 python 接口。它易于使用，具有逆向传感器模拟和深度学习集成。PyBullet 支持实时物理仿真，包括刚体动力学、碰撞检测和约束求解。表[II]。

```
Habitat iGibson TDW
Fig. 6. Examples of Real-Scene Based Simulators.
```

presents the key features and primary application scenarios of 10 general-purpose simulators. They each offer unique advantages in the field of embodied AI. Researchers can select the most appropriate simulator based on their specific research needs, thereby accelerating the development and application of embodied AI technologies. Fig. [5](#_bookmark6) shows the visualization effects of the general simulators.

> 介绍了 10 个通用模拟器的关键特性和主要应用场景。它们在具身人工智能领域都有独特的优势。研究人员可以根据自己的具体研究需求选择最合适的模拟器，从而加速具身人工智慧技术的开发和应用。图[5]显示了通用模拟器的可视化效果。

B. _Real-Scene Based Simulators_

Achieving universal embodied agents in household activities has been a primary focus in the field of embodied AI research. These embodied agents need to deeply understand human daily life and perform complex embodied tasks such as navigation and interaction in indoor environments. To meet the demands of these complex tasks, the simulated environments need to be as close to real world as possible, which places high demands on the complexity and realism of the simulators. This led to the creation of simulators based on real world environments. These simulators mostly collect data from the real world, create photorealistic 3D assets, and build scenes using 3D game engines like UE5 and Unity. The rich and realistic scenes make simulators based on real world environments the top choice for research on embodied AI in household activities.

> 在家庭活动中实现普遍的具身代理(agents)一直是具身人工智能研究领域的主要焦点。这些具身代理(agents)需要深入了解人类的日常生活，并在室内环境中执行复杂的具身任务，如导航和交互。为了满足这些复杂任务的需求，模拟环境需要尽可能接近现实世界，这对模拟器的复杂性和真实性提出了很高的要求。这导致了基于现实世界环境的模拟器的创建。这些模拟器主要从现实世界收集数据，创建逼真的 3D 资产，并使用 UE5 和 Unity 等 3D 游戏引擎构建场景。丰富而逼真的场景使基于现实世界环境的模拟器成为家庭活动中具身人工智能研究的首选。

**AI2-THOR** [[61]](#bookmark83) is an indoor embodied scene simulator based on Unity3D, led by the Allen Institute for Artificial Intelligence. As a high-fidelity simulator built in the real world, AI2-THOR has richly interactive scene objects and the physical properties assigned to them (such as open/close or even cold/hot). AI2-THOR consists of two parts: iTHOR and RoboTHOR. iTHOR contains 120 rooms categorized as kitchens, bedrooms, bathrooms, and living rooms, with over 2000 unique interactive objects, and supports multi-agent simulation; RoboTHOR contains 89 modular apartments with 600+ objects, the uniqueness of which is that these apartments correspond to real scenes in the real world. So far, more than a hundred works have been published based on AI2-THOR.

> **AI2-THOR**是由艾伦人工智能研究所领导的基于 Unity3D 的室内嵌入式场景模拟器。作为构建在现实世界中的高保真模拟器，AI2-THOR 具有丰富的交互式场景对象和分配给它们的物理属性(如打开/关闭甚至冷/热)。AI2-THOR 由两部分组成：iTHOR 和 RoboTHOR。iTHOR 包含 120 个房间，分为厨房、卧室、浴室和客厅，有 2000 多个独特的交互对象，并支持多智能体模拟；RoboTHOR 包含 89 个模块化公寓，有 600 多个物品，其独特之处在于这些公寓对应于现实世界中的真实场景。到目前为止，基于 AI2-THOR 已经发表了 100 多部作品。

```table
[]{#bookmark7 .anchor}TABLE III
```

**Matterport 3D** [[62]](#bookmark84) is proposed in R2R [[63]](#bookmark85) and is more commonly used as a large-scale 2D-3D visual dataset. The Matterport3D dataset includes 90 architectural indoor scenes, comprises 10,800 panoramas and 194,400 RGB-D images, and provides surface reconstruction, camera posture, and 2D and 3D semantic segmentation annotations. Matterport3D trans­forms 3D scenes into discrete "viewpoints", and embodied agents move between adjacent "viewpoints" in Matterport3D scenes. At each "viewpoint", embodied agents can obtain a 1280x1024 panorama image (18*×* RGB-D) centered on the "viewpoint". Matterport3D is one of the most important embodied navigation benchmarks.

> **Matterport 3D**在 R2R 中提出，更常用于大规模 2D-3D 视觉数据集。Matterport3D 数据集包括 90 个建筑室内场景，包括 10800 张全景图和 194400 张 RGB-D 图像，并提供表面重建、相机姿态以及 2D 和 3D 语义分割注释。Matterport3D 将 3D 场景转换为离散的“视点”，化身代理(agents)在 Matterport 三维场景中的相邻“视点”之间移动。在每个“视点”，实体代理(agents)可以获得以“视点”为中心的 1280x1024 全景图像(18*×*RGB-D)。Matterport3D 是最重要的嵌入式导航基准之一。

**Virtualhome** [[64]](#bookmark86) is a home activity embodied AI simulator brought by Puig et al. What makes Virtualhome special most is its environment represented by an environment graph. The environment graph represents the objects in the scene and their related relationships. Users can also customize and modify the environment graph to achieve the custom configuration of scene objects. This kind of environment graph provides a new way for embodied agents to understand the environment. Sim­ilar to AI2-THOR, Virtualhome also provides a large number of interactive objects, and embodied agents can interact with them and change their status. Another feature of Virtualhome is its simple and easy-to-use API. The actions of embodied agents are simplified to the format of "operation + object". This feature makes Virtualhome widely used in the research fields of embodied planning, instruction decomposition, etc.

> **Virtualhome**是 Puig 等人带来的一个家庭活动体现的人工智能模拟器。Virtualhome 最特别的地方在于它的环境由环境图表示。环境图表示场景中的对象及其相关关系。用户还可以自定义和修改环境图，实现场景对象的自定义配置。这种环境图为具身代理(agents)理解环境提供了一种新的方法。与 AI2-THOR 类似，Virtualhome 还提供了大量的交互对象，实体代理(agents)可以与它们交互并改变它们的状态。Virtualhome 的另一个功能是其简单易用的 API。化身主体的行为被简化为“操作+对象”的形式。这一特性使得 Virtualhome 在具体规划、指令分解等研究领域得到了广泛的应用。

**Habitat** [[65]](#bookmark87) is an open-source simulator for large-scale human-robot interaction launched by Meta. Based on the Bullet physics engine, Habitat has implemented high-performance, high-speed, parallel 3D simulation, and provides a rich interface for reinforcement learning of embodied agents. Habitat has extremely high degree of openness. Researchers can import and create 3D scenes in Habitat or use the rich open resources on the Habitat platform for expansion. Habi­tat has many customizable sensors and supports multi-agent simulation. Multiple embodied agents from open resources or customizations (e.g., humans and robot dogs) can cooperate in the simulator, move freely, and perform simple interactions with the scene. Thus, Habitat is attracting increasing attention. Different from other simulators that focus more on the scene, **SAPIEN** [[66]](#bookmark88) pays more attention to simulate the inter­action of objects. Based on the PhysX physics engine, SAPIEN provides fine-grained embodied control, which can implement joint control based on force and torque through the ROS interface. Based on the PartNet-Mobility Dataset, SAPIEN provides indoor simulation scenes containing rich interactive objects and supports the import of custom resources. Dif­ferent from simulators like AI2-THOR that directly changes the status of objects, SAPIEN supports simulated physical interactions, and embodied agents can control the hinged parts of objects through physical actions, thereby changing the status of objects. These features make SAPIEN very suitable for training the fine-grained object operation of embodied AI.

> **Habitat**是 Meta 推出的大规模人机交互开源模拟器。基于 Bullet 物理引擎，Habitat 实现了高性能、高速、并行的 3D 仿真，并为具身代理(agents)的强化学习提供了丰富的界面。栖息地具有极高的开放性。研究人员可以在 Habitat 中导入和创建 3D 场景，也可以使用 Habitat 平台上丰富的开放资源进行扩展。Habitat 有许多可定制的传感器，并支持多智能体模拟。来自开放资源或定制的多个实体代理(agents)(例如，人类和机器狗)可以在模拟器中协作，自由移动，并与场景进行简单的交互。因此，Habitat 正受到越来越多的关注。
> 与其他更关注场景的模拟器不同，**SAPIEN**更注重模拟对象的相互作用。SAPIEN 基于 PhysX 物理引擎，提供细粒度的实体控制，可以通过 ROS 接口实现基于力和扭矩的联合控制。基于 PartNet Mobility Dataset，SAPIEN 提供包含丰富交互对象的室内仿真场景，并支持导入自定义资源。与直接改变对象状态的 AI2-THOR 等模拟器不同，SAPIEN 支持模拟物理交互，体现代理(agents)可以通过物理动作控制对象的铰接部分，从而改变对象的状态。这些特性使得 SAPIEN 非常适合训练嵌入式 AI 的细粒度对象操作。

**iGibson** [[67]](#bookmark89) [[68]](#bookmark90) is an open-source simulator launched by Stanford. Built on the Bullet physics engine, iGibson provides 15 high-quality indoor scenes, and supports the import of as­sets from other datasets such as Gibson and Matterport3D. As an object-oriented simulator, iGibson assigns rich changeable attributes to objects, not limited to the kinematic properties of objects (posture, speed, acceleration, etc.), but also in­cludes temperature, humidity, cleanliness, switch status, etc. In addition, besides the depth and semantic sensors that are standard in other simulators, iGibson also provides LiDAR for embodied agents, allowing agents to obtain 3D point clouds in the scene easily. Regarding embodied agent configuration, iGibson supports continuous action control and fine-grained joint control. This allows the embodied agents in iGibson to interact delicately with the objects while moving freely.

> **iGibson**是斯坦福大学推出的开源模拟器。iGibson 基于 Bullet 物理引擎构建，提供 15 个高质量的室内场景，并支持从 Gibson 和 Matterport3D 等其他数据集导入数据集。作为一个面向对象的模拟器，iGibson 为对象分配了丰富的可变属性，不仅限于对象的运动学属性(姿势、速度、加速度等)，还包括温度、湿度、清洁度、开关状态等。此外，除了其他模拟器中标准的深度和语义传感器外，iGibon 还为实体代理(agents)提供了 LiDAR，使代理(agents)能够轻松获取场景中的 3D 点云。关于实体代理(agents)配置，iGibson 支持连续动作控制和细粒度联合控制。这允许 iGibson 中的实体代理(agents)在自由移动的同时与对象进行微妙的交互。

**TDW** [[69]](#bookmark91) was launched by MIT. As one of the latest embodied simulators, TDW combines high-fidelity video and audio rendering, realistic physical effects, and a single flexible controller, making certain progress in the perception and inter­action of the simulated environment. TDW integrates multiple physics engines into one framework, which can realize the physical interaction simulation of various materials such as rigid bodies, soft bodies, fabrics, and fluids, and provides situational sounds when interacting with objects. Thus, TDW has taken an important step compared to other simulators. TDW supports the deployment of multiple intelligent agents and provides users with a rich API library and asset library, allowing users to freely customize scenes and tasks according to their own needs, even outdoor scenes and related tasks.

> **TDW**是由麻省理工学院发起的。作为最新的嵌入式模拟器之一，TDW 结合了高保真视频和音频渲染、逼真的物理效果和一个灵活的控制器，在模拟环境的感知和交互方面取得了一定的进步。TDW 将多个物理引擎集成到一个框架中，可以实现刚体、软体、织物和流体等各种材料的物理交互模拟，并在与物体交互时提供情境声音。因此，与其他模拟器相比，TDW 迈出了重要的一步。TDW 支持部署多个智能代理(agents)，并为用户提供丰富的 API 库和资产库，允许用户根据自己的需求自由定制场景和任务，甚至户外场景和相关任务。

Table [III](#_bookmark7) summarizes all the simulators based on the real scenarios mentioned above. Sapien stands out for its design, specifically tailored to simulate interactions with joint objects like doors, cabinets, and drawers. VirtualHome is notable for its unique environment graph, which facilitates high-level embodied planning based on natural language descriptions of environments. While AI2Thor offers a wealth of interactive scenes, these interactions, similar to those in VirtualHome, are script-based and lack real physical interactions. This de­sign suffices for embodied tasks not requiring fine-grained interactions. Both iGibson and TDW provide fine-grained embodied control and highly simulated physical interactions.

> 表[III]总结了基于上述真实场景的所有模拟器。Sapien 因其设计而脱颖而出，专门为模拟与门、橱柜和抽屉等关节对象的交互而量身定制。VirtualHome 以其独特的环境图而闻名，它促进了基于环境自然语言描述的高级体现规划。虽然 AI2Thor 提供了丰富的交互式场景，但这些交互与 VirtualHome 中的交互类似，都是基于脚本的，缺乏真正的物理交互。这种设计足以满足不需要细粒度交互的具体任务。iGibson 和 TDW 都提供了细粒度的实体控制和高度模拟的物理交互。

```
[]{#bookmark8 .anchor}TABLE IV
[The comparison of the active visual perception methods.]{.smallcaps}
```

iGibson excels in offering abundant and realistic large-scale scenes, making it suitable for complex and long-term mobile operations, whereas TDW allows greater user freedom in scene expansion and features unique audio and flexible fluid simula­tions, making it indispensable for related simulation scenarios. Matterport3D, a foundational 2D-3D visual dataset, is widely used and extended in embodied AI benchmarks. Although the embodied agent in Habitat lacks interaction capabilities, its extensive indoor scenes, user-friendly interfaces, and open framework make it highly regarded in embodied navigation.

> iGibson 擅长提供丰富而逼真的大规模场景，使其适用于复杂和长期的移动操作，而 TDW 允许用户在场景扩展方面有更大的自由度，并具有独特的音频和灵活的流体模拟功能，使其在相关模拟场景中不可或缺。Matterport3D 是一个基础的 2D-3D 视觉数据集，在嵌入式 AI 基准测试中得到了广泛的应用和扩展。虽然 Habitat 中的实体代理(agents)缺乏交互能力，但其广泛的室内场景、用户友好的界面和开放的框架使其在实体导航中受到高度重视。

Besides, automated simulation scene construction is greatly beneficial for obtaining high-quality embodied data. **RoboGen** [[70]](#bookmark92) customizes tasks from randomly sampled 3D assets through LLMs, thereby creating scenes and automatically training agents; **HOLODECK** [[71]](#bookmark93) can automatically cus­tomize corresponding high-quality simulation scenes in AI2-THOR based on human instructions; **PhyScene** [[72]](#bookmark94) gen­erates interactive and physically consistent high-quality 3D scenes based on conditional diffusion. The Allen Institute for Artificial Intelligence expanded AI2-THOR and proposed **ProcTHOR** [[73]](#bookmark95) which can automatically generate simulated scenes with sufficient interactivity, diversity, and rationality. These methods provide insights into the embodied AI.

> 此外，自动模拟场景构建对于获得高质量的体现数据非常有益**RoboGen**通过 LLM 从随机采样的 3D 资产中定制任务，从而创建场景并自动训练代理(agents)**HOLODECK**可以根据人类指令在 AI2-THOR 中自动定位相应的高质量模拟场景**PhyScene**基于条件扩散生成交互式和物理一致的高质量 3D 场景。艾伦人工智能研究所扩展了 AI2-THOR，并提出了**ProcTHOR**，它可以自动生成具有足够交互性、多样性和合理性的模拟场景。这些方法提供了对所体现的人工智能的见解。

## IV. EMBODIED PERCEPTION

The "north stars" of the future of visual perception is embodied-centric visual reasoning and social intelligence [[74]](#bookmark96) Unlike merely recognizing objects in images, agent with em­bodied perception must move in the physical world and inter­act with the environment. This requires a deeper understanding of 3D space and dynamic environments. Embodied perception requires visual perception and reasoning, understanding the 3D relations within a scene, and predicting and performing complex tasks based on visual information.

> 视觉感知未来的“北极星”是以身体为中心的视觉推理和社会智能与仅仅识别图像中的物体不同，具有身体感知的主体必须在物理世界中移动并与环境互动。这需要对 3D 空间和动态环境有更深入的了解。体现感知需要视觉感知和推理，理解场景中的 3D 关系，并根据视觉信息预测和执行复杂的任务。

A. _Active Visual Perception_

Active visual perception systems require fundamental ca­pabilities such as state estimation, scene perception, and environment exploration. As shown in Fig. [7,](#_bookmark9) these capabilities have been extensively studied within the domains of Visual Si­multaneous Localization and Mapping (vSLAM) [[118]](#bookmark140) [[119]](#bookmark141) 3D Scene Understanding [[120]](#bookmark142) and Active Exploration [[12]](#bookmark35) These research areas contribute to developing robust active visual perception systems, facilitating improved environmental interaction and navigation in complex, dynamic settings. We briefly introduce these three components and summarize the methods mentioned in each part in Table [IV.](#_bookmark8)

> 主动视觉感知系统需要基本的能力，如状态估计、场景感知和环境探索。如图[7]所示，这些功能在视觉同步定位和映射(vSLAM)3D 场景理解和主动探索领域得到了广泛的研究。这些研究领域有助于开发强大的主动视觉感知系统，促进复杂动态环境中的环境交互和导航。我们简要介绍了这三个组成部分，并总结了表[IV]中每个部分提到的方法

> Fig. 7. []{#bookmark9 .anchor}The schematic diagram of active visual perception. Visual SLAM and 3D Scene Understanding provide the foundation for passive visual perception, while active exploration can provide activeness to the passive perception system. These three elements complement each other and are essential to the active visual perception system.

> 图 7. 主动视觉感知的示意图。视觉 SLAM 和 3D 场景理解为被动视觉感知提供了基础，而主动探索可以为被动感知系统提供主动性。这三个元素相辅相成，对主动视觉感知系统至关重要。

1.  _Visual Simultaneous Localisation and Mapping:_ Simul­taneous Localization and Mapping (SLAM) is a technique that determines a mobile robot's position in an unknown environment while concurrently constructing a map of that environment [[121]](#bookmark143) [[122]](#bookmark144) Range-based SLAM [[123]--[125]](#bookmark146) creates point cloud representations using rangefinders (e.g., laser scanners, radar, and/or sonar), but is costly and provides limited environmental information. Visual SLAM (vSLAM) [[118]](#bookmark140) [[119]](#bookmark141) uses on-board cameras to capture frames and construct a representation of the environment. It has gained popularity due to its low hardware cost, high accuracy in small-scale scenarios, and ability to capture rich environmental information. Classical vSLAM techniques can be divided into Traditional vSLAM and Semantic vSLAM [[119]](#bookmark141)

> 1. _视觉同步定位和绘图_：同步定位和制图(SLAM)是一种技术，可以确定移动机器人在未知环境中的位置，同时构建该环境的地图基于距离的 SLAM 使用车载摄像头捕捉帧并构建环境表示。它因其低硬件成本、小规模场景中的高精度以及捕获丰富环境信息的能力而广受欢迎。经典的 vSLAM 技术可分为传统 vSLAM 和语义 vSLAM

Traditional vSLAM systems estimate the robot's pose in an unknown environment using image information and multi­view geometry principles to construct a low-level map (e.g., sparse maps, semi-dense maps, and dense maps) composed of point clouds, such as filter-based methods (e.g., MonoSLAM [[75]](#bookmark97) MSCKF [[76]),](#_bookmark98) keyframe-based methods (e.g., PTAM [[77]](#bookmark99) ORB-SLAM [[78]),](#_bookmark100) and direct tracking methods (e.g., DTAM [[79]](#bookmark101) LSD-SLAM [[80]](#bookmark102) Since point clouds in low-level maps do not correspond directly to objects in the en­vironment, making them difficult for embodied robots to in­terpret and utilize. However, the advent of semantic concepts, particularly semantic vSLAM systems integrated with seman­tic information solutions, has significantly improved robots' ability to perceive and navigate unexplored environments.

> 传统的 vSLAM 系统使用图像信息和多视图几何原理来估计机器人在未知环境中的姿态，以构建由点云组成的低级地图(例如稀疏地图、半密集地图和密集地图)，例如基于滤波器的方法(例如 MonoSLAM MSCKF ORB-SLAM LSD-SLAM 由于低级地图中的点云与环境中的对象不直接对应，这使得实体机器人难以解释和利用它们。然而，语义概念的出现，特别是与语义信息解决方案集成的语义 vSLAM 系统，显著提高了机器人感知和导航未开发环境的能力。

Early works, such as SLAM++ [[81]](#bookmark103) use real-time 3D object recognition and tracking to create efficient object graphs, enabling robust loop closure, relocalization, and object detection in cluttered environments. CubeSLAM [[82]](#bookmark104) and HDP-SLAM [[83]](#bookmark105) introduce 3-D rectangular into the map to construct a lightweight semantic map. QuadricSLAM [[84]](#bookmark106) employs semantic 3D ellipsoids to achieve precise modeling of object shapes and poses in complex geometrical environments. So-SLAM [[85]](#bookmark107) incorporates fully coupled spatial structure constraints (coplanarity, collinearity, and proximity) in indoor environments. To meet the challenges of dynamic environ­ments, DS-SLAM [[86]](#bookmark108) DynaSLAM [[87]](#bookmark109) and SG-SLAM [[88]](#bookmark110) employ semantic segmentation for motion consistency checks and multiview geometry algorithms to identify and filter dynamic objects, ensuring stable localization and mapping. OVD-SLAM [[89]](#bookmark111) leverages semantic, depth, and optical flow information to distinguish dynamic regions without predefined labels, achieving more accurate and robust localization. GS-SLAM [[90]](#bookmark112) utilizes 3D Gaussian representation that balances efficiency and accuracy through a real-time differentiable splatting rendering pipeline and adaptive expansion strategy.

> 早期的作品，如 SLAM++，使用实时 3D 对象识别和跟踪来创建高效的对象图，从而在混乱的环境中实现鲁棒的循环闭合、重新定位和对象检测。CubeSLAM 和 HDP-SLAM 将三维矩形引入到地图中，以构建轻量级语义地图。QuadricSLAM 采用语义 3D 椭球体来实现复杂几何环境中物体形状和姿态的精确建模。因此，SLAM 在室内环境中引入了完全耦合的空间结构约束(共面性、共线性和邻近性)。为了应对动态环境的挑战，DS-SLAM DynaSLAM 和 SG-SLAM 采用语义分割进行运动一致性检查，并采用多视图几何算法来识别和过滤动态对象，确保稳定的定位和映射。OVD-SLAM 利用语义、深度和光流信息来区分没有预定义标签的动态区域，从而实现更准确、更稳健的定位。GS-SLAM 利用 3D 高斯表示，通过实时可微分的飞溅渲染管道和自适应扩展策略来平衡效率和准确性。

2.  _3D Scene Understanding:_ 3D scene understanding aims to distinguish objects' semantics, identify their locations, and infer the geometric attributes from 3D scene data, which is fundamental in autonomous driving [[126]](#bookmark147) robot navigation [[127]](#bookmark148) and human-computer interaction [[128]](#bookmark149) etc. A scene may be recorded as 3D point clouds using 3D scanning tools like LiDAR or RGB-D sensors. Unlike images, point clouds are sparse, disordered, and irregular, [[120]](#bookmark142) makes scene interpretation extremely challenging.

> 2. _3D 场景理解_：3D 场景理解旨在区分对象的语义，识别它们的位置，并从 3D 场景数据中推断几何属性，这是自动驾驶机器人导航和人机交互等的基础。场景可以使用 LiDAR 或 RGB-D 传感器等 3D 扫描工具记录为 3D 点云。与图像不同，点云是稀疏、无序和不规则的，使场景解释极具挑战性。

In recent years, numerous deep learning methods for 3D scene understanding have been proposed, which can be divided into projection-based, voxel-based, and point-based meth­ods. Concretely, projection-based methods (e.g., MV3D [[91]](#bookmark113) PointPillars [[92]](#bookmark114) MVCNN [[93]](#bookmark115) ) project 3D points onto various image planes and employ 2D CNN-based backbones for feature extraction. Voxel-based methods convert point clouds into regular voxel grids to facilitate 3D convolution operations (e.g., VoxNet [[94]](#bookmark116) SSCNet [[95]),](#_bookmark117) and some works improve their efficiency through sparse convolution (e.g., MinkowskiNet [[96]](#bookmark118) SSCNs [[97]](#bookmark119) Embodiedscan [[98]](#bookmark120) In contrast, point-based methods process point clouds directly (e.g., PointNet [[99]](#bookmark121) PointNet++ [[100]](#bookmark122) PointMLP [[101]](#bookmark123) Recently, to achieve model scalability, Transformers-based (e.g., PointTransformer [[102]](#bookmark124) Swin3d [[103]](#bookmark125) PT2 [[104]](#bookmark126) PT3 [[105]](#bookmark127), 3D-VisTA [[106]](#bookmark128) LEO [[107]](#bookmark129) PQ3D [[108])](#_bookmark130) and Mamba-based (e.g., PointMamba [[109]](#bookmark131) PCM [[110]](#bookmark132) Mamba3D [[111])](#_bookmark133) architectures have emerged. It is worth noting that in addition to directly using features from point clouds, PQ3D [[108]](#bookmark130) also seamlessly combines features from multi-view images and voxels to enhance scene understanding capabilities.

> 近年来，用于三维场景理解的深度学习方法层出不穷，可分为基于投影的方法、基于体素的方法和基于点的方法。具体来说，基于投影的方法(如 MV3D PointPillars MVCNN)将三维点投影到不同的图像平面上，并采用基于二维 CNN 的骨干进行特征提取。基于体素的方法将点云转换成规则的体素网格，以方便三维卷积操作(如 VoxNet SSCNet)，有些工作通过稀疏卷积提高了效率(如 MinkowskiNet SSCNs Embodiedscan)、最近，为了实现模型的可扩展性，出现了基于变换器的架构(如 PointTransformer Swin3d PT2 PT3 3D-VisTA LEO PQ3D) 和基于 Mamba 的架构(如 PointMamba PCM Mamba3D)。值得注意的是，除了直接使用点云的特征外，PQ3D 还能无缝结合多视角图像和体素的特征，以增强场景理解能力。

1.  _Active Exploration:_ The previously introduced 3D scene understanding methods endow robots with the ability to per­ceive the environment in a passive manner. In such cases, the perception system's information acquisition and decision-making do not adapt to the evolving scene. However, passive perception serves as a crucial foundation for active exploration. Given that robots are capable of movement and frequent interaction with their surroundings, they should also be able to explore and perceive their environment actively. The rela­tionship between them is shown in Fig. [7.](#_bookmark9) Current methods addressing active perception focus on interacting with the environment [[112]](#bookmark134) [[113]](#bookmark135) or by changing the viewing direction to obtain more visual information [[114]--[117]](#bookmark139)

> 1. _主动探索_：之前介绍的 3D 场景理解方法赋予机器人以被动方式感知环境的能力。在这种情况下，感知系统的信息获取和决策不适应不断变化的场景。然而，被动感知是主动探索的重要基础。鉴于机器人能够移动并与周围环境频繁互动，它们也应该能够主动探索和感知周围的环境。它们之间的关系如图[7]所示。(#bookmark9)当前解决主动感知的方法侧重于与环境交互或通过改变查看方向来获得更多视觉信息

```
[]{#bookmark10 .anchor}TABLE V
[Comparison of Different 3D VG Methods.]{.smallcaps}
```

For example, Pinto et al. [[112]](#bookmark134) proposed a curious robot that learns visual representations through physical interaction with the environment rather than relying solely on category labels in a dataset. To address the challenge of interactive object perception across robots with varying morphologies, Tatiya et al. [[113]](#bookmark135) proposed a multi-stage projection framework that transfers implicit knowledge through learned exploratory interactions, enabling robots to effectively recognize object properties without the need to relearn from scratch. Recog­nizing the challenge of autonomously capturing informative observations, Jayaraman et al. [[114]](#bookmark136) proposed a reinforcement learning method where an agent learns to actively acquire informative visual observations by reducing its uncertainty about unobserved parts of its environment, using recurrent neural networks for the active completion of panoramic scenes and 3D object shapes. NeU-NBV [[115]](#bookmark137) introduced a mapless planning framework that iteratively positions an RGB camera to capture the most informative images of an unknown scene, using a novel uncertainty estimation in image-based neural rendering to guide data collection towards the most uncertain views. Hu et al. [[116]](#bookmark138) developed a robot exploration algorithm that predicts the value of future states using a state value function, combining offline Monte-Carlo training, online Tem­poral Difference adaptation, and an intrinsic reward function based on sensor information coverage. To address the issue of accidental input in open-world environments, Fan et al. [[117]](#bookmark139) treated active recognition as a sequential evidence-gathering process, providing step-by-step uncertainty quantification and reliable prediction under evidence combination theory while effectively characterizing the merit of actions in open-world environments through a specially developed reward function.

> 例如，Pinto 等人提出了一种好奇的机器人，它通过与环境的物理交互来学习视觉表示，而不是仅仅依赖数据集中的类别标签。为了应对不同形态机器人之间的交互式物体感知挑战，Tatiya 等人提出了一种多阶段投影框架，通过学习到的探索性交互传递隐含知识，使机器人能够有效地识别物体属性，而无需从头开始重新学习。认识到自主捕获信息观测的挑战，Jayaraman 等人提出了一种强化学习方法，其中代理(agents)通过减少其环境中未观察到的部分的不确定性来学习主动获取信息视觉观测，使用循环神经网络主动完成全景场景和 3D 对象形状。NeU NBV 引入了一种无地图规划框架，该框架迭代地定位 RGB 相机以捕获未知场景的最有信息量的图像，使用基于图像的神经渲染中的一种新的不确定性估计来引导数据收集到最不确定的视图。Hu 等人开发了一种机器人探索算法，该算法使用状态值函数预测未来状态的值，结合了离线蒙特卡洛训练、在线时间差自适应和基于传感器信息覆盖的内在奖励函数。为了解决开放世界环境中的意外输入问题，Fan 等人将主动识别视为一个连续的证据收集过程，在证据组合理论下提供逐步的不确定性量化和可靠的预测，同时通过专门开发的奖励函数有效地表征开放世界环境下行动的优点。

B. _3D Visual Grounding_

Unlike traditional 2D visual grounding (VG), which oper­ates within the confines of flat images, 3D VG incorporates depth, perspective, and spatial relationships between objects, providing a more robust framework for agents to interact with their environment. The task of 3D VG involves locating objects within a 3D environment using natural language descriptions

> 与在平面图像范围内操作的传统 2D 视觉基础(VG)不同，3D VG 结合了对象之间的深度、视角和空间关系，为代理(agents)与环境交互提供了更强大的框架。3D VG 的任务涉及使用自然语言描述在 3D 环境中定位对象

Description:

> There is a sofa chair near a couch. The sofa chair has a table on each side
>
> Fig. 8. []{#bookmark11 .anchor}The diagram of two-stage (upper) and one-stage (bottom) 3D visual grounding methods [[141]](#bookmark162) (a) shows the example of 3D visual grounding. (b) two-stage method includes Sparse proposals that may overlook the target in the detection stage and Dense proposals that may confuse the matching stage.
>
> \(c\) one-stage methods can progressively select keypoints (blue points _−→_ red points _−→_ green points) with the guidance of the language description.

[[129]](#bookmark150), [[130]](#bookmark151) As summarized in Table [V,](#_bookmark10) recent methodologies in 3D visual grounding can be roughly divided into two categories: two-stage and one-stage methods [[145]](#bookmark166)

> [[129]](#bookmark150)，[[130](#bookmark 151)如表[V](#_bookmark10)所示，3D视觉接地的最新方法大致可分为两类：两阶段方法和一阶段方法[[145]](#bookmark166)

1.  _Two-stage 3D Visual Grounding methods:_ Similar to corresponding 2D tasks [[146]](#bookmark167) early research in 3D grounding predominantly utilized a two-stage detect-then-match pipeline. They initially employ pretrained detector [[147]](#bookmark168) or segmentor [[148]--[150]](#bookmark170) to extract features from numerous object propos­als within a 3D scene, which are then fused with linguistic query features to match the target object. The focus of the two-stage research is mainly on the second stage, such as exploring the correlation between object proposal features and linguistic query features to select the best-matched object. ReferIt3D [[130]](#bookmark151) and TGNN [[131]](#bookmark152) not only learn to match the proposal features with textual embedding but also encode the contextual relationship among the objects via graph neural networks. To enhance 3D visual grounding in free-form descriptions and irregular point cloud, FFL-3DOG [[133]](#bookmark154) utilized a language scene graph for phrase correlations, a multi-level 3D proposal relation graph for enriching visual features, and a description-guided 3D visual graph for encoding global contexts.

> 1. _两阶段 3D 视觉接地方法_：类似于相应的 2D 任务，早期对 3D 接地的研究主要利用了两阶段检测然后匹配的流水线。他们最初使用预训练检测器或分割器从 3D 场景中的众多对象提案中提取特征，然后将其与语言查询特征融合以匹配目标对象。两阶段研究的重点主要集中在第二阶段，例如探索对象建议特征和语言查询特征之间的相关性，以选择最匹配的对象。Refrait3D 和 TGNN 不仅学习将提案特征与文本嵌入相匹配，还通过图神经网络对对象之间的上下文关系进行编码。为了增强自由形式描述和不规则点云的 3D 视觉基础，FFL-3DOG 使用了用于短语相关性的语言场景图、用于丰富视觉特征的多级 3D 提案关系图和用于编码全局上下文的描述引导 3D 视觉图。

Recently, as the transformer architecture has demonstrated outstanding performance in natural language processing [[151]](#bookmark171) [[152]](#bookmark172) and computer vision tasks [[14]](#bookmark37) [[153]](#bookmark173) research has increasingly focused on using transformers for extracting and fusing visual language features in 3D visual grounding tasks. For example, LanguageRefer [[135]](#bookmark156) employed a transformer-based architecture combining 3D spatial embeddings, lan­guage descriptions, and class label embeddings to achieve robust 3D visual grounding. 3DVG-Transformer [[134]](#bookmark155) is a relation-aware visual grounding method for 3D point clouds, featuring a coordinate-guided contextual aggregation module for relation-enhanced proposal generation and a multiplex attention module for cross-modal proposal disambiguation. To enable more fine-grained reasoning of 3D objects and referring expressions, TransRefer3D [[154]](#bookmark174) enhanced cross-modal feature representation using entity-and-relation aware attention, incorporating self-attention, entity-aware attention and relation-aware attention. GPS [[140]](#bookmark161) proposes a unified learning framework to distill knowledge from million-scale 3D vision-language dataset (i.e. SCENEVERSE [[140])](#_bookmark161) by leveraging three levels of contrastive alignment learning and masked language modeling objective learning. Most of the above methods for 3D VG focus on specific viewpoints, but the learned visual-linguistic correspondences may fail when the viewpoint changes. In order to learn more view-robust visual representations, MVT [[137]](#bookmark158) proposed a multi­view transformer that learns view-independent multi-modal representations.To mitigate the limitations of sparse, noisy, and incomplete point clouds, various methods have explored the incorporation of detailed 2D visual features from captured (e.g., SAT [[132]](#bookmark153) or synthesized (e.g., LAR [[136])](#_bookmark157) images to enhance 3D visual grounding tasks.

> 最近，随着变换器架构在自然语言处理和计算机视觉任务采用了一种基于转换器的架构，结合了 3D 空间嵌入、语言描述和类标签嵌入，以实现强大的 3D 视觉基础。3DVG Transformer 是一种用于 3D 点云的关系感知视觉接地方法，其特征在于用于关系增强提案生成的坐标引导上下文聚合模块和用于跨模态提案消歧的多路注意力模块。为了能够对 3D 对象和引用表达式进行更细粒度的推理，TransRefer3D 使用实体和关系感知注意力增强了跨模态特征表示，结合了自我注意力、实体感知注意力和关系感知注意。GPS 提出了一个统一的学习框架，通过利用三个层次的对比对齐学习和掩码语言建模目标学习，从百万级 3D 视觉语言数据集(即 SCENEVERSE) 中提取知识。上述大多数 3D VG 方法都集中在特定的视点上，但当视点发生变化时，学习到的视觉语言对应可能会失败。为了学习更具视图鲁棒性的视觉表示，MVT 提出了一种学习视图无关多模态表示的多视图变换器。为了减轻稀疏、嘈杂和不完整点云的局限性，各种方法探索了从捕获的(例如 SAT 或合成的(例如 LAR 图像中引入详细的 2D 视觉特征，以增强 3D 视觉基础任务。

Existing 3D VG methods often rely on extensive labeled data for training or show limitations in processing complex language queries. Inspired by the impressive language under­standing capabilities of LLMs, LLM-Grounder [[138]](#bookmark159) proposed an open vocabulary 3D visual grounding pipeline that requires no labeled data, leveraging LLM to decompose queries and generate plans for object identification, followed by evaluating spatial and commonsense relations to select the best matching object. To capture view-dependent queries and decipher spatial relations in 3D space, ZSVG3D [[139]](#bookmark160) designed a zero-shot open-vocabulary 3D visual grounding method that uses LLM to identify relevant objects and perform reasoning, transform­ing this process into a scripted visual program and then into executable Python code to predict object locations.

> 现有的 3D VG 方法通常依赖于大量的标记数据进行训练，或者在处理复杂的语言查询时显示出局限性。受 LLM 令人印象深刻的语言理解能力的启发，LLM Grounder 提出了一种不需要标记数据的开放词汇 3D 视觉接地管道，利用 LLM 分解查询并生成对象识别计划，然后评估空间和常识关系以选择最佳匹配的对象。为了捕捉与视图相关的查询并解读 3D 空间中的空间关系，ZSVG3D 设计了一种零样本开放词汇 3D 视觉基础方法，该方法使用 LLM 来识别相关对象并进行推理，将该过程转换为脚本可视化程序，然后转换为可执行的 Python 代码来预测对象位置。

However, as shown in Fig. [8](#_bookmark11) (b), these two-stage methods face the dilemma of determining the number of proposals because the 3D detectors in the first stage require sampling keypoints to represent the entire 3D scene and generate corre­sponding proposals for each keypoint. Sparse proposals may overlook targets in the first stage, making them unmatchable in the second stage. Conversely, dense proposals may contain inevitable redundant objects, leading to difficulties in distin­guishing targets in the second stage due to overly complex inter-proposal relationships. Moreover, the keypoint sampling strategy is language-agnostic, which increases the difficulty for detectors to identify language-related proposals.

> 然而，如图[8](b)所示，这些两阶段方法面临着确定提案数量的困境，因为第一阶段的 3D 探测器需要采样关键点来表示整个 3D 场景，并为每个关键点生成相应的提案。稀疏的提案可能会忽略第一阶段的目标，使其在第二阶段无法匹配。相反，密集的提案可能包含不可避免的冗余对象，由于提案间关系过于复杂，导致在第二阶段难以区分目标。此外，关键点采样策略与语言无关，这增加了检测器识别语言相关提案的难度。

2.  _One-stage 3D Visual Grounding methods:_ In Fig. [8](#_bookmark11) (c), in contrast to two-stage 3D VG methods, one-stage 3D VG methods integrate object detection and feature extraction guided by language queries, making it easier to locate objects. 3D-SPS [[141]](#bookmark162) took the 3D VG task as a keypoint selection problem and avoided the separation of detection and match­ing. Specifically, 3D-SPS initially coarsely samples language-related keypoints through the description-aware keypoint sam­pling module. Subsequently, it finely selects target keypoints and predicts the foundation using the goal-oriented progressive mining module. Inspired by the 2D image language pre-train model such as MDETR [[155]](#bookmark175) and GLIP [[156]](#bookmark176) BUTD-DETR [[142]](#bookmark163) proposed a bottom-up top-down detection transformer that can be used for 2D and 3D VG. Concretely, BUTD-DETR utilizes labeled bottom-up box proposals and top-down language descriptions to guide the decoding of target objects and corresponding language spans through the prediction head.

> 2. _一阶段 3D 视觉接地方法_：在图[8](c)中，与两阶段 3D VG 方法相比，一阶段 3D VGs 方法集成了语言查询引导的对象检测和特征提取，使定位对象更容易。3D-SPS 将 3D VG 任务视为关键点选择问题，避免了检测和匹配的分离。具体来说，3D-SPS 最初通过描述感知关键点采样模块对语言相关关键点进行粗略采样。随后，它精细地选择目标关键点，并使用面向目标的渐进式挖掘模块预测基础。受 2D 图像语言预训练模型的启发，如 MDETR 和 GLIP，BUTD-DER 提出了一种自下而上的自上而下检测变换器，可用于 2D 和 3D VG。具体来说，BUTD-ETR 利用标记的自下而上的框建议和自上而下的语言描述来指导目标对象和相应语言跨度的解码通过预测头。

```
> Fig. 9. (a) Overview of VLN. The embodied agent communicates with humans through natural language. Humans issue instructions to the embodied agent, who completes tasks such as planning and dialog. Subsequently, through collaborative cooperation or the embodied agent's independent actions, actions are made in interactive or non-interactive environments based on visual observations and instructions, (b) Different tasks of VLN.

> 图 9 (a) VLN 概述。实体代理通过自然语言与人类交流。人类向具身代理发出指令，由具身代理完成规划和对话等任务。随后，通过协同合作或化身代理的独立行动，根据视觉观察和指令在交互式或非交互式环境中采取行动。
```

However, these methods either extract sentence-level fea­tures that couple all words or focus more on object names in the description, which would lose the word-level information or neglect other attributes. To address these issues, EDA [[143]](#bookmark164) explicitly decoupled the textual attributes in a sentence and conducted dense alignment between fine-grained language and point cloud objects. Firstly, the long text is decoupled into five semantic components, including main object, auxiliary object, attributes, pronoun, and relation. Subsequently, the dense alignment is designed to align all object-related decoupled textual semantic components with visual features. To reason human intentions from implicit instructions, ReGround3D

> 然而，这些方法要么提取连接所有单词的句子级特征，要么更多地关注描述中的对象名称，这将丢失单词级信息或忽略其他属性。为了解决这些问题，EDA 明确地解耦了句子中的文本属性，并在细粒度语言和点云对象之间进行了密集的对齐。首先，将长文本分解为五个语义成分，包括主宾语、辅宾语、定语、代词和关系。随后，密集对齐被设计为将所有与对象相关的解耦文本语义组件与视觉特征对齐。为了从隐含指令中推断人类意图，ReGround3D

[[144]](#bookmark165) designed a visual-centric reasoning module, powered by a MLMs, and a 3D grounding module that accurately obtains object locations by revisiting enhanced geometry and fine-grained details from 3D scenes. Additionally, a Chain-of-Grounding mechanism is employed to improve 3D reasoning grounding through interleaved reasoning and grounding steps.

> 设计了一个以视觉为中心的推理模块，由 MLM 提供支持，以及一个 3D 基础模块，该模块通过重新访问 3D 场景中的增强几何和细粒度细节来准确获得对象位置。此外，采用接地链机制，通过交错推理和接地步骤来改善 3D 推理接地。

C. _Visual Language Navigation_

Visual Language Navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navi­gate in unseen environments following linguistic instructions. VLN requires robots to understand complex and diverse visual observations and meanwhile interpret instructions at different granularities. The input for VLN typically consists of two parts: visual information and natural language instructions. The visual information can either be a video of past trajectories or a set of historical-current observation images. The natural language instructions include the target that the embodied agent needs to reach or the task that the embodied agent is expected to complete. The embodied agent must use the above information to select one or a series of actions from a list of candidates to fulfill the requirements of the natural language instructions. This process could be represented as:

> 视觉语言导航(VLN)是嵌入式人工智能的一个关键研究问题，旨在使智能体能够按照语言指令在看不见的环境中导航。VLN 要求机器人理解复杂多样的视觉观察，同时解释不同粒度的指令。VLN 的输入通常由两部分组成：视觉信息和自然语言指令。视觉信息可以是过去轨迹的视频，也可以是一组历史当前观测图像。自然语言指令包括体现代理(agents)需要达到的目标或体现代理(agents)预期完成的任务。所体现的代理(agents)必须使用上述信息从候选列表中选择一个或一系列动作，以满足自然语言指令的要求。这一过程可以表示为：

```
_Action_ = _M_(_O, H, I_) (1)
```

where _Action_ is the chosen action or a list of action can­didates, _O_ is the current observation, _H_ is the historical information, and _I_ is the natural language instruction.

> 其中 _Action_ 是所选动作或可执行动作列表，_O_ 是当前观察值，_H_ 是历史信息，_I_ 是自然语言指令。

**SR** (Success Rate), **TL** (Trajectory Length), and **SPL**

(Success Weighted by Path Length) are the most commonly

used metrics in VLN. Among them, SR directly reflects the navigation performance of the embodied agent, TL reflects the navigation efficiency, and SPL combines both to indicate the overall performance of the embodied agent. Below, we introduce VLN in two parts: datasets and methods.

> VLN 中使用的度量。其中，SR 直接反映了体现代理(agents)的导航性能，TL 反映了导航效率，SPL 将两者结合起来表示体现代理(agents)的整体性能。下面，我们分两部分介绍 VLN：数据集和方法。

1.  _Datasets:_ In VLN, natural language instructions can be a series of detailed action descriptions, a fully described goal, or just a roughly described task, even only the demands of human. The tasks that embodied agents need to complete maybe just a single navigation, or navigation with interaction, or multiple navigation tasks that need to be completed in sequence. These differences bring different challenges to VLN, and many different datasets have been built. Based on these differences, we introduce some important VLN datasets.

> 1. _数据集_：在 VLN 中，自然语言指令可以是一系列详细的动作描述、一个完整描述的目标，或者只是一个粗略描述的任务，甚至只是人类的需求。体现代理(agents)需要完成的任务可能只是单个导航，或具有交互的导航，或需要按顺序完成的多个导航任务。这些差异给 VLN 带来了不同的挑战，并且已经构建了许多不同的数据集。基于这些差异，我们介绍了一些重要的 VLN 数据集。

Room to Room (R2R) [[63]](#bookmark85) is a VLN dataset based on Matterport3D. In R2R, embodied agents navigate according to step-by-step instructions, choosing the next adjacent nav­igation graph node to advance based on visual observations until they reach the target location. Embodied agents need to dynamically track progress to align the navigation process with fine-grained instructions. Room-for-Room [[157]](#bookmark177) extends the paths in R2R to longer trajectories, which requires stronger long-distance instruction and history alignment capabilities of embodied agents. VLN-CE [[158]](#bookmark178) extends R2R and R4R to continuous environments, embodied agents can move freely in the scene. This makes the action decision of embodied agents more difficult. Different from the above datasets based on indoor scenes, TOUCHDOWN dataset [[159]](#bookmark179) is created based on Google Street View. In TOUCHDOWN, embodied agents follow instructions to navigate in the street view rendering simulation of New York City to find the specified object.

> 房间到房间(R2R)是一个基于 Matterport3D 的 VLN 数据集。在 R2R 中，实体代理(agents)根据分步指令进行导航，根据视觉观察选择下一个相邻的导航图节点前进，直到到达目标位置。实体代理(agents)需要动态跟踪进度，以使导航过程与细粒度指令保持一致。Room for Room 将 R2R 中的路径扩展到更长的轨迹，这需要体现代理(agents)更强的远程指令和历史对齐能力。VLN-CE 将 R2R 和 R4R 扩展到连续环境中，实体代理(agents)可以在场景中自由移动。这使得化身代理(agents)的行动决策变得更加困难。与上述基于室内场景的数据集不同，TOUCHDOWN 数据集是基于谷歌街景创建的。在 TOUCHDOWN 中，实体代理(agents)按照指示在纽约市的街景渲染模拟中导航，以找到指定的对象。

Similar to R2R, the REVERIE dataset [[160]](#bookmark180) is also built based on the Matterport3D simulator. REVERIE requires embodied agents to accurately locate the distant invisible target object specified by concise, human-annotated high-level natural language instructions, which means that embodied agents need to find the target object among a large number of objects in the scene. In SOON [[161]](#bookmark181) agents receive a long and complex instruction from coarse to fine to find the target object in the 3D environment. During navigation, agents first search a larger area, and then gradually narrow the search range according to the visual scene and instructions. This makes

> 与 R2R 类似，REVERIE 数据集也是基于 Matterport3D 模拟器构建的。REVERIE 要求实体代理(agents)准确定位由简洁的、人类注释的高级自然语言指令指定的遥远的不可见目标对象，这意味着实体代理(agents)需要在场景中的大量对象中找到目标对象。在 SOON 中，代理(agents)接收从粗到细的长而复杂的指令，以在 3D 环境中找到目标对象。在导航过程中，代理(agents)首先搜索更大的区域，然后根据视觉场景和指令逐渐缩小搜索范围。这使得

```
TABLE VI
```

SOON's navigation target-oriented and independent of the initial position. DDN [[162]](#bookmark182) moves a step further beyond these datasets, only providing human demands without specifying explicit objects. The agent needs to navigate through the scene to find objects that meet human needs.

> SOON 的导航目标与初始位置无关。DDN 进一步超越了这些数据集，只提供人类需求，而不指定明确的对象。代理(agents)需要在场景中导航，以找到满足人类需求的对象。

ALFRED dataset [[163]](#bookmark183) is based on the AI2-THOR sim­ulator. In ALFRED, embodied agents need to understand environmental observations and complete household tasks in an interactive environment according to coarse-grained and fine-grained instructions. The task of OVMM [[164]](#bookmark184) is to pick any object in any unseen environment and place it in a specified location. Agents need to locate the target object in the home environment, navigate and grab it, and then navigate to the target location to put down the object. OVMM provides a simulation based on Habitat and a framework for implemen­tation in the real world. Behavior-1K dataset [[165]](#bookmark196) is based on human needs, comprising 1,000 long-sequence, complex, skill-dependent daily tasks, which are designed within OmniGibson, an extension of the iGibson simulation environment. Agents need to complete long-span navigation-interaction tasks which contain thousands of low-level action steps based on visual information and language instructions. These complex tasks requires strong capabilities of understanding and memory.

> ALFRED 数据集基于 AI2-THOR 模拟器。在 ALFRED 中，具身代理(agents)需要理解环境观察结果，并根据粗粒度和细粒度指令在交互式环境中完成家务。OVMM 的任务是在任何看不见的环境中拾取任何对象并将其放置在指定位置。代理(agents)需要在家庭环境中定位目标对象，导航并抓取它，然后导航到目标位置放下对象。OVMM 提供了一个基于 Habitat 的模拟和一个在现实世界中实施的框架。Behavior-1K 数据集基于人类需求，包括 1000 个长序列、复杂、依赖技能的日常任务，这些任务是在 iGibson 模拟环境的扩展 OmniGibson 中设计的。代理(agents)需要完成大跨度的导航交互任务，其中包含基于视觉信息和语言指令的数千个低级动作步骤。这些复杂的任务需要强大的理解和记忆能力。

There are also some more special datasets. CVDN [[166]](#bookmark198) requires embodied agents to navigate to the target based on dialogue history, and ask questions for help to decide the next action when uncertain. DialFRED [[167]](#bookmark199) an extension of ALFRED, allows agents to ask questions during the navigation and interaction process to get help. These datasets all introduce additional oracles, and embodied agents need to obtain more information beneficial to navigation by asking questions.

> 还有一些更特殊的数据集。CVDN 要求实体代理(agents)根据对话历史导航到目标，并在不确定时询问问题以帮助决定下一步行动。DialFRED 是 ALFRED 的扩展，允许代理(agents)在导航和交互过程中提问以获得帮助。这些数据集都引入了额外的神谕，实体代理(agents)需要通过提问来获取更多有利于导航的信息。

2.  _Method:_ VLN has made great strides recently with the astonishing performance of LLMs, the direction and focus of VLN have been profoundly influenced. Nevertheless, the VLN methods can be divided into two directions: **Memory-Understanding Based** and **Future-Prediction Based**.

> 2. 方法：近年来，随着 LLM 的惊人性能，VLN 取得了长足的进步，VLN 的方向和重点受到了深刻的影响。然而，VLN 方法可以分为两个方向：**基于记忆理解的**和**基于未来预测的**。

Memory-Understanding based methods focus on the per­ception and understanding of the environment, as well as model design based on historical observations or trajectories, which is a method based on past learning. Future-Prediction based methods pay more attention to modeling, predicting, and understanding the future state, which is a method for future learning. Since VLN can be regarded as a partially observable Markov decision process, where future observations depend on the current environment and actions of the intelligent agent, historical information has important significance for navi­gation decisions, especially long-span navigation decisions, hence Memory-Understanding based methods have always been the mainstream of VLN. However, Future-Prediction based methods still have important significance. Its essential understanding of the environment has great value in VLN in continuous environments, especially with the rise of the concept of world model, Future-Prediction based methods are receiving more and more attention from researchers.

> 基于记忆理解的方法侧重于对环境的感知和理解，以及基于历史观察或轨迹的模型设计，这是一种基于过去学习的方法。基于未来预测的方法更注重对未来状态的建模、预测和理解，这是一种未来学习的方法。由于 VLN 可以被视为一个部分可观测的马尔可夫决策过程，其中未来的观测取决于智能代理(agents)的当前环境和行为，历史信息对导航决策，特别是大跨度导航决策具有重要意义，因此基于记忆理解的方法一直是 VLN 的主流。然而，基于未来预测的方法仍然具有重要意义。它对环境的本质理解在连续环境下的 VLN 中具有重要价值，特别是随着世界模型概念的兴起，基于未来预测的方法越来越受到研究人员的关注。

```
TABLE VII
[Comparison of VLN methods.]{.smallcaps}
```

**Memory-Understanding based.** Graph-based learning is an essential part of the memory-understanding based method. Graph-based learning usually represents the navigation process in the form of a graph, where the information obtained by the embodied agent at each time step is encoded as nodes of the graph. Embodied agent obtains global or partial navi­gation graph information as a representation of the historical trajectory. LVERG [[168]](#bookmark200) encoded the language information and visual information of each node separately, design a new language and visual entity relationship graph to model the inter-modal relationship between text and vision, and the intra-modal relationship between visual entities. LM-Nav [[172]](#bookmark204) used a goal-conditioned distance function to infer connections between original observation sets and construct a navigation graph, and extracts landmarks from the instructions through a LLM, uses a visual language model to match them with the nodes of the navigation graph. Although HOP [[173]](#bookmark205) is not based on graph learning, its method is similar to the graph, requiring the model to model time-ordered information at different granularities, thereby achieving a deep understanding of historical trajectories and memories.

> **基于记忆理解的学习。**基于图的学习是基于记忆理解方法的重要组成部分。基于图的学习通常以图的形式表示导航过程，其中由体现的代理(agents)在每个时间步获得的信息被编码为图的节点。隐含代理(agents)获取全局或部分导航图信息，作为历史轨迹的表示。LVERG 分别对每个节点的语言信息和视觉信息进行编码，设计一个新的语言和视觉实体关系图，对文本和视觉之间的模态间关系以及视觉实体之间的模态内关系进行建模。LM Nav 使用目标条件距离函数来推断原始观测集之间的连接并构建导航图，并通过 LLM 从指令中提取地标，使用视觉语言模型将其与导航图的节点进行匹配。尽管 HOP 不是基于图学习的，但它的方法类似于图，要求模型以不同的粒度对时间顺序信息进行建模，从而实现对历史轨迹和记忆的深入理解。

The navigation graph discretizes the environment, but con­currently understanding and encoding the environment is also important. FILM [[171]](#bookmark203) used RGB-D observations and seman­tic segmentation to gradually build a semantic map from 3D voxels during the navigation. VER [[178]](#bookmark187) quantified the phys­ical world into structured 3D units through 2D-3D sampling, providing fine-grained geometric details and semantics.

> 导航图将环境离散化，但目前理解和编码环境也很重要。FILM 使用 RGB-D 观察和语义分割，在导航过程中从 3D 体素逐步构建语义图。VER 通过 2D-3D 采样将物理世界量化为结构化的 3D 单元，提供细粒度的几何细节和语义。

Different learning schemes explore how to utilize historical trajectories and memories better. Through adversarial learning, CMG [[169]](#bookmark201) alternated between imitation learning and explo­ration encouragement schemes, effectively strengthening the understanding of instructions and historical trajectories, short­ening the difference between training and inference. GOAT [[177]](#bookmark186) directly trained unbiased models through Backdoor Ad­justment Causal Learning (BACL) and Frontdoor Adjustment Causal Learning (FACL), conducts contrastive learning with vision, navigation history, and their combination to instruc­tions, enabling the agent to make fuller use of information. The enhanced cross-modal matching method proposed by RCM [[170]](#bookmark202) used goal-oriented external rewards and instruction-oriented internal rewards to perform cross-modal grounding globally and locally and learns from its own historical good decisions through self-supervised imitation learning. FSTT [[175]](#bookmark207) introduced TTA into VLN and optimizes the model in terms of gradients and model parameters at two scales of time steps and tasks, effectively improving model performance.

> 不同的学习方案探索如何更好地利用历史轨迹和记忆。通过对抗性学习，CMG 在模仿学习和探究鼓励方案之间交替，有效地加强了对指令和历史轨迹的理解，缩短了训练和推理之间的差异。GOAT 通过后门调整因果学习(BACL)和前门调整因果学习。RCM 提出的增强型跨模式匹配方法使用面向目标的外部奖励和面向指令的内部奖励在全局和局部进行跨模式接地，并通过自我监督的模仿学习从自己的历史良好决策中学习。FSTT 将 TTA 引入 VLN，并在时间步长和任务的两个尺度上根据梯度和模型参数优化模型，有效地提高了模型性能。

The specific application of large models in Memory-Understanding based methods is to understand the representa­tion of historical memory and to understand the environment and tasks based on its extensive world knowledge. NaviLLM [[174]](#bookmark206) integrated the historical observation sequence into the embedding space through the visual encoder, inputs the multi­modal information of the fusion encoding into the LLM and fine-tunes it, reaching the state-of-the-art on multiple benchmarks. NaVid [[179]](#bookmark188) maked improvements in the en­coding of historical information, achieves different degrees of information retention on historical observations and current observations through different degrees of pooling. DiscussNav [[176]](#bookmark185) assigned large model experts with different abilities to different roles, drives the large models to discuss before navi­gation actions to complete navigation decisions, and achieves excellent performance in zero-shot VLN.

> 大型模型在基于记忆理解的方法中的具体应用是理解历史记忆的表示，并根据其广泛的世界知识理解环境和任务。NaviLLM 通过视觉编码器将历史观测序列集成到嵌入空间中，将融合编码的多模态信息输入 LLM 并对其进行微调，在多个基准上达到最先进水平。NaVid 改进了历史信息的编码，通过不同程度的池化实现了历史观测和当前观测的不同程度的信息保留。DiscussNav 将具有不同能力的大型模型专家分配到不同的角色，推动大型模型在导航前进行讨论，以完成导航决策，并在零样本 VLN 中取得优异的性能。

**Future-Prediction Based.** Graph-based learning is also widely used in Future-Prediction based methods. BGBL [[182]](#bookmark191) and ETPNav [[185]](#bookmark194) used a similar method to design a waypoint predictor that can predict movable path points in a continuous environment based on the observation of the current navigation graph node. They aim to migrate complex navigation in a continuous environment to node-to-node navigation in a discrete environment, thereby bridging the performance gap from discrete environments to continuous environments.

> **基于未来预测。**基于图的学习也广泛应用于基于未来预测的方法中。BGBL 和 ETPNav 使用类似的方法设计了一个航路点预测器，该预测器可以基于对当前导航图节点的观察来预测连续环境中的可移动路径点。他们的目标是将连续环境中的复杂导航迁移到离散环境中的节点到节点导航，从而弥合从离散环境到连续环境的性能差距。

Improving the understanding and perception of the future environment through environmental encoding is also one of the research directions for predicting and exploring the future. NvEM [[181]](#bookmark190) used a theme module and a reference module to perform fusion encoding of neighbor views from the global and local perspectives. This is actually an understanding and learning of future observations. HNR [[184]](#bookmark193) used a large-scale pre-trained hierarchical neural radiation representation model to directly predict the visual representation of the future environment rather than pixel-level images using three-dimensional feature space encoding, and builds a navigable future path tree based on the representation of the future envi­ronment. They predict the future environment from different levels, providing effective references for navigation decisions. Some reinforcement learning methods are also applied to predict and explore future states. LookBY [[180]](#bookmark189) employed reinforcement prediction to enable the prediction module to imitate the world and forecast future states and rewards. This allows the agent to directly map "current observations" and "predictions of future observations" to actions, achieving state-of-the-art performance at the time. The rich world knowledge and zero-shot performance of large models provide many possibilities for Future-Prediction based methods. MiC [[183]](#bookmark192) required the LLM to directly predict the target and its possible location from the instructions and provides navigation instruc­tions through the description of scene perception. This method requires LLMs to fully exert its 'imagination' and build an imagined scene through prompts.

> 通过环境编码提高对未来环境的理解和感知也是预测和探索未来的研究方向之一。NvEM 使用主题模块和参考模块对来自全局和局部视角的相邻视图进行融合编码。这实际上是对未来观测的理解和学习。HNR 使用大规模预训练的分层神经辐射表示模型直接预测未来环境的视觉表示，而不是使用三维特征空间编码的像素级图像，并基于未来环境的表示构建可导航的未来路径树。它们从不同层面预测未来的环境，为导航决策提供有效的参考。
> 一些强化学习方法也被应用于预测和探索未来的状态。LookBY 采用强化预测，使预测模块能够模拟世界并预测未来的状态和奖励。这允许代理(agents)将“当前观测”和“未来观测的预测”直接映射到行动，从而在当时实现最先进的性能。丰富的世界知识和大型模型的零样本性能为基于未来预测的方法提供了许多可能性。MiC 要求 LLM 根据指令直接预测目标及其可能的位置，并通过描述场景感知提供导航指令。这种方法要求 LLM 充分发挥其“想象力”，通过提示构建想象中的场景。

In addition, there are some methods that both learn from the past and for the future. MCR-Agent [[186]](#bookmark195) designed a three-layer action strategy, which requires the model to predict the target from the instructions, predict the pixel-level mask for the target to be interact, and learn from the previous navigation decision; OVLM [[187]](#bookmark197) required the LLMs to predict the corresponding operations and landmark sequences for the instructions. During the navigation process, the visual language map will be continuously updated and maintained, and the operations will be linked to the waypoints on the map.

> 此外，还有一些方法**既可以从过去学习，也可以为未来学习**。MCR Agent 设计了一种三层动作策略，该策略要求模型根据指令预测目标，预测要交互的目标的像素级掩码，并从之前的导航决策中学习；OVLM 要求 LLM 预测指令的相应操作和地标序列。在导航过程中，视觉语言地图将不断更新和维护，操作将链接到地图上的航路点。

D. _Non-Visual Perception: Tactile_

Tactile sensors provide agents with detailed information such as texture, hardness and temperature. For the same action, the knowledge learned from vision and tactile sensors may be related and complementary, allowing robots to fully grasp the high-precision tasks in hand. Tactile perception is therefore vi­tal for agents in the physical world and undoubtedly enhances human-computer interaction [[188]--[190]](#bookmark210)

> 触觉传感器为试剂提供纹理、硬度和温度等详细信息。对于相同的动作，从视觉和触觉传感器学到的知识可能是相互关联和互补的，使机器人能够完全掌握手中的高精度任务。因此，触觉感知对物理世界中的智能体至关重要，无疑增强了人机交互

For tactile perception tasks, the agent needs to gather tactile information from the physical world and then perform complex tasks. In this section, as shown in Fig. 10, we first introduce the existing types of tactile sensors and their datasets, and then discuss three main tasks in tactile perception: estimation, recognition, and manipulation.

> 对于触觉感知任务，智能体需要从物理世界收集触觉信息，然后执行复杂的任务。在本节中，如图 10 所示，我们首先介绍了现有类型的触觉传感器及其数据集，然后讨论了触觉感知中的三个主要任务：估计、识别和操纵。

1.  _Sensor Design:_ The principle of human tactile is that the skin changes shape when touched, and its abundant nerve cells send electrical signals, which also serves as the basis for designing tactile sensors. Tactile sensor design methods can be divided into three categories: non-vision-based, vision-based, and multi-modal. Non-vision-based tactile sensors, primarily using electrical and mechanical principles, chiefly register fundamental, low-dimensional sensory outputs such as force, pressure, vibration, and temperature [[191]--[196]](#bookmark212) One of the notable representatives is BioTac [[197]](#bookmark213) and its simulator [[198]](#bookmark214) Vision-based tactile sensors are based on optical principles. Using images of the gel's deformation as tactile information, vision-based tactile sensors such as GelSight [[199]](#bookmark215) Gelslim [[200]](#bookmark216) DIGIT [[201]](#bookmark217) 9DTact [[202]](#bookmark218) TacTip [[203]](#bookmark219) GelTip [[204]](#bookmark220) and AllSight [[205]](#bookmark221) have been used for numerous applications. Simulations like TACTO [[206]](#bookmark222) and Taxim [[207]](#bookmark223) are also popular. Recent work has focused on cost reduction [[202]](#bookmark218) and integration into robotic hands [[201]](#bookmark217) [[208]](#bookmark224) [[209]](#bookmark225) Multi­modal tactile sensors, inspired by human skin, combine multi­modal information like pressure, proximity, acceleration, and temperature, using flexible materials and modular design.

> 1. 传感器设计：人体触觉的原理是，皮肤在被触摸时会改变形状，其丰富的神经细胞会发送电信号，这也是设计触觉传感器的基础。触觉传感器设计方法可分为三类：非视觉、视觉和多模态。非视觉触觉传感器主要使用电气和机械原理，主要记录基本的低维感官输出，如力、压力、振动和温度其中一个著名的代表是 BioTac 及其模拟器视觉触觉传感器基于光学原理。使用凝胶变形的图像作为触觉信息，基于视觉的触觉传感器，如 GelSightGelslimDIGIT9DTactTacTipGelTip，已被用于许多应用。TACTO 和 Taxim 等模拟也很受欢迎。最近的工作重点是降低成本和集成到机器人手中多模态触觉传感器，灵感来自人类皮肤，使用柔性材料和模块化设计，结合压力、接近度、加速度和温度等多模态信息。

> Fig. 10. Different types of Tactile Sensors. **Non-vision sensors** (a) mainly use sensors of force, pressure, vibration and temperature to get tactile knowledge. **Vision-based tactile sensors** ((b)-(e)) are based on optical principles. A camera is placed behind the gel to record the image of its deformation, using illumination from light sources at different directions. (a)-(e) are the details from BioTac, Gelsight, DIGIT, 9DTact and Gelsilm.

1.  _Datasets:_ The datasets of non-vision sensors, mainly collected by BioTac series [[197]](#bookmark213) contain electrode values, force vectors and contact location. Since tasks are mainly the estimation of force and grasping details, objects in datasets are usually force and grasping samples. Vision-based sensors, with high-resolution images of deformation gel, focus more on higher estimation, texture recognition and manipulation. The datasets are collected by Geisight sensors, DIGIT sensors and their simulators [[199]](#bookmark215) [[201]](#bookmark217) [[202]](#bookmark218) [[206]](#bookmark222) consisting house­hold object, wildlife environments, different materials and grasping items. Since image information can be easily aligned and bound with the other modalities (images, language, audio, etc) [[14]](#bookmark37) [[210]](#bookmark226) tactile perception in embodied agent mainly revolves around visual-based sensors. We introduce ten main tactile datasets, which are summarized in Table VIII.

> 1. _数据集_：非视觉传感器的数据集，主要由 BioTac 系列收集，包含电极值、力矢量和接触位置。由于任务主要是力的估计和抓取细节，数据集中的对象通常是力和抓取样本。基于视觉的传感器，具有变形凝胶的高分辨率图像，更侧重于更高的估计、纹理识别和操作。这些数据集由 Geisight 传感器、DIGIT 传感器及其模拟器，因此具体代理(agents)中的触觉感知主要围绕基于视觉的传感器展开。我们介绍了表八中总结的十个主要触觉数据集。

2.  _Methods:_ Tactile perception has numerous applications and can be categorized into three types: estimation, precise robotic manipulation, and multi-modal recognition tasks.

> 2. 方法：触觉感知有许多应用，可分为三类：估计、精确的机器人操作和多模态识别任务。

a. _Estimation:_ Early works in estimation predominantly focused on basic algorithm for shape, force and slip measure­ment [[202]](#bookmark218) [[220]](#bookmark235) [[221]](#bookmark237) Researchers simply used a threshold or applied Convolutional Neural Networks (CNN) to settle these task, based on the colors of tactile images and the change of markers distributions at different frames. The emphasis of the estimation work is mainly on the second stage, the generation of tactile image and the reconstruction of the object. The generation of tactile image [[222]--[225]](#bookmark241) aims at generating tactile image from vision data. At first it applied deep learning model that takes RGB-D images as input and outputted tactile images [[222]](#bookmark238) [[223]](#bookmark239) Recently, as the image generation developed fast, Higuera et al. [[224](#_bookmark240)] and Yang et al. [[225]](#bookmark241) applied diffusion model on tactile generation and it performed well. The reconstruction of the object can be divided into 2D reconstruction [[226]](#bookmark242) [[227]](#bookmark243) and 3D reconstruction [[202]](#bookmark218) [[219]](#bookmark236) [[228]--[241]](#bookmark246) 2D reconstruction mainly focuses on the shape and segmentation of the object while 3D focuses on the surface and pose, and then even the full scene perception. The tasks first adopted mathematics methods, auto-encoder methods and neural network methods to fuse visual (sometime point clouds) and tactile features together. Lately, researchers like Comi et al. [[236]](#bookmark245) and Dou et al. [[219]](#bookmark236) implemented new methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) into tactile reconstruction work.

> a. _估计_：早期的估计工作主要集中在形状、力和滑移测量的基本算法的生成旨在从视觉数据生成触觉图像。起初，它应用了以 RGB-D 图像为输入和输出触觉图像的深度学习模型最近，随着图像生成的快速发展，Higuera 等人和 Yang 等人在**触觉生成中应用了扩散模型**，表现良好。对象的重建可分为 2D 重建和 Dou 等研究人员将基于神经辐射场(NeRF)和 3D 高斯散斑(3DGS)的新方法应用于触觉重建工作中。

b. _Robotic Manipulation:_ In tactile task, bridging the sim-to-real gap is more than important. Reinforcement learn­ing and GAN-based methods have been proposed to address variations in accurate, on-time robotic manipulation tasks.

> b. _反向操纵_：在触觉任务中，将模拟与现实之间的差距连接起来非常重要。已经提出了强化学习和基于 GAN 的方法来解决准确、准时的机器人操纵任务的变化。

**Reinforcement Learning method.** Visuotactile-RL [[242]](#bookmark247) proposed several methods to existing RL methods, including tactile gating, tactile data augmentation and visual degradation. Rotateit [[243]](#bookmark248) is a system that enables fingertip-based object rotation along multiple axes by leveraging multimodal sensory inputs. It trained the network by reinforcement learning poli­cies with privileged information and enabled online inference.

> **强化学习方法** Visuotactic RL 对现有的 RL 方法提出了几种方法，包括触觉门控、触觉数据增强和视觉退化。Rotateit 是一个通过利用多模态感官输入实现基于指尖的对象沿多个轴旋转的系统。它通过使用特权信息的强化学习策略来训练网络，并启用在线推理。

[[244]](#bookmark249) proposed a deep RL approach to object pushing using only tactile perception. It came up with a goalconditioned formulation that allows both model-free and modelbased RL to obtain accurate policies for pushing an object to a goal. Any-Rotate [[245]](#bookmark250) focused on in-hand manipulation. It is a system for gravity-invariant multi-axis in-hand object rotation using dense featured sim-to-real touch, constructing a continuous contact feature representation to provide tactile feedback for training a policy in simulation and introduce an approach to perform zero-shot policy transfer by training an observation model to bridge the sim-to-real gap.

> 提出了一种仅使用触觉感知进行对象推送的深度 RL 方法。它提出了一个目标条件公式，允许无模型和基于模型的 RL 获得将对象推向目标的准确策略。任何专注于手持操作的旋转。这是一个使用密集特征的模拟对现实触摸进行重力变化多轴手上物体旋转的系统，构建连续接触特征表示，为模拟中的策略训练提供触觉反馈，并介绍了一种通过训练观察模型来桥接模拟对现实间隙来执行零样本策略转移的方法。

**GAN-based method.** ACTNet [[246]](#bookmark251) proposed an unsu­pervised adversarial domain adaptation method to narrow the domain gap for pixel-level tactile perception tasks. An adaptively correlative attention mechanism was introduced to improve the generator, which is capable of leveraging global information and focusing on salient regions. However, pixel-level domain adaptation lead to error accumulation, degrade performance, and increased structural complexity and training costs. In comparison, STR-Net [[247]](#bookmark252) proposed a feature-level unsupervised framework for tactile images, narrowing the do­main gap for feature-level tactile perception tasks. Moreover, some methods focused on sim-to-real. For example, the Tactile Gym 2.0 [[248]](#bookmark253) However, due to its complexity and high cost, it is challenging for practical application.

> **基于 GAN 的方法** ACTNet 提出了一种无监督的对抗性域自适应方法，以缩小像素级触觉感知任务的域差距。引入了一种自适应相关注意力机制来改进生成器，该生成器能够利用全局信息并专注于显著区域。然而，像素级域自适应会导致错误累积，降低性能，增加结构复杂性和训练成本。相比之下，STR Net 提出了一种用于触觉图像的特征级无监督框架，缩小了特征级触觉感知任务的主要差距。此外，一些方法侧重于模拟到真实。例如，触觉健身房 2.0 然而，由于其复杂性和高成本，它在实际应用中具有挑战性。

```
TABLE VIII
[Comparison of different tactile datasets.]{.smallcaps}
```

c. _Recognition:_ Tactile representations learning focuses on material classification and multi-modal understanding, which can be divided into two categories: Traditional Methods and LLMs&VLMs Methods.

> c. _识别_：触觉表征学习侧重于材料分类和多模态理解，可分为两类：传统方法和 LLMs&VLMs 方法。

**Traditional Methods.** Various traditional approaches have been employed to enhance tactile representation learning. Au­toencoder frameworks have been instrumental in developing compact tactile data representations. Polic et al. [[249]](#bookmark254) used a convolutional neural network autoencoder for dimensionality reduction of optical-based tactile sensor images. Gao et al.

> **传统方法** 已经采用了各种传统方法来增强触觉表征学习。Autoncoder 框架在开发紧凑的触觉数据表示方面发挥了重要作用。Polic 等人使用卷积神经网络自动编码器对基于光学的触觉传感器图像进行降维。Gao 等人。

[[250]](#bookmark255) created a supervised recurrent autoencoder to handle heterogeneous sensor datasets, while Cao et al. [[251]](#bookmark256) created TacMAE used a masked autoencoder for incomplete tactile data. Zhang et al. [[252]](#bookmark257) introduced MAE4GM, a multimodal autoencoder integrating visuo-tactile data. Since tactile acts as a complement to other modes, Joint Training methods are used to fuse multiple modalities. Yuan et al. [[253]](#bookmark258) trained CNNs with modalities that included depth, vision, and tactile data. Similarly, Lee et al. [[254]](#bookmark259) used a variational Bayesian ap­proach for modalities like force sensors series and end-effector metrics. For better learning representation, Self-supervised methods like contrastive learning are also key techniques in binding modalities together. Researches differ in contrastive methods Lin et al. [[255]](#bookmark260) simply paired tactile inputs with multiple visual inputs and Yang et al. [[256]](#bookmark261) employed visuo­tactile contrastive multiview features. Kerr et al. [[215]](#bookmark231) used InfoNCE loss and Guzey et al. [[257]](#bookmark262) used BYOL. These traditional methods have established a solid foundation for tactile representation learning.

> 创建了一个有监督的循环自动编码器来处理异构传感器数据集，而 Cao 等人创建的 TacMAE 使用了一个掩码自动编码器来存储不完整的触觉数据。Zhang 等人介绍了 MAE4GM，这是一种集成视觉触觉数据的多模态自动编码器。由于触觉是对其他模式的补充，因此联合训练方法被用来融合多种模式。Yuan 等人使用包括深度、视觉和触觉数据在内的模态训练 CNN。同样，Lee 等人对力传感器系列和末端执行器度量等模态使用了变分贝叶斯方法。为了更好地学习表征，对比学习等自我监督方法也是将模态结合在一起的关键技术。Lin 等人简单地将触觉输入与多个视觉输入配对，而 Yang 等人采用了视觉-触觉对比多视图特征。Kerr 等人使用 InfoNCE 损失，Guzey 等人使用 BYOL。这些传统方法为触觉表征学习奠定了坚实的基础。

**LLMs&VLMs Methods.** LLM and VLM have shown an amazing understanding of cross-modal interactions and strong zero-shot performance recently. Recent works from Yang et al. [[189]](#bookmark209) Fu et al. [[218]](#bookmark234) and Yu et al. [[258]](#bookmark263) encoded and aligned tactile data with visual and language modalities by contrastive pretrained method. Then an LLM like LLaMA would be applied, using a fine-tune method to fit tasks like tactile description. The advent of LLM and VLM techniques has further advanced the field, enabling more comprehensive and robust cross-modal tactile representations.

> **LLM 和 VLM 方法** LLM 和 VLM 最近对跨模态相互作用和强大的零样本性能表现出了惊人的理解。Yang 等人 Fu 等人和 Yu 等人的最新研究通过对比预训练方法将触觉数据与视觉和语言模态进行编码和对齐。然后，将应用类似 LLaMA 的 LLM，使用微调方法来适应触觉描述等任务。LLM 和 VLM 技术的出现进一步推动了该领域的发展，实现了更全面、更稳健的跨模态触觉表示。

4.  _Difficulties:_ a) Disadvantages of different sensor types: Traditional sensors deliver simple and low-dimensional data, challenging multi-modal learning. Vision-based sensors and electronic skins, while highly accurate, are expensive. b) Data acquisition challenges: Collecting data, especially both tactile and visual simultaneously, is difficult despite some progress in developing simplified collection devices. c) Inconsistent standards: Tactile sensors operate with inconsistent standards and principles, hindering large-scale learning and limiting the usefulness of public datasets. There is a need for standardized and extensive datasets.

> 4. _难点_：
>    a) 不同传感器类型的缺点：传统传感器提供简单且低维的数据，对多模态学习具有挑战性。基于视觉的传感器和电子皮肤虽然精度很高，但价格昂贵。
>    b) 数据采集挑战：尽管在开发简化的采集设备方面取得了一些进展，但同时收集数据，特别是触觉和视觉数据是困难的。
>    c) 标准不一致：触觉传感器的操作标准和原理不一致，阻碍了大规模学习，限制了公共数据集的有用性。需要标准化和广泛的数据集。

## V. EMBODIED INTERACTION

Embodied interaction tasks refer to scenarios where agents interact with humans and environment in a physical or sim­ulated space. The typical embodied interaction tasks are Em­bodied Question Answering (EQA) and embodied grasping.

> 实体交互任务是指代理(agents)在物理或模拟空间中与人类和环境交互的场景。典型的具身交互任务是具身问答(EQA)和具身抓握。

A. _Embodied Question Answering_

For EQA task, the agent needs to explore the environment from a first-person perspective to gather information necessary to answer the given questions. An agent with autonomous exploration and decision-making capabilities must not only consider which actions to take to explore the environment but also determine when to stop exploring to answer questions. Existing works focus on different types of questions, some of which are shown in Fig. [11.](#_bookmark12) In this section, we will introduce the existing datasets, discuss the related methods, describe the metrics used to evaluate model performance, and address the remaining limitations of this task.

> 对于 EQA 任务，代理(agents)需要从第一人称的角度探索环境，以收集回答给定问题所需的信息。具有自主探索和决策能力的代理(agents)不仅必须考虑采取哪些行动来探索环境，还必须确定何时停止探索以回答问题。现有工作侧重于不同类型的问题，其中一些问题如图[11]所示。在本节中，我们将介绍现有的数据集，讨论相关方法，描述用于评估模型性能的指标，并解决此任务的剩余局限性。

1.  _Datasets:_ Conducting robot experiments in real environ­ments is often constrained by scenarios and robot hardware. As virtual experimental platforms, simulators offer suitable environmental conditions for constructing embodied question answering datasets. Training and testing models on datasets created in simulators significantly reduce experimental costs and enhance the success rate of deploying models on real machines. We briefly introduce several embodied question answering datasets, which are summarized in Table [IX.](#_bookmark13)

> 1. _数据集_：在真实环境中进行机器人实验通常受到场景和机器人硬件的限制。作为虚拟实验平台，模拟器为构建具体的问答数据集提供了合适的环境条件。在模拟器中创建的数据集上训练和测试模型可以显著降低实验成本，并提高在真实机器上部署模型的成功率。我们简要介绍了几个具体的问答数据集，这些数据集在表[IX]中进行了总结

**EQA v1** [[259]](#bookmark264) is the first dataset designed for EQA. Built on synthetic 3D indoor scenes from the SUNCG dataset [[95]](#bookmark117) within the House3D [[269]](#bookmark274) simulator, EQA v1 comprises four types of questions: location, color, color![](./media/image104.png){width="6.917979002624672e-2in" height="6.916010498687664e-3in"}room, and preposi­tion. It features over 5,000 questions distributed across more than 750 environments. The questions are constructed via functional program execution, using templates to select and combine basic operations.

> **EQA v1** 是第一个为 EQA 设计的数据集。EQA v1 基于 House3D 模拟器中 SUNCG 数据集的合成 3D 室内场景构建，包括四种类型的问题：位置、颜色、颜色、房间，以及预处理。它包含分布在 750 多个环境中的 5000 多个问题。这些问题是通过功能程序执行构建的，使用模板来选择和组合基本操作。

Similar to EQA v1, **MT-EQA** [[260]](#bookmark265) is built in House3D using SUNCG by executing functional programs consisting of some basic operations. However, it further extends the single-object question answering task to a multi-object setting. Six types of questions are designed, involving the comparison of color, distance, and size between multiple objects. The dataset contains 19,287 questions in 588 environments.

> 与 EQA v1 类似，**MT-EQA**是在 House3D 中使用 SUNCG 通过执行由一些基本操作组成的功能程序构建的。然而，它进一步将单对象问答任务扩展到多对象设置。设计了六种类型的问题，涉及多个物体之间的颜色、距离和大小的比较。该数据集包含 588 个环境中的 19287 个问题。

> Fig. 11. []{#bookmark12 .anchor}The gray box displays the scenes an agent observes during exploration. The other boxes show various types of question answering tasks. Except for the task of answering questions based on episodic memory, the agent ceases exploration once it has gathered sufficient information to answer the question.

```
[]{#bookmark13 .anchor}TABLE IX
[Comparison of Different EQA Datasets.]{.smallcaps}
```

**MP3D-EQA** [[261]](#bookmark266) is built on a simulator developed based on MINOS [[270]](#bookmark275) using the Matterport3D dataset [[271]](#bookmark276) ex­panding the question-answering task to a realistic 3D environ­ment. Referring to EQA v1, MP3D-EQA utilizes three types of templates: location, color, and color![](./media/image131.png){width="6.917979002624672e-2in" height="6.916010498687664e-3in"}room, generating a total of 1,136 questions in 83 home environments.

> **MP3D-EQA**基于 MINOS 开发的模拟器构建，该模拟器使用 Matterport3D 数据集将问答任务扩展到逼真的 3D 环境中。参考 EQA v1，MP3D-EQA 使用三种类型的模板：位置、颜色和颜色、房间，在 83 个家庭环境中共生成 1136 个问题。

**IQUAD V1** [[262]](#bookmark267) is built upon AI2-THOR and consists of three types of questions: existence, counting, and spatial relationships. It uses a set of templates written down a priori to generate more than 75,000 multiple choice questions, each accompanied by a unique scene configuration. Unlike other datasets, answering IQUAD V1 questions requires the agent to have a good understanding of affordances and interact with the dynamic environment.

> **IQUAD V1**基于 AI2-THOR 构建，由三种类型的问题组成：存在、计数和空间关系。它使用一组预先写下的模板来生成 75000 多个选择题，每个问题都有一个独特的场景配置。与其他数据集不同，回答 IQUAD V1 问题需要代理(agents)对启示有很好的理解，并与动态环境进行交互。

**VideoNavQA** [[263]](#bookmark268) decouples the visual reasoning from the navigation aspect of the EQA problem. In this task, the agent accesses videos corresponding to exploration trajectories with sufficient information to answer questions. Still referring to EQA v1, VideoNavQA generates questions according to func­tional, template-style representation. It also renders shortest trajectories to simulate near-optimal navigation paths, creating videos corresponding to what an agent would see while explor­ing the environment. VideoNavQA generates about 101,000 pairs of videos and questions in the House3D environment using SUNCG, covering 28 types of questions belonging to 8 categories such as existence, counting, and localization.

> **VideoNavQA** 将视觉推理与 EQA 问题的导航方面解耦。在此任务中，代理(agents)访问与探索轨迹相对应的视频，其中包含足够的信息来回答问题。仍然参考 EQA v1，VideoNavQA 根据功能、模板风格的表示生成问题。它还渲染最短轨迹以模拟接近最优的导航路径，创建与代理(agents)在探索环境时看到的内容相对应的视频。VideoNavQA 使用 SUNCG 在 House3D 环境中生成约 101000 对视频和问题，涵盖 28 种类型的问题，分为 8 类，如存在、计数和定位。

**SQA3D** [[264]](#bookmark269) simplifies protocol (QA only) while still preserving the function of benchmarking embodied scene understanding, enabling more complex, knowledge-intensive questions and a much larger scale of data collection. Specif­ically, SQA3D offers a dataset with about 6,800 unique situations, 20,400 descriptions, and 33,400 diverse reasoning questions for these situations based on ScanNet [[272]](#bookmark277) scenes.

> **SQA3D** 简化了协议(仅限 QA)，同时仍然保留了对标具体场景理解的功能，实现了更复杂、知识密集型的问题和更大规模的数据收集。具体来说，SQA3D 提供了一个数据集，其中包含约 6800 个独特的情况、20400 个描述和 33400 个基于 ScanNet 场景的不同推理问题。

Unlike previous datasets that explicitly specify target objects in questions, **K-EQA** [[265]](#bookmark270) features complex questions with logical clauses and knowledge-related phrases, requiring prior knowledge to answer. It is built in AI2Thor and includes four types of questions: existence, counting, enumeration, and comparison. Each entity is mapped to a knowledge base and a knowledge graph is further constructed. In this work, the templates provided in IQA [[262]](#bookmark267) and MT-EQA are extended to a set of grammars. After specifying objects and logical rela­tionships, knowledge graphs, scene graphs, etc. are introduced to generate questions and compute the ground truth answer. The resulting K-EQA dataset consists of 60,000 questions across 6000 different environment setups.

> 与之前在问题中明确指定目标对象的数据集不同，**K-EQA**以具有逻辑子句和知识相关短语的复杂问题为特征，需要先验知识来回答。它建立在 AI2Thor 中，包括四种类型的问题：**存在、计数、枚举和比较**。每个实体被映射到知识库，并进一步构建知识图。在这项工作中，IQA 和 MT-EQA 中提供的模板被扩展为一组语法。在指定对象和逻辑关系后，引入知识图、场景图等来生成问题并计算真实答案。由此产生的 K-EQA 数据集包含 6000 个不同环境设置中的 60000 个问题。

**OpenEQA** [[266]](#bookmark271) is the first open-vocabulary dataset for EQA, supporting both episodic memory and active exploration cases. The episodic memory EQA (EM-EQA) tasks involve an agent developing an understanding of the environment from its episodic memory to answer questions, similar to VideoNavQA. In active EQA (A-EQA) tasks, the agent answers questions by taking exploratory actions to gather necessary information. Us­ing ScanNet and HM3D [[273]](#bookmark278) human annotators constructed over 1,600 high-quality questions from more than 180 real world environments in Habitat.

> **OpenEQA**是 EQA 的第一个开放词汇数据集，支持情景记忆和主动探索案例。情景记忆 EQA(EM-EQA)任务涉及一个代理(agents)从其情景记忆中理解环境以回答问题，类似于 VideoNavQA。在主动 EQA(A-EQA)任务中，代理(agents)通过采取探索性行动来收集必要的信息来回答问题。我们的 ScanNet 和 HM3D 人类注释者从 Habitat 的 180 多个现实世界环境中构建了 1600 多个高质量的问题。

Utilizing GPT4-V, **HM-EQA** [[267]](#bookmark272) is constructed in the Habitat simulator using HM3D. It includes 500 questions across 267 different scenes, which can be roughly categorized into identification, counting, existence, status, and location. For consistency, each question has four multiple choices.

> 利用 GPT4-V，在栖息地模拟器中使用 HM3D 构建**HM-EQA**。它包括 267 个不同场景中的 500 个问题，大致可分为身份识别、计数、存在、状态和位置。为了保持一致性，每个问题都有四个多选题。

**S-EQA** [[268]](#bookmark273) leverages GPT-4 in VirtualHome for data generation and employs cosine similarity calculations to de­cide whether to retain the generated data, thereby enhancing dataset diversity. In S-EQA, answering questions requires the assessment of a collection of consensus objects and states to reach an existential "Yes/No" answer.

> **S-EQA**利用 VirtualHome 中的 GPT-4 进行数据生成，并采用余弦相似性计算来决定是否保留生成的数据，从而增强数据集的多样性。在 S-EQA 中，回答问题需要评估一系列共识对象和状态，以得出存在的“是/否”答案。

1.  _Methods:_ The embodied question answering task mainly involves navigation and question-answering subtasks, with implementation methods broadly categorized into two types: neural network-based and LLMs/VLMs-based.

> 1. 方法：嵌入式问答任务主要涉及导航和问答子任务，实现方法大致分为两类：基于神经网络和基于 LLM/VLMs。

**Neural Network Methods.** In early work, researchers mainly addressed the embodied question answering task by building deep neural networks. They trained and fine-tuned these models using techniques such as imitation learning and reinforcement learning to improve performance.

> **神经网络方法**在早期的工作中，研究人员主要通过构建深度神经网络来解决具体的问答任务。他们使用模仿学习和强化学习等技术对这些模型进行训练和微调，以提高性能。

The EQA task was first proposed by Das et al. [[259]](#bookmark264) In their work, the agent consists of four main modules: vision, language, navigation, and answering. These modules are pri­marily constructed using traditional neural building blocks: Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). They undergo training in two phases. Initially, the navigation and response modules are trained independently on automatically generated expert navigation demonstrations using imitation or supervised learning. Sub­sequently, in the second phase, the navigation architecture is fine-tuned using policy gradients. Some subsequent works [[274]](#bookmark279) [[275]](#bookmark280) retained modules like the question answering module proposed by Das et al. [[259]](#bookmark264) and improved the model. Additionally, Wu et al. [[275]](#bookmark280) proposed integrating the navigation and QA modules into a unified SGD training pipeline for joint training, thereby avoiding employing deep reinforcement learning to simultaneously train the separately trained navigation and question answering modules.

> EQA 任务最早由 Das 等人提出。在他们的工作中，代理(agents)由四个主要模块组成：视觉、语言、导航和应答。这些模块主要使用传统的神经构建块构建：卷积神经网络(CNN)和循环神经网络(RNN)。他们分两个阶段接受训练。最初，导航和响应模块使用模仿或监督学习在自动生成的专家导航演示上独立训练。随后，在第二阶段，使用策略梯度对导航架构进行微调。后续的一些工作提出的问答模块，并改进了模型。此外，Wu 等人提出将导航和 QA 模块集成到统一的 SGD 训练管道中进行联合训练，从而避免使用深度强化学习来同时训练单独训练的导航和问答模块。

There are also some works that attempted to increase the complexity and completeness of question answering tasks. From the perspective of task singularity, several works [[260]](#bookmark265) [[276]](#bookmark281) expanded the task to include multiple objectives and multi-agent, respectively, making it necessary for the model to store and integrate the information obtained by the agent's ex­ploration through methods such as feature extraction and scene reconstruction. Taking into account the interaction between the agent and the dynamic environment, Gordon et al. [[262]](#bookmark267) intro­duced the Hierarchical Interactive Memory Network. Control alternates between the planner, responsible for task selection, and the low level controllers, which carry out task execution. During this process, an Egocentric Spatial GRU (esGRU) is utilized to store spatial memory, enabling the agent to navigate and provide answers. There is also a limitation in previous works where agents are unable to use external knowledge to answer complex questions and lack knowledge of the explored parts of the scene. To address this, Tan et al. [[265]](#bookmark270) proposed a framework that leverages the neural program synthesis method and the table converted from the knowledge and 3D scene graphs, allowing the action planner to access object-related information. Additionally, an approach based on Monte Carlo Tree Search (MCTS) is used to determine the next location for the agent to move to.

> 还有一些作品试图增加问答任务的复杂性和完整性。从任务奇异性的角度来看，几项工作分别将任务扩展到包括多个目标和多代理(agents)，使得模型有必要通过特征提取和场景重建等方法存储和整合代理(agents)展开获得的信息。考虑到代理(agents)与动态环境之间的交互，Gordon 等人引入了分层交互式记忆网络。控制在负责任务选择的计划器和执行任务的低级控制器之间交替进行。在此过程中，利用自我中心空间 GRU(esGRU)来存储空间记忆，使代理(agents)能够导航并提供答案。在之前的工作中也存在一个局限性，即代理(agents)无法使用外部知识来回答复杂的问题，并且缺乏对场景探索部分的知识。为了解决这个问题，Tan 等人提出了一种框架，该框架利用神经程序合成方法和从知识和 3D 场景图转换而来的表，允许动作规划者访问与对象相关的信息。此外，使用基于蒙特卡洛树搜索(MCTS)的方法来确定代理(agents)要移动到的下一个位置。

**LLMs/VLMs Methods.** In recent years, LLMs and VLMs have made continuous progress and demonstrated outstanding capabilities across various fields. Consequently, researchers attempt to apply these models to solve embodied question-answering tasks without any additional fine-tuning.

> **LLM/VLMs 方法** 近年来，LLMs 和 VLMs 在各个领域取得了持续的进步，并展现出了出色的能力。因此，研究人员试图将这些模型应用于解决具身问答任务，而无需任何额外的微调。

Majumdar et al. [[266]](#bookmark271) explored using LLMs and VLMs for episodic memory EQA (EM-EQA) task and Active EQA (A-EQA) task. For EM-EQA task, they considered Blind LLMs, Socratic LLMs with language descriptions of the episodic memory, Socratic LLMs with descriptions of the constructed scene graph, and VLMs processing multiple scene frames. The A-EQA task extended EM-EQA methods with frontier-based exploration (FBE) [[277]](#bookmark282) for problem-independent environment exploration. Some other works [[267]](#bookmark272) [[278]](#bookmark283) also employed frontier-based exploration method to identify areas for subse­quent exploration and to build semantic maps. They ended the exploration early utilizing conformal prediction or image-text matching to avoid over-exploration. Patel et al. [[279]](#bookmark284) empha­sized the question answering aspect of the task. They leveraged multiple LLM-based agents to explore the environment and enable them to independently answer questions with "yes" or "no" answers. These individual responses are utilized to train a Central Answer Model, responsible for aggregating the responses and generating robust answers.

> Majumdar 等人探索了使用 LLM 和 VLM 进行情景记忆 EQA(EM-EQA)任务和主动 EQA(A-EQA)任务。对于 EM-EQA 任务，他们考虑了盲 LLM、带有情景记忆语言描述的苏格拉底 LLM、具有构建场景图描述的苏格拉底 LL 以及处理多个场景帧的 VLM。A-EQA 任务通过基于前沿的探索(FBE)扩展了 EM-EQA 方法，用于独立于问题的环境探索。其他一些作品也采用了基于边界的探索方法来识别后续探索的区域并构建语义图。他们利用共形预测或图像文本匹配提前结束了探索，以避免过度探索。Patel 等人强调了任务的问答方面。他们利用多个基于 LLM 的代理(agents)来探索环境，并使他们能够独立地回答“是”或“否”的问题。这些单独的响应被用来训练一个中央答案模型，负责聚合响应并生成稳健的答案。

3.  _Metrics:_ The performance is usually assessed based on two aspects: navigation and question answering. In the navigation, many works adhered to the approach introduced by Das et al. [[259]](#_bookmark264) and utilized indicators like the distance to the target object upon completion of navigation (\_d~T~_ ), the change in distance to target from initial to final position (_d~∆~_) and the smallest distance to the target at any point in the episode (_d~min~_) to evaluate the performance of the model. They are tested at 10, 30, or 50 actions away from the target. There are also works that measured it based on indicators such as trajectory length, intersection-over-union score for target object (_IoU_ ), etc. For question answering, the evaluation mainly involves mean rank (_MR_) of the ground-truth answer in the answer list and accuracy of the answers. Recently, Majumdar et al. [[266]](#bookmark271) introduced the concept of an aggre­gate LLM-based correctness metric (LLM-Match) to evaluate the accuracy of open-vocabulary answers. Additionally, they assessed efficiency by incorporating the normalized length of the agent's path as a weight for the correctness metric.

> 3. _度量_：性能通常基于两个方面进行评估：导航和问答。在导航中，许多作品坚持 Das 等人提出的方法，并利用导航完成时到目标对象的距离(\_d~T~_)、从初始位置到最终位置到目标的距离变化(\_d~∆~_)和事件中任何点到目标的最小距离(_d~min~_)等指标来评估模型的性能。它们在距离目标 10、30 或 50 个动作处进行测试。也有一些作品根据轨迹长度、目标对象联合得分上的交集(_IoU_)等指标来衡量它。对于问答，评估主要涉及答案列表中地面真实答案的平均排名(_MR_)和答案的准确性。最近，Majumdar 等人引入了基于聚合 LLM 的正确性度量(LLM-Match)的概念，以评估开放词汇答案的准确性。此外，他们通过将代理(agents)路径的归一化长度作为正确性度量的权重来评估效率。

**(a) Language-guided Grasping (b) Human-Agent-Object Interaction (c) Publication Status**

> **(a) 语言引导掌握(b)人机交互(c)出版状态**

```
Fig. 12. []{#bookmark14 .anchor}The overview of the embodied grasping task. (a) demonstrates examples of language-guided grasping for different types of tasks, (b) provides an overview of human-agent-object interaction, (c) shows Google Scholar search results for topics of "Language-guided Grasping".

[]{#bookmark15 .anchor}TABLE X
[Embodied grasping datasets.]{.smallcaps}
```

1.  _Limitations:_ a) Dataset: Constructing datasets requires substantial manpower and resources. Additionally, there are still few large-scale datasets, and the metrics for evaluating model performance vary across different datasets, complicat­ing the testing and comparison of performance, b) Model: Despite the advancements brought by LLMs, the performance of these models still lags significantly behind human levels. Future work may focus more on effectively storing environ­mental information explored by agents and guiding them to plan actions based on environmental memory and questions, while also enhancing the interpretability of their actions.

> 1. _模仿_：
>    a)数据集：构建数据集需要大量的人力和资源。此外，大规模数据集仍然很少，评估模型性能的指标因数据集而异，从而完成了性能的测试和比较，
>    b)模型：尽管 LLM 带来了进步，但这些模型的性能仍然远远落后于人类水平。未来的工作可能更侧重于有效地存储代理(agents)人探索的环境信息，并指导他们根据环境记忆和问题规划行动，同时提高他们行动的可解释性。

B. _Embodied Grasping_

Embodied interaction, in addition to question-answering interactions with humans, also involves performing operations based on human instructions, such as grasping and placing objects, thereby completing interactions among the robots, humans and objects. Embodied grasping requires compre­hensive semantic understanding, scene perception, decision-making, and robust control planning. The embodied grasping methods integrate traditional robotic kinematic grasping with large models such as LLMs [[280]](#bookmark285) and vision-language foun­dation models [[14]](#bookmark37) which enables agents to perform grasping tasks under multi-sensory perceptions, including visual active perception, language understanding and reasoning. Figure [12](#_bookmark14)

> 体现交互除了与人类的问答交互外，还涉及根据人类指令执行操作，例如抓取和放置物体，从而完成机器人、人类和物体之间的交互。具身抓握需要全面的语义理解、场景感知、决策和稳健的控制规划。具体的抓取方法将传统的机器人运动学抓取与大型模型相结合，如 LLM 和视觉语言基础模型，使代理(agents)能够在多感官感知下执行抓取任务，包括视觉主动感知、语言理解和推理。图[12]

(b) illustrates an overview of human-agent-object interaction, where the agent accomplishes embodied grasping tasks.

> (b)示出了人类-代理(agents)-对象交互的概述，其中代理(agents)完成了具体的抓取任务。

1.  _Gripper:_ The current research focus in grasping technol­ogy is on two-finger parallel grippers and five-finger dexterous hands. For two-finger parallel grippers, grasping postures are generally categorized into two types: 4-DOF and 6-DOF [[290]](#bookmark295) The 4-DOF grasp synthesis [[281]](#bookmark286) [[282]](#bookmark287) [[286]](#bookmark291) defines the grasp using a three-dimensional position and a top-down hand orientation (yaw), commonly referred to as "top-down grasping". In contrast, 6-DOF grasp synthesis [[284]](#bookmark289) [[291]](#bookmark296) [[292]](#bookmark297) defines the grasp posture through a six-dimensional po­sition and orientation. For five-finger dexterous hand grippers, the ShadowHand, a widely used five-finger robotic dexterous hand, features 26 degrees of freedom (DOF). This high dimen­sionality significantly increases the complexity of generating effective grasp postures and planning execution trajectories.

> 1. 抓取器：目前抓取技术的研究重点是两指平行抓取器和五指灵巧手。对于双指平行抓握器，抓握姿势通常分为两类：4-DOF 和 6-DOF 4-DOF 抓握合成使用三维位置和自上而下的手方向(偏航)定义抓握，通常称为“自上而下抓握”。相比之下，6-DOF 抓握合成通过六维位置和方向定义抓握姿势。对于五指灵巧手抓握器，ShadowHand 是一种广泛使用的五指机器人灵巧手，具有 26 个自由度(DOF)。这种高维度显著增加了生成有效抓握姿势和规划执行轨迹的复杂性。

2.  _Datasets:_ Recently, a substantial number of grasping datasets [[281]--[285]](#bookmark290) have been generated. These datasets typically contain annotated grasping data based on images (RGB, depth), point clouds, or 3D scenes. With the advent of MLMs and the application of foundational language models to robotic grasping, there is an urgent need for datasets that include linguistic text. Consequently, existing datasets have been extended or reconstructed to create semantic-grasping datasets [[287]--[289]](#bookmark294) [[293]](#bookmark298) These datasets are instrumental in studying grasping models grounded in language, enabling agents to develop a broad understanding of semantics.

> 2. _数据集_：最近，已经生成了大量抓取数据集。这些数据集通常包含基于图像(RGB、深度)、点云或 3D 场景的带注释抓取数据。随着传销的出现和基础语言模型在机器人抓取中的应用，迫切需要包括语言文本的数据集。因此，现有的数据集已被扩展或重建，以创建语义抓取数据集。这些数据集有助于研究基于语言的抓取模型，使代理(agents)能够广泛理解语义。

Traditional grasping datasets encompass data for both single objects [[281]](#bookmark286) and cluttered scenes [[286]](#bookmark291) providing stable grasp annotations (4-DOF or 6-DOF) that conform to kine-matics for each object. These data can be collected from real desktop environments [[281]](#bookmark286) typically including RGB, depth, and point cloud data, or from virtual environments [[284]](#bookmark289) which include image data, point clouds, or scene models. While these datasets are useful for grasping models, they lack semantic information. To bridge this gap, these datasets have been augmented or extended with semantic expressions [[287]](#bookmark292) [[294]](#bookmark299) thereby linking language, vision, and grasping. By incorporating semantic information, agents can better un­derstand and execute grasping tasks. This enhancement allows for the development of more sophisticated and semantically aware grasping models, facilitating more intuitive and effective interaction with the environment. Table [X](#_bookmark15) presents the datasets described above, including traditional grasping datasets and language-based grasping datasets.

> 传统的抓取数据集包括单个对象和杂乱场景的数据，为每个对象提供符合运动学的稳定抓取注释(4-DOF 或 6-DOF)。这些数据可以从真实的桌面环境收集，通常包括 RGB、深度和点云数据，也可以从虚拟环境收集，其中包括图像数据、点云或场景模型。虽然这些数据集对于掌握模型很有用，但它们缺乏语义信息。为了弥合这一差距，这些数据集已经用语义表达进行了增强或扩展，从而将语言、视觉和抓握联系起来。通过整合语义信息，代理(agents)可以更好地理解和执行抓取任务。这种增强允许开发更复杂和语义感知的抓取模型，促进与环境更直观和有效的交互。表[X]显示了上述数据集，包括传统抓取数据集和基于语言的抓取数据集。

3.  _Language-guided grasping:_ The concept of language-guided grasping [[287]](#bookmark292) [[288]](#bookmark293) [[294]](#bookmark299) which has evolved from this integration, combines MLMs to provide agents with the capability of semantic scene reasoning. This allows the agent to execute grasping operations based on implicit or explicit human instructions. Figure [12](#_bookmark14) (c) illustrates the publica­tion trends in recent years on the topic of language-guided grasping. With the advancement of LLMs, researchers have shown increasing interest in this topic. Currently, grasping research is increasingly focused on open-world scenarios, emphasizing the open-set generalization [[295]](#bookmark300) methods. By leveraging the generalization capabilities of MLMs, robots can perform grasping tasks in open-world environments with greater intelligence and efficiency.

> 3. 语言引导抓取：语言引导抓取的概念是从这种集成发展而来的，它结合了多语言模型，为代理(agents)提供了语义场景推理的能力。这允许代理(agents)根据隐式或显式的人类指令执行抓取操作。图[12](c)显示了近年来关于语言引导掌握主题的公开趋势。随着 LLM 的进步，研究人员对这一主题表现出越来越大的兴趣。目前，抓取研究越来越关注开放世界场景，强调开放集泛化方法。通过利用 MLM 的泛化能力，机器人可以在开放世界环境中以更高的智能和效率执行抓取任务。

In language-guided grasping, semantics can originate from explicit instructions [[295]](#bookmark300) [[296]](#bookmark301) and implicit instructions [[288]](#bookmark293) [[289]](#bookmark294) Explicit instructions clearly specify the category of the object to be grasped, such as a banana or an apple. Implicit instructions, however, require reasoning to identify the object or a part of the object to be grasped, involving spatial reasoning and logical reasoning.

> 在语言引导的抓取中，语义可以来源于显式指令和隐式指令显式指令明确指定了要抓取的对象的类别，如香蕉或苹果。然而，隐式指令需要推理来识别要掌握的对象或对象的一部分，涉及空间推理和逻辑推理。

Spatial reasoning [[287]](#bookmark292) refers to instructions that may include the spatial relationship of the object or part to be grasped, necessitating the inference of grasping posture based on the spatial relationships of objects within the scene. For example, "Grasp the keyboard that is to the right of the brown kleenex box" involves understanding and inferring the spatial arrangement of objects. Logical reasoning [[288]](#bookmark293) on the other hand, involves instructions that may contain logical relationships requiring inference to discern human intent and subsequently grasp the target. For instance, "I am thirsty, can you give me something to drink?" would prompt the agent to potentially hand over a glass of water or a bottle of a beverage. The agent must ensure that the liquid does not spill during the handover, thus generating a reasonable grasping posture.

> 空间推理是指可能包括要抓取的对象或部分的空间关系的指令，需要基于场景内对象的空间关系推断抓取姿势。例如，“抓住棕色面巾纸盒子右侧的键盘”涉及理解和推断物体的空间排列。另一方面，逻辑推理涉及可能包含逻辑关系的指令，这些指令需要推理来辨别人类意图并随后掌握目标。例如，“我渴了，你能给我喝点什么吗？”会促使代理(agents)人可能交出一杯水或一瓶饮料。代理(agents)人必须确保液体在交接过程中不会溢出，从而产生合理的抓握姿势。

In both cases, the integration of semantic understanding with spatial and logical reasoning enables the agent to perform complex grasping tasks effectively and accurately. Figure [12](#_bookmark14)

> 在这两种情况下，语义理解与空间和逻辑推理的集成使代理(agents)能够有效准确地执行复杂的抓取任务。图[12]

(a) depicts various types of language-guided grasping tasks.

4. _End-to-End Approaches:_ CLIPORT [[294]](#bookmark299) is a language-conditioned imitation learning agent that combines the vision-language pre-trained model CLIP with the Transporter Net to create an end-to-end dual-stream architecture for semantic understanding and grasp generation. It is trained using a large number of expert demonstration data collected from virtual environments, enabling the agent to perform semantically guided grasping. Based on the OCID dataset, CROG [[287]](#bookmark292) proposes a vision-language-grasping dataset and introduces a competitive end-to-end baseline. It leverages CLIP's visual foundation capabilities to learn grasp synthesis directly from image-text pairs. Reasoning Grasping [[288]](#bookmark293) introduces the first reasoning grasping benchmark dataset based on the GraspNet-1 Billion dataset and proposes an end-to-end reasoning grasp­ing model. The model integrates multimodal LLMs with vision-based robotic grasping frameworks to generate grasps based on semantics and vision. SemGrasp [[289]](#bookmark294) is a method for semantic-based grasp generation that incorporates semantic information into grasp representations to generate dexterous hand grasp postures. It introduces a discrete representation aligning grasp space with semantic space, enabling the gen­eration of grasp postures according to language instructions. To facilitate training, a large-scale grasp-text alignment dataset CapGrasp is proposed.

> 4. _端到端方法_：CLIPORT 是一种语言条件模仿学习代理(agents)，它将视觉语言预训练模型 CLIP 与 Transporter Net 相结合，创建了一个用于语义理解和掌握生成的端到端双流架构。它使用从虚拟环境中收集的大量专家演示数据进行训练，使代理(agents)能够执行语义引导的抓取。基于 OCID 数据集，CROG 提出了一种视觉语言抓取数据集，并引入了有竞争力的端到端基线。它利用 CLIP 的视觉基础功能，直接从图像文本对中学习抓取合成。Reasoning Mastering 介绍了基于 GraspNet-1 Billion 数据集的第一个推理抓取基准数据集，并提出了一个端到端的推理抓取模型。该模型将多模态 LLM 与基于视觉的机器人抓取框架相结合，基于语义和视觉生成抓取。SemGrasp 是一种基于语义的抓取生成方法，它将语义信息整合到抓取表示中，以生成灵巧的手抓取姿势。它引入了一种离散表示，将抓握空间与语义空间对齐，从而能够根据语言指令生成抓握姿势。为了便于训练，提出了一种大规模的抓取文本对齐数据集 CapGrasp。

> Before 2022 2022 2023 2024
>
> Fig. 13. []{#bookmark16 .anchor}Chronological overview of the embodied agents. Different colors show different paradigms. MLM means the Multimodal Language Model which directly perceive the world and control the embodiment, VLM means the Visual-Language Model with the outer policy models, LLM + VLM means the LLM-based agent that perceives the world utilizing the VLM, and LLM means the Large-Language Model with visual context and outer policy models.

5. _Modular Approaches:_ F3RM [[295]](#bookmark300) seeks to elevate CLIP's text-image priors into 3D space, using extracted fea­tures for language localization followed by grasp generation. It combines precise 3D geometry with rich semantics from 2D foundational models, utilizing features extracted from CLIP to specify objects for manipulation through free-text natural language. It demonstrates the ability to generalize to unseen expressions and new object categories. GaussianGrasper [[296]](#bookmark301) utilizes a 3D Gaussian field to achieve language-guided grasp­ing tasks. The proposed methodology begins with the construc­tion of a 3D Gaussian field, followed by feature distillation. Subsequently, language-based localization is performed using the extracted features. Finally, grasp pose generation is carried out based on a SOTA pre-trained grasping network [[297]](#bookmark302) It integrates open-vocabulary semantics with precise geometry, enabling grasping based on language instructions.

> 5. _模块化方法_：F3RM 试图将 CLIP 的文本图像先验提升到 3D 空间，使用提取的特征进行语言定位，然后生成抓取。它将精确的 3D 几何与 2D 基础模型的丰富语义相结合，利用从 CLIP 提取的特征通过自由文本自然语言指定要操作的对象。它展示了泛化到看不见的表达式和新对象类别的能力。GaussianGrasper 利用 3D 高斯场来实现语言引导的抓取任务。所提出的方法首先构建 3D 高斯场，然后进行特征提取。随后，使用提取的特征进行基于语言的定位。最后，基于 SOTA 预训练的抓取网络进行抓取姿势生成。它将开放词汇语义与精确几何相结合，实现了基于语言指令的抓取。

These approaches advance the field of language-guided grasping by leveraging both end-to-end and modular frame­works, thereby enhancing the ability of robotic agents to un­derstand and execute complex grasping tasks through natural language instructions. Embodied grasping allows robots to interact with objects, thus improving their intelligence and utility in home services and industrial manufacturing. How­ever, existing embodied grasping methods have limitations, such as reliance on extensive data and poor generalization to unseen data. Future research will focus on improving the generality of agents, enabling robots to understand more complex semantics, grasp a wider variety of unseen objects, and complete intricate grasping tasks.

> 这些方法通过利用端到端和模块化框架来推进语言引导抓取领域，从而增强机器人代理(agents)通过自然语言指令理解和执行复杂抓取任务的能力。具身抓取允许机器人与物体交互，从而提高了它们在家庭服务和工业制造中的智能和实用性。然而，现有的具身抓取方法存在局限性，例如依赖于大量数据和对未知数据的泛化能力差。未来的研究将侧重于提高智能体的通用性，使机器人能够理解更复杂的语义，掌握更广泛的看不见的物体，并完成复杂的抓取任务。

## VI. EMBODIED AGENT

An agent is defined as an autonomous entity capable of per­ceiving its environment and acting to achieve specific objec­tives. Recent advancements in MLMs have further expanded the application of agents to practical scenarios. When these MLM-based agents are embodied in physical entities, they can effectively transfer their capabilities from virtual space to physical world, thereby becoming embodied agents [[298]](#bookmark303) Fig. [13](#_bookmark16) shows the chronological overview of embodied agents. To enable embodied agents to operate in the information-rich and complex real world, the embodied agents have been developed to show strong multimodal perception, interaction and planning capabilities, as shown in Fig. [14.](#_bookmark17) To complete a task, embodied agents typically involves the following process: 1) decomposing the abstract and complex task into specific subtasks, which is referred to as high-level Embod­ied Task Planning. 2) gradually implementing these subtasks by effectively utilizing Embodied Perception and Embodied Interaction models or leveraging the Foundation Model's pol­icy function, named low-level Embodied Action Planning. It is worth noting that task planning involves thinking before acting, and is therefore typically considered in cyber space. In contrast, action planning must account for effective interaction with the environment and feedback on this information to the task planner to adjust task planning. Thus, it is crucial for embodied agents to align and generalize their abilities from the cyber space to the physical world.

> 代理(agents)被定义为能够接收其环境并采取行动实现特定目标的自治实体。MLM 的最新进展进一步将代理(agents)的应用扩展到实际场景。当这些基于 MLM 的代理(agents)体现在物理实体中时，它们可以有效地将其能力从虚拟空间转移到物理世界，从而成为体现代理(agents)图[13]显示了体现代理(agents)的时间顺序概述。为了使具身代理(agents)能够在信息丰富和复杂的现实世界中运行，已开发出具有强大多模态感知、交互和规划能力的具身代理(agents)，如图[14]所示。
> 为了完成任务，具身代理(agents)通常涉及以下过程：1)将抽象和复杂的任务分解为特定的子任务，这被称为高级具身任务规划。2) 通过有效利用体现感知和体现交互模型或利用基础模型的策略功能(称为低级体现行动计划)逐步实施这些子任务。值得注意的是，任务规划涉及行动前的思考，因此通常在网络空间中被考虑。相比之下，行动计划必须考虑到与环境的有效交互，并将此信息反馈给任务规划者以调整任务计划。因此，对于具身代理(agents)来说，将他们的能力从网络空间推广到物理世界是至关重要的。

> Fig. 14. []{#bookmark17 .anchor}The architecture of the embodied agent based on embodied multimodal foundation model, which consists of visual perception module, high-level task planning module, and low-level action planning module.

A. _Embodied Multimodal Foundation Model_

Embodied agents are required to recognize their environ­ment visually, understand instructions audibly, and compre­hend their own state to enable complex interactions and oper­ations. This demands a model that integrates multiple sensory modalities and natural language processing capabilities to enhance the agent's understanding and decision-making by synthesizing diverse data types. Thus, the Embodied Mul­timodal Foundation Model is emerging. Recently, Google DeepMind found that leveraging foundation models and large, diverse datasets is the optimal strategy. They developed a series of works based on the Robotic Transformer (RT) [[11]](#bookmark34) offering substantial insights for future research on embodied agents.

> 实体代理(agents)需要视觉识别其环境，听觉理解指令，并理解其自身状态，以实现复杂的交互和操作。这需要一个整合多种感官模式和自然语言处理能力的模型，通过合成不同的数据类型来增强智能体的理解和决策能力。因此，体现的多边基金会模式正在出现。最近，谷歌 DeepMind 发现，利用基础模型和大型、多样化的数据集是最佳策略。他们基于机器人变压器(RT)开发了一系列作品，为未来关于具身代理(agents)的研究提供了实质性的见解。

Significant progress has been made in foundational robotics models, evolving from the initial approach in SayCan [[299]](#bookmark304) which used three separate models for planning, affordance, and low-level policy. Q-Transformer [[300]](#bookmark305) later unified affordance and low-level policy, and PaLM-E [[301]](#bookmark306) integrated planning and affordance. Then, RT-2 [[302]](#bookmark307) achieved a breakthrough by consolidating all three functions into a single model, enabling joint scaling and positive transfer. This represents a substantial advancement in robotics foundational models. RT-2 introduced the Vision-Language-Action (VLA) model, featuring "chain-of-thought" reasoning abilities that enable multi-step semantic reasoning, such as selecting alternative tools or beverages in various contexts. Ultimately, RT-H [[4]](#bookmark28) achieved an end-to-end robot transformer with action hierarchies, to reason about the task planning at a fine-grained level.

> 在基础机器人模型方面取得了重大进展，从 SayCan 的最初方法演变而来，该方法使用三个单独的模型进行规划、启示和低级策略。Q-Transformer 后来统一了启示和低级策略，PaLM-E 将规划和启示融为一体。然后，RT-2 实现了突破，将所有三个功能整合到一个模型中，实现了联合缩放和正向传输。这代表了机器人基础模型的重大进步。RT-2 引入了视觉语言动作(VLA)模型，具有“思维链”推理能力，可以实现多步语义推理，例如在各种情况下选择替代工具或饮料。最终，RT-H 实现了一个具有动作层次结构的端到端机器人转换器，可以在细粒度上对任务规划进行推理。

To address the generalization limitations of embodied mod­els, Google collaborated with 33 leading academic insti­tutions to create the comprehensive Open X-Embodiment dataset [[303]](#bookmark308) integrating 22 diverse data types. Using this dataset, they trained the universal large model RT-X. This has also promoted the participation of more open-source VLMs for robotics, such as EmbodiedGPT [[304]](#bookmark309) based on LLaVA and RoboFlamingo [[305]](#bookmark310) based on Flamingo. Although Open X-Embodiment provides a vast array of datasets, constructing datasets remains a challenge given the rapid evolution of em­bodied robotic platforms. To address this issue, AutoRT [[306]](#bookmark311) created a system for deploying robots in new environments to collect training data, leveraging LLMs to enhance learning capabilities through more comprehensive and diverse data.

> 为了解决隐含模型的泛化局限性，谷歌与 33 家领先的学术机构合作，创建了全面的 Open X-Implementation 数据集，整合了 22 种不同的数据类型。使用这个数据集，他们训练了通用的大型模型 RT-X。这也促进了更多开源 VLM 参与机器人技术，例如**基于 LLaVA 的 EmbodiedGPT 和基于 Flamingo 的 RoboFlamingo**。尽管 Open X-Implementation 提供了大量的数据集，但鉴于实体机器人平台的快速发展，构建数据集仍然是一个挑战。为了解决这个问题，AutoRT 创建了一个系统，用于在新环境中部署机器人来收集训练数据，利用 LLM 通过更全面和多样化的数据来增强学习能力。

Additionally, transformer-based architectures face ineffi­ciency problems because embodied models require long con­texts that include information from vision, language, and em­bodied states, as well as memory related to the currently exe­cuted tasks. For instance, RT-2, despite its strong performance, has an inference frequency of only 1-3Hz. Several efforts have been made, such as deploying models through quantization and distillation. Moreover, improving the model framework is another viable approach. SARA-RT [[307]](#bookmark312) employs more efficient linear attention, while RoboMamba [[308]](#bookmark313) utilizes the mamba architecture, which is more efficient for long-sequence tasks. This enables it to achieve inference speeds seven times faster than existing robotic MLMs.

> 此外，基于 transformer-based 的架构面临着效率低下的问题，因为具体化模型需要长文本，其中包括来自视觉、语言和身体状态的信息，以及与当前执行任务相关的内存。例如，RT-2 尽管性能强劲，但推理频率仅为 1-3Hz。已经做出了一些努力，例如通过量化和蒸馏部署模型。此外，改进模型框架是另一种可行的方法。SARA-RT 采用了更高效的线性注意力，而 RoboMamba 采用了曼巴架构，这对于长序列任务更有效。这使其能够实现比现有机器人 MLM 快七倍的推理速度。

Generative-model-based RT excels in high-level task un­derstanding and planning but has limitations in low-level action planning due to the generative model's inability to precisely generate action parameters and the gap between high-level task planning and low-level action execution. To address this, Google introduced RT-Trajectory [[309]](#bookmark314) which provides low-level visual cues for learning robot control strategies by automatically adding robot trajectories. Similarly, building on the RT-2 framework, the Robot Transformer with Action Hierarchies (RT-H) incorporates a hierarchical action framework, linking high-level task descriptions with low-level robot motions through intermediate linguistic actions [[4]](#bookmark28) Furthermore, VLA models exhibit emergent capabilities only in high-level planning and affordance tasks related to VLMs. They fail to demonstrate new skills in low-level physical interactions and are constrained by the skill categories in their datasets, resulting in clumsy actions. Future research should integrate reinforcement learning into the training framework of large models to improve generalization, enabling VLA models to autonomously learn and optimize low-level physical inter­action strategies in real-world environments, thus executing various physical actions more dexterously and accurately.

> 基于生成模型的 RT 在高级任务理解和规划方面表现出色，但在低级行动规划方面存在局限性，因为生成模型无法精确生成行动参数，并且高级任务规划和低级行动执行之间存在差距。为了解决这个问题，谷歌推出了 RT 轨迹，它通过自动添加机器人轨迹为学习机器人控制策略提供低级视觉提示。同样，在 RT-2 框架的基础上，具有动作层次结构的机器人转换器(RT-H)结合了一个层次化的动作框架，通过中间语言动作将高级任务描述与低级机器人动作联系起来。此外，VLA 模型仅在与 VLM 相关的高级规划和启示任务中表现出应急能力。他们未能在低级物理交互中展示新技能，并受到数据集中技能类别的限制，导致行动笨拙。未来的研究应将强化学习整合到大型模型的训练框架中，以提高泛化能力，使 VLA 模型能够在现实环境中自主学习和优化低级物理交互策略，从而更灵活、更准确地执行各种物理动作。

B. []{#bookmark18 .anchor}_Embodied Task Planning_

As previously discussed, a task _"put an apple on a plate"_, the task planner will divide it into sub-tasks _"find the apple, pick the apple", "find the plate", "put down the apple"_. Since how to find (navigation task) or pick/put down actions (grasping task) are not within the scope of task planning. These actions are typically predefined within simulators or executed in real-world scenarios using pre-trained policy models, such as using CLIPort [[294]](#bookmark299) for grasping tasks.

> 如前所述，对于一个任务——“把苹果放在盘子里”，任务规划器会将其划分为子任务——“找到苹果，摘苹果”、“找到盘子”、“放下苹果”。因为如何查找(导航任务)或拾取/放下动作(抓取任务)不在任务规划的范围内。这些操作通常在模拟器中预定义，或在使用预训练策略模型的真实场景中执行，例如使用 CLIPort 来抓取任务。

Traditional embodied task planning methods are usually based on explicit rules and logical reasoning. For example, symbolic planning algorithms such as STRIPS [[310]](#bookmark315) and PDDL [[311]](#bookmark316) and search algorithms like MCTS [[312]](#bookmark317) and A\* [[313]](#bookmark318) are used to generate plans. However, these methods often rely on predefined rules, constraints, and heuristics that are rigid and may not adapt well to dynamic or unforeseen changes in the environment. With the popularity of LLMs, many works have attempted to use LLMs for planning or to combine traditional methods with LLMs, leveraging the rich world knowledge embedded within them for reasoning and planning without the need for handcrafted definitions, greatly enhancing the model's generalization capabilities.

> 传统的具身任务规划方法通常基于显式规则和逻辑推理。例如，STRIPS 和 PDDL 和 A\*等搜索算法用于生成计划。然而，这些方法通常依赖于预定义的规则、约束和启发式方法，这些规则、约束是僵化的，可能无法很好地适应环境中的动态或不可预见的变化。随着 LLM 的普及，许多工作试图使用 LLM 进行规划，或将传统方法与 LLM 相结合，利用其中嵌入的丰富知识进行推理和规划，而不需要手工定义，大大提高了模型的泛化能力。

1.  _Planning utilizing the Emergent Capabilities of LLMs:_ Before the scale-up of natural language models, task planners were similarly implemented by training models like BERT on embodied instruction datasets such as Alfred [[314]](#bookmark319) and Alfworld [[315]](#bookmark320) as demonstrated by FILM [[316]](#bookmark321) However, this approach was limited by the examples in the training set and could not effectively align with the physical world. Nowadays, thanks to the emergent capabilities of LLMs, they can decompose abstract tasks using their internal world knowledge and chain-of-thought reasoning, similar to how humans reason through task completion steps before acting. For example, Translated LM [[317]](#bookmark322) and Inner Monologue [[318]](#bookmark323) can break down complex tasks into manageable steps and devise solutions using their internal logic and knowledge systems without additional training, such as ReAct [[319]](#bookmark324) Sim­ilarly, the multi-agent collaboration framework ReAd [[320]](#bookmark325) is proposed for efficient self-refinement of plans via the different prompts. Additionally, some approaches abstract past successful examples into a series of skills stored in a mem­ory bank to consider during inference and improve planning success rates [[321]--[323]](#bookmark327) And some works utilize code as the reasoning medium instead of natural language. where task planning is generated as code based on the available API library [[324]--[326]](#bookmark329) Furthermore, multi-turn reasoning can effectively correct potential hallucinations in task planning, a focus of many LLM-based agent studies. For instance, Socratic Models [[327]](#bookmark332) and Socratic Planner [[328]](#bookmark334) use Socratic questioning to derive reliable planning.

> 1. _利用 LLM 的涌现能力进行规划_：在自然语言模型扩大之前，任务规划器同样是通过 BERT 等训练模型在 Alfred 和 Alfworld 等具体指令数据集上实现的，如 FILM 和 Inner Monologue 可以将复杂的任务分解为可管理的步骤，并使用其内部逻辑和知识系统设计解决方案，而无需额外的培训，如 ReAct。此外，还提出了多代理(agents)协作框架 ReAd。一些作品**利用代码作为推理媒介，而不是自然语言**。其中任务规划被生成为基于可用 API 库的代码此外，多转弯推理可以有效地纠正任务规划中的潜在幻觉，这是许多基于 LLM 的代理(agents)研究的焦点。例如，苏格拉底模型和苏格拉底计划使用苏格拉底式提问来推导可靠的计划。

However, during task planning, potential failures may occur during execution, often resulting from the planner not fully accounting for the complexity of the real environment and the difficulty of task execution [[318]](#bookmark323) [[329]](#bookmark336) Due to a lack of visual information, planned subtasks may deviate from the actual scenario, leading to task failure. Therefore, integrating visual information into planning or replanning during execu­tion is necessary. This approach can significantly enhance the accuracy and feasibility of task planning, better addressing the challenges of real-world environments.

> 然而，在任务规划过程中，执行过程中可能会出现潜在的失败，这通常是由于规划者没有充分考虑真实环境的复杂性和任务执行的难度。由于缺乏视觉信息，计划的子任务可能会偏离实际情况，导致任务失败。因此，在执行过程中将视觉信息整合到规划或重新规划中是必要的。这种方法可以显著提高任务规划的准确性和可行性，更好地应对现实世界环境的挑战。

2. _Planning utilizing the visual information from embod­ied perception model:_ Based on the above discussion, it is particularly important to further integrate visual information into task planning (or replanning). In this process, object labels, locations, or descriptions provided by visual input can offer critical references for task decomposition and execution by LLMs. Through visual information, LLMs can more ac­curately identify target objects and obstacles in the current environment, thereby optimizing task steps or modifying sub­task objectives. Some works use an object detector to query the objects present in the environment during task execution and feed this information back to the LLM, allowing it to modify unreasonable steps in the current plan [[327]](#bookmark332) [[329]](#bookmark336) [[330]](#bookmark338) RoboGPT considers the different names of similar objects within the same task, further improving the feasibility of replanning [[10]](#bookmark33) However, the information provided by labels is still too limited. Can further scene information be provided? SayPlan [[331]](#bookmark340) proposes using hierarchical 3D scene graphs to represent the environment, effectively mitigating the challenges of task planning in large, multi-floor, and multi-room settings. Similarly, ConceptGraphs [[332]](#bookmark343) also adopts 3D scene graphs to provide environmental information to LLMs. Compared to SayPlan, it offers more detailed open-world object detection and presents task planning in a code-based format, which is more efficient and better suited to the demands of complex tasks.

> 2. 利用体现感知模型中的视觉信息进行规划：基于上述讨论，将视觉信息进一步整合到任务规划(或重新规划)中尤为重要。在这个过程中，视觉输入提供的对象标签、位置或描述可以为 LLM 的任务分解和执行提供关键参考。通过视觉信息，LLM 可以更准确地识别当前环境中的目标对象和障碍物，从而优化任务步骤或修改子任务目标。一些作品使用对象检测器在任务执行期间查询环境中存在的对象，并将此信息反馈给 LLM，使其能够修改当前计划中的不合理步骤。然而，标签提供的信息仍然太有限。能否提供更多的现场信息？SayPlan 提出使用分层 3D 场景图来表示环境，有效地缓解了大型、多楼层和多房间环境中任务规划的挑战。同样，ConceptGraphs 也采用 3D 场景图为 LLM 提供环境信息。与 SayPlan 相比，它提供了更详细的开放世界对象检测，并以基于代码的格式呈现任务规划，这更高效，更适合复杂任务的需求。

However, limited visual information can result in an agent's inadequate understanding of its environment. While LLMs are provided with visual cues, they often fail to capture the environment's complexity and dynamic changes, leading to misunderstandings and task failures. For example, if a towel is locked in a bathroom cabinet, the agent might repeatedly search the bathroom without considering this possibility [[10]](#bookmark33) To address this, more robust algorithms must be developed to integrate multiple sensory data, enhancing the agent's en­vironmental understanding. Additionally, leveraging historical data and contextual reasoning, even when visual information is limited, can aid the agent in making reasonable judgments and decisions. This approach of multimodal integration and context-based reasoning not only increases task execution success rates but also provides new perspectives for the advancement of embodied artificial intelligence.

> 然而，有限的视觉信息可能会导致代理(agents)对其环境的理解不足。虽然 LLM 提供了视觉提示，但它们往往无法捕捉到环境的复杂性和动态变化，从而导致误解和任务失败。例如，如果毛巾被锁在浴室橱柜里，智能体可能会在不考虑这种可能性的情况下反复搜索浴室。为了解决这个问题，必须开发更强大的算法来整合多种感官数据，增强智能体对环境的理解。此外，即使视觉信息有限，利用历史数据和上下文推理也可以帮助智能体做出合理的判断和决策。这种多模态集成和基于上下文的推理方法不仅提高了任务执行成功率，还为嵌入式人工智能的发展提供了新的视角。

3.  _Planning utilizing the VLMs:_ Compared to converting environmental information into text using external visual mod­els, VLM models can capture visual details in latent space, particularly contextual information that is difficult to represent with object labels. VLMs can discern rules underlying visual phenomena; for instance, even if a towel is not visible in the environment, it can be inferred that the towel might be stored in a cabinet. This process essentially demonstrates how abstract visual features and structured textual features can be more effectively aligned in latent space. In Embod-iedGPT [[304]](#bookmark309) the Embodied-Former module aligns embodied, visual, and textual information, effectively considering the agent's state and environmental information during task plan­ning. Unlike EmbodiedGPT, which directly uses third-person perspective images, LEO [[333]](#bookmark345) encodes 2D egocentric images and 3D scenes into visual tokens. This method effectively per­ceives 3D world information and executes tasks accordingly. Similarly, the EIF-Unknow model utilizes Semantic Feature Maps extracted from Voxel Features as visual tokens, which are input along with text tokens into a trained LLaVA model for task planning [[334]](#bookmark346) Furthermore, embodied multimodal foundation models, or VLA models, have been extensively trained with large datasets in studies like the RT series [[11]](#bookmark34) [[302]](#bookmark307) PaLM-E [[301]](#bookmark306) and Matcha [[335]](#bookmark348) to achieve alignment of visual and textual features in embodied scenarios.

> 3. _利用 VLM 进行规划_：与使用外部视觉模型将环境信息转换为文本相比，VLM 模型可以捕获潜在空间中的视觉细节，特别是难以用对象标签表示的上下文信息。VLM 可以辨别视觉现象背后的规则；例如，即使毛巾在环境中不可见，也可以推断毛巾可能存放在橱柜中。这个过程本质上展示了抽象视觉特征和结构化文本特征如何在潜在空间中更有效地对齐。在 Emboded iedGPT 中，Emboded Former 模块对齐体现的、视觉的和文本的信息，在任务规划过程中有效地考虑代理(agents)的状态和环境信息。与直接使用第三人称视角图像的 EmbodiedGPT 不同，LEO 将 2D 以自我为中心的图像和 3D 场景编码为视觉符号。这种方法有效地接收 3D 世界信息并相应地执行任务。同样，EIF 未知模型利用从体素特征中提取的语义特征图作为视觉标记，这些语义特征图与文本标记一起输入到训练好的 LLaVA 模型中，用于任务规划。此外，在 RT 系列[[302]](#Mookmark307)PaLM-E[[301]](#0ookmark306)和 Matcha[[335]](#4ookmark348)等研究中，已使用大型数据集对体现多模态基础模型或 VLA 模型进行了广泛训练，以实现体现场景中视觉和文本特征的对齐。

However, task planning is only the first step for an agent in completing an instruction task; subsequent action planning determines whether the task can be accomplished. In the ex­periments from RoboGPT [[10]](#bookmark33) the accuracy of task planning reached 96%, but the overall task completion rate was only 60%, limited by the performance of the low-level planner. Therefore, whether an embodied agent can transition from the cyber space of "imagining how tasks are completed" to the physical world of "interacting with the environment and completing tasks" hinges on effective action planning.

> 然而，任务规划只是代理(agents)完成指令任务的第一步；后续行动计划决定了任务是否可以完成。在 RoboGPT[10]的实验中，任务规划的准确率达到 96%，但总体任务完成率仅为 60%，受到低级规划器性能的限制。因此，具身代理(agents)能否从“想象任务是如何完成的”的网络空间过渡到“与环境互动并完成任务”的物理世界，取决于有效的行动计划。

C. _Embodied Action Planning_

Section [VI-B](#_bookmark18) discusses the definitions and differences be­tween task planning and action planning. It is evident that action planning must address real-world uncertainties because the granularity of subtasks provided by task planning is insuf­ficient to guide agents in environmental interaction. Generally, agents can achieve action planning in two ways: 1) using pretrained embodied perception and embodied intervention mod­els as tools, incrementally completing the subtasks specified by task planning through APIs, 2) utilizing the VLA model's inherent capabilities to derive action planning. Furthermore, the results of the action planner's execution are fed back to the task planner to adjust and improve task planning.

> 第[VI-B]节讨论了任务规划和行动规划之间的定义和区别。很明显，行动规划必须解决现实世界的不确定性，因为任务规划提供的子任务的粒度不足以指导环境交互中的主体。一般来说，代理(agents)可以通过两种方式实现行动计划：1)使用预训练的具身感知和具身干预模型作为工具，通过 API 逐步完成任务计划指定的子任务，2)利用 VLA 模型的固有能力来制定行动计划。此外，动作规划器的执行结果被反馈给任务规划器，以调整和改进任务规划。

1.  _Action utilizing APIs:_ A typical approach involves pro­viding LLMs with the definitions and descriptions of various well-trained policy models as context, enabling them to under­stand these tools and determine how and when to invoke them for specific tasks [[299]](#bookmark304) [[329]](#bookmark336) Additionally, by generating code, a series of more granular tools can be abstracted into a function library for invocation rather than directly passing the parameters needed for sub-tasks to navigation and grasping models [[326]](#bookmark329) Given the uncertainty of the environment, Reflexion can further adjust these tools during execution to achieve better generalization [[336]](#bookmark350) Optimizing these tools can enhance the robustness of the agent, and new tools may be required to complete unknown tasks. DEPS, under the premise of zero-shot learning, endows LLMs with various role settings to learn diverse skills while interacting with the environment. During subsequent interactions, LLMs can learn to select and combine these skills to develop new ones [[337]](#bookmark352)

> 1. _使用 API 的操作_：一种典型的方法是向 LLM 提供各种训练有素的策略模型的定义和描述作为上下文，使其能够理解这些工具，并确定如何以及何时为特定任务调用它们此外，通过生成代码，可以将一系列更细粒度的工具抽象到函数库中进行调用，而不是直接将子任务所需的参数传递给导航和抓取模型优化这些工具并且可能需要新的工具来完成未知任务。DEPS 在零样本学习的前提下，赋予 LLM 不同的角色设置，使其在与环境互动的同时学习不同的技能。在后续的互动中，LLM 可以学习选择和组合这些技能来开发新的技能

This hierarchical planning paradigm allows agents to focus on high-level task planning and decision-making while dele­gating specific action execution to policy models, simplifying the development process. The modularity of the task planner and action planner enables independent development, testing, and optimization, enhancing system flexibility and maintain­ability. This method allows the agent to adapt to various tasks and environments by invoking different action planners and facilitating modifications without requiring significant changes to the agent's structure. However, invoking external policy models may introduce latency, potentially impacting response time and efficiency, especially in real-time tasks. The agent's performance is critically dependent on the quality of the policy models. If the policy models are ineffective, the overall performance of the agent will suffer.

> 这种分层规划范式允许代理(agents)专注于高级任务规划和决策，同时将具体的行动执行委托给策略模型，简化了开发过程。任务规划器和行动规划器的模块化实现了独立开发、测试和优化，提高了系统的灵活性和可维护性。这种方法允许代理(agents)通过调用不同的行动计划器和促进修改来适应各种任务和环境，而不需要对代理(agents)的结构进行重大更改。然而，调用外部策略模型可能会引入延迟，从而可能影响响应时间和效率，尤其是在实时任务中。代理(agents)的性能在很大程度上取决于策略模型的质量。如果策略模型无效，代理(agents)的整体性能将受到影响。

2. _Action utilizing VLA model:_ Different form previous approach that task planning and action execution are per­formed within the same system, this paradigm leverages the capabilities of embodied multimodal foundation models for planning and executing actions, reducing communication latency and improving system response speed and efficiency. In VLA models, the tight integration of perception, decision-making, and execution modules allows the system to handle complex tasks and adapt to changes in dynamic environments more efficiently. This integration also facilitates real-time feedback, enabling the agent to self-adjust strategies, thereby enhancing the robustness and adaptability of task execution [[3]](#bookmark27) [[303]](#bookmark308) [[30](#_bookmark309)4]. However, this paradigm is undoubtedly more complex and costly, particularly when dealing with intricate or long-term tasks. Additionally, a key issue is that an action planner, without an embodied world model, cannot simulate physical laws using only the internal knowledge of an LLM. This limitation hinders the agent to accurately and effectively complete various tasks in the physical world, preventing the seamless transfer from cyber space to physical world.

> 2. _利用 VLA 模型的行动_：与之前在同一系统中执行任务规划和行动的方法不同，这种范式利用了多模态基础模型的功能来规划和执行行动，减少了通信延迟，提高了系统响应速度和效率。在 VLA 模型中，感知、决策和执行模块的紧密集成使系统能够更有效地处理复杂任务并适应动态环境的变化。这种集成还促进了实时反馈，使代理(agents)能够自我调整策略，从而增强任务执行的鲁棒性和适应性。然而，这种范式无疑更加复杂和昂贵，特别是在处理复杂或长期任务时。此外，一个关键问题是，没有具体世界模型的行动规划者无法仅使用 LLM 的内部知识来模拟物理定律。这种限制阻碍了代理(agents)准确有效地完成物理世界中的各种任务，阻碍了从网络空间到物理世界的无缝转换。

> Fig. 15. []{#bookmark19 .anchor}Embodied world models can be roughly divided into three type.

(a) **Generation-based Methods** train to learn the transformation relationship between the input space and the output space using an autoencoder framework.
(b) **Prediction-based Methods** can be seen as a more general framework where a world model is trained in latent space.
(c) **Knowledge-driven Methods** inject artificially constructed knowledge into the model, giving the model world knowledge to obtain output that meets the given knowledge constraints. Note that the components within the dashed line are optional.

> (a) **基于生成的方法**使用自动编码器框架训练学习输入空间和输出空间之间的变换关系。
> (b) **基于预测的方法**可以被视为一个更通用的框架，其中世界模型在潜在空间中训练。
> (c) **知识驱动方法**将人工构建的知识注入模型，赋予模型世界知识，以获得满足给定知识约束的输出。请注意，虚线内的组件是可选的。

## VII. SIM-TO-REAL ADAPTATION

Sim-to-Real adaptation in embodied AI refers to the process of transferring capabilities or behaviors learned in simulated environments (cyber space) to real-world scenarios (physical world). It involves validating and improving the effectiveness of algorithms, models, and control strategies developed in sim­ulation to ensure they perform robustly and reliably in physical environments. To achieve sim-to-real adaptation, embodied world models, data collection and training methods, and embodied control algorithms are three essential components.

> 具身人工智能中的模拟到真实适应是指将在模拟环境(网络空间)中学习到的能力或行为转移到现实世界场景(物理世界)的过程。它涉及验证和改进模拟中开发的算法、模型和控制策略的有效性，以确保它们在物理环境中稳健可靠地运行。为了实现模拟到真实的适应，体现世界模型、数据收集和训练方法以及体现控制算法是三个基本组成部分。

A. []{#bookmark20 .anchor}_Embodied World Model_

Sim-to-Real involves creating world models in simulation that closely resemble real-world environments, helping algo­rithms generalize better when transferred. The world model approach aims to build an end-to-end model that maps vision to action, or even anything to anything, by predicting the next state in a generative or predictive manner to make decisions. The biggest difference between such world models and VLA models is that VLA models are first trained on large-scale internet datasets to achieve high-level emergent capabilities and then co-finetuned with real world robot data. In contrast, world models are trained from scratch on physical world data, gradually developing high-level capabilities as the amount of data increases. However, they remain low-level physical world models, somewhat akin to the mechanism of human neural reflex systems. This makes them more suitable for scenarios where both inputs and outputs are relatively structured, such as autonomous driving (input: vision, output: throttle, brake, steering wheel) or object sorting (input: vision, instructions, numerical sensors, output: grasping the target object and placing it in the target location). They are less suited for generalization to unstructured, complex embodied tasks.

> Sim-to-Real 涉及在模拟中创建与现实世界环境非常相似的世界模型，帮助算法在传输时更好地泛化。世界模型方法旨在构建一个端到端的模型，通过以生成或预测的方式预测下一个状态来做出决策，将愿景映射到行动，甚至将任何东西映射到任何东西。这种世界模型和 VLA 模型之间最大的区别在于，VLA 模型首先在大规模互联网数据集上进行训练，以实现高级应急能力，然后与现实世界的机器人数据进行微调。相比之下，世界模型是在物理世界数据上从头开始训练的，随着数据量的增加，逐渐发展出高级功能。然而，它们仍然是低级的物理世界模型，有点类似于人类神经反射系统的机制。这使得它们更适合输入和输出都相对结构化的场景，例如自动驾驶(输入：视觉，输出：油门、刹车、方向盘)或物体分类(输入：视力、指令、数字传感器，输出：抓取目标物体并将其放置在目标位置)。它们不太适合泛化到非结构化、复杂的具体任务。

Learning world models is promising of the physical simu­lation field. Compared to traditional simulation methods, it offers significant advantages, such as the ability to reason about interactions with incomplete information, meet real-time computation requirements, and improve prediction accuracy over time. The predictive capability of such world models is crucial, enabling robots to develop the physical intuition necessary to operate in the human world. As shown in Fig. [15,](#_bookmark19) according to the learning pipeline of the world environ­ment, they can be divided into Generation-based methods,

> 学习世界模型在物理模拟领域很有前景。与传统的仿真方法相比，它具有显著的优势，例如能够推理不完整信息的交互，满足实时计算要求，并随着时间的推移提高预测精度。这种世界模型的预测能力至关重要，使机器人能够发展在人类世界中操作所需的物理直觉。如图[15](#_bookmark19)所示，根据世界环境的学习管道，它们可以分为基于生成的方法，

Prediction-based methods and Knowledge-driven methods. We briefly summarize the methods mentioned in Table [XI.](#_bookmark21)

> 基于预测的方法和知识驱动的方法。我们简要总结了表[neneneba XI]中提到的方法

1. _Generation-based Methods:_ As the scale of models and data progressively increases, generative models have demon­strated the ability to understand and generate images (e.g., World Models [[338]),](#_bookmark354) videos (e.g., Sora [[17]](#bookmark40) Pandora [[339]),](#_bookmark355) point clouds (e.g., 3D-VLA [[340])](#_bookmark357) or other formats of data (e.g., DWM [[341])](#_bookmark359) that conform to physical laws. This ability indicates that generative models can learn and internalize world knowledge. Specifically, after being exposed to vast amounts of data, generative models can not only capture the statistical properties of the data but also simulate the physical and causal relations of the real world through their intrinsic structures and mechanisms. Therefore, these generative models can be considered more than simple pattern recognition tools: they exhibit characteristics of world models. Consequently, the world knowledge embedded in generative models can be lever­aged to enhance the performance of other models. By mining and utilizing the world knowledge represented in generative models, we can improve model generalization and robustness. This approach not only enhances the model's adaptability to new environments but also increases its predictive accuracy on unknown data [[339]](#bookmark355) [[340]](#bookmark357) However, generative models also have certain limitations and drawbacks. For instance, when data distribution is significantly biased or training data is insufficient, generative models may produce inaccurate or distorted outputs. Additionally, the training process for these models typically requires substantial computational resources and time, and the models often lack interpretability, which complicates their practical application. Overall, while gen­erative models have shown great potential in understanding and generating content that conforms to physical laws, several technical and practical challenges must be addressed for their effective application. These challenges include improving model efficiency, enhancing interpretability, and addressing issues related to data bias. With ongoing research and devel­opment, generative models are expected to demonstrate even greater value and potential in future applications.

> 1. _基于生成的方法_：随着模型和数据规模的逐渐增加，生成模型已经证明了理解和生成图像(例如，世界模型视频(例如，SoraPandora 点云(例如，3D-VLA 或符合物理定律的其他格式数据(例如，DWM。然而，生成模型也有一定的局限性和缺点。例如，当数据分布存在明显偏差或训练数据不足时，生成模型可能会产生不准确或失真的输出。此外，这些模型的训练过程通常需要大量的计算资源和时间，而且这些模型往往缺乏可解释性，这使它们的实际应用变得复杂。总体而言，虽然生成模型在理解和生成符合物理定律的内容方面显示出巨大的潜力，但要有效应用它们，必须解决几个技术和实践挑战。这些挑战包括提高模型效率、增强可解释性以及解决与数据偏差相关的问题。随着不断的研究和开发，生成模型有望在未来的应用中显示出更大的价值和潜力。

2.  _Prediction-based Methods:_ The prediction-based world model predicts and understands the environment by construct­ing and utilizing internal representations. By reconstructing corresponding features in the latent space based on provided conditions, it captures deeper semantics and associated world knowledge. This model maps input information to a latent space and operates within that space to extract and utilize high-level semantic information, thereby enabling the robots to perceive the essential representation of the world environment (e.g., I-JEPA [[16]](#bookmark39) MC-JEPA [[342]](#bookmark361) A-JEPA [[343]](#bookmark362) Point-JEPA [[354]](#bookmark337), IWM [[344])](#_bookmark363) and more accurately perform embodied downstream tasks (e.g., iVideoGPT [[345]](#bookmark365) IRASim [[346]),](#_bookmark367) STP [[347]](#bookmark368) MuDreamer [[348]](#bookmark370) Compared to pixel-level infor­mation, latent feature can abstract and decouple various forms of knowledge, allowing the model to handle complex tasks and scenes more effectively and improve its generalization capability [[355]](#bookmark339) For instance, in spatiotemporal modeling, the world model needs to predict the post-interaction state of an object based on its current state and the nature of the interaction, combining this information with its internal knowledge. Specifically, an embodied world model generates dynamic predictions of the environment by integrating percep­tual information and prior knowledge. This approach relies not only on sensory data but also on inherent world knowledge to infer and predict environmental changes, thereby producing more accurate spatiotemporal predictions [[345]](#bookmark365) [[347]](#bookmark368) [[348]](#bookmark370) This process considers both the current state of objects and their historical data and contextual information.

> 2. _基于预测的方法_：基于预测的世界模型通过构建和利用内部表征来预测和理解环境。它根据提供的条件在潜在空间中重建相应的特征，从而捕捉到更深层的语义和相关的世界知识。该模型将输入信息映射到潜在空间，并在该空间内提取和利用高级语义信息，从而使机器人能够感知世界环境的基本表征(例如，I-JEPA[[16]])、I-JEPA [[16]]MC-JEPA[[342]]A-JEPA[[343]]Point-JEPA[[354]]，IWM[[344])]，并更准确地执行嵌入式下游任务(如iVideoGPT [[345]] IRASim [[346]] STP [[347]] MuDreamer [[348]] 与像素级信息相比，潜在特征可以抽象和解耦各种形式的知识、例如，在时空建模中，世界模型需要根据对象的当前状态和交互性质来预测其交互后的状态，并将这些信息与对象的内部信息结合起来。具体来说，具身世界模型通过整合感知信息和先验知识来生成环境的动态预测。这种方法**不仅依赖于感官数据，还依赖于固有的世界知识来推断和预测环境变化，从而产生更准确的时空预测。这一过程考虑了对象的当前状态及其历史数据和上下文信息。**

```
[]{#bookmark21 .anchor}TABLE XI
[Summary of the embodied world methods discussed in [VII-A](#_bookmark20).]{.smallcaps}
```

**Real-World Data Simulated Data Data Format**

> Fig. 16. []{#bookmark22 .anchor}The illustration of demonstration data collection. The yellow box on the left features operational demonstrations of the Franka and WidowX robotic arms, while human demonstrations are shown in the blue box. In the middle, the yellow box showcases operational scenarios of the UR5e and Franka robotic arms in simulation environments, and the blue box displays labeled simulation data. The purple box on the right presents the data formats of these datasets.

Similarly, leveraging the world knowledge embedded in its representations can further enhance the model's perception and robustness [[16]](#bookmark39) [[342]](#bookmark361) [[344]](#bookmark363) [[356]](#bookmark341) By operating in latent space, it is expected that robots can maintain high performance in different environments at a lower cost [[348]](#bookmark370) The key to this approach lies in abstract processing and knowl­edge decoupling, enabling efficient adaptation to complex situations. However, such models may exhibit limitations and instability when dealing with previously unseen environments and conditions. Additionally, the world knowledge decoupled in the latent space may have interpretability issues.

> 同样，利用嵌入其表示中的世界知识可以进一步增强模型的感知和鲁棒性这种方法的关键在于抽象处理和知识解耦，从而能够有效地适应复杂的情况。然而，在处理以前看不到的环境和条件时，这些模型可能会表现出局限性和不稳定性。此外，潜在空间中解耦的世界知识可能存在可解释性问题。

3. _Knowledge-driven Methods:_ Knowledge-driven world models inject artificially constructed knowledge into the mod­els, endowing them with world knowledge. This method has shown broad application potential in the field of embodied AI. For example, in the real2sim2real approach [[357]](#bookmark342) real-world knowledge is used to build physics-compliant simulators, which are then used to train robots, enhancing model robustness and generalization capabilities. Additionally, artificially constructing common sense or physics-compliant knowledge and applying them to generative models or simulators is a common strategy (e.g., ElastoGen [[350]](#bookmark330) One-2-3-45 [[351]](#bookmark331) PLoT [[349]](#bookmark372) This approach imposes more physically accurate constraints on the model, enhancing its reliability and inter­pretability in generation tasks. These constraints ensure the model's knowledge is both accurate and consistent, reducing uncertainty during training and application. Some approaches combine artificially created physical rules with LLMs or MLMs. By leveraging the commonsense capabilities of LLMs and MLMs, these approaches (e.g., Holodeck [[71]](#bookmark93) LEGENT [[352]](#bookmark333) GRUtopia [[353])](#_bookmark335) generate diverse and semantically rich scenes through automatic spatial layout optimization. This greatly advances the development of general-purpose embod­ied agents by training them in novel and diverse environments.

> 3. 知识驱动的方法：知识驱动的世界模型将人工构建的知识注入模型，赋予模型世界知识。该方法在嵌入式人工智能领域显示出广泛的应用潜力。例如，在 real2sim2real 方法中，真实世界的知识被用来构建符合物理的模拟器，然后用于训练机器人，增强模型的鲁棒性和泛化能力。此外，人工构建常识或物理兼容知识并将其应用于生成模型或模拟器是一种常见的策略(例如，ElastoGenOne-2-3-45PLoT 这种方法对模型施加了更精确的物理约束，提高了其在生成任务中的可靠性和可解释性。这些约束确保了模型的知识既准确又一致，从而减少了训练和应用过程中的不确定性。一些方法将人工创建的物理规则与 LLM 或 MLM 相结合。通过利用 LLM 和 MLM 的常识性功能，这些方法(例如，HolodeckLEGENTGRUtopia 通过自动空间布局优化生成多样化和语义丰富的场景。通过在新颖多样的环境中对其进行训练，这极大地推动了通用压花代理(agents)的发展。

B. _Data Collection and Training_

For sim-to-real adaptation, the high-quality data is impor­tant. Traditional data collection methods involve expensive equipment, precise operations, and are time-consuming and labor-intensive, often lacking flexibility. Recently, some effi­cient and cost-effective methods have been proposed for high-quality demonstration data collection and training. This section will discuss various methods for data collection in both real-world and simulated environments. Fig. [16](#_bookmark22) presents demon­stration data from both real-world and simulated environments.

> 对于模拟到真实的适应，高质量的数据非常重要。传统的数据收集方法涉及昂贵的设备、精确的操作，耗时耗力，往往缺乏灵活性。最近，已经提出了一些高效且具有成本效益的方法，用于高质量的演示数据收集和培训。本节将讨论在真实世界和模拟环境中收集数据的各种方法。图[16]显示了来自真实世界和模拟环境的演示数据。

> Fig. 17. []{#bookmark23 .anchor}Five pipelines to achieve sim-to-real gap. **"Real2Sim2Real"** reduces the gap by reconstructing real scenes. **"TRANSIC"** compensates for the sim-to-real transfer gap through human-corrected interventions. **"Domain Randomization"** enhances model transfer adaptability by simulating environmental diversity. **"System Identification"** improves sim-to-real environment similarity, thereby mitigating the sim-real gap. **"Lang4Sim2Real"** uses natural language to bridge two domains, learning invariant image representations and reducing visual gaps.

1. _Real-World Data:_ Training large, high-capacity models on high-volume, rich datasets has demonstrated remarkable capabilities and significant success in effectively addressing downstream applications. For instance, LLMs such as Chat-GPT, GPT-4, and LLaMA have not only excelled in the field of NLP but have also provided excellent problem-solving capabilities for downstream tasks. Therefore, is it possible to train an embodied large model in the robotics field, which possesses strong generalization capabilities through training and can adapt to new scenarios and robotic tasks. This requires a large volume of embodied datasets to provide data for model training. Open X-Embodiment [[303]](#bookmark308) is an embodied dataset from 22 different robots, with 527 skills and 160,266 tasks. The collected data consists of authentic demonstration data from robots, obtained by recording the process of executing operations. This dataset primarily focuses on domestic and kitchen settings, involving items such as furniture, food, and tableware. The operations are mainly centered around pick-and-place tasks, with a small portion involving more complex maneuvers. The high-capacity model RT-X trained on this dataset demonstrates excellent transfer capabilities. UMI [[358]](#bookmark344) proposed a data collection and policy learning framework. They designed a handheld gripper and an elegant interface for data collection, enabling portable, low-cost, and information-rich data collection for challenging bimanual and dynamic demonstration data. By simply modifying the training data, the robot can achieve zero-shot generalizable, bimanual, and precise tasks. Mobile ALOHA [[359]](#bookmark347) is a low-cost full-body mobile manipulation system. It can be used to collect task data for bimanual operations under full-body mobility, such as frying shrimp and serving dishes. Training agents with data collected by this system and static ALOHA can improve the performance of mobile manipulation tasks. Such agents can serve as home assistants or work assistants. In human-agent collaboration [[360]](#bookmark349) humans and agents learn together during data collection, reducing human workload, accelerating data acquisition, and improving data quality. Specifically, in an embodied scenario, during data collection, humans provide initial action inputs. Subsequently, the agent refines these actions through iterative perturbation and denoising processes, gradually optimizing them to produce precise, high-quality operational demonstrations. This entire process can be sum­marized as follows: humans contribute intuition and diversity in operations, while agents handle optimization and stability, reducing reliance on operators, enabling execution of more complex tasks, and gathering higher-quality data.

> 1. _真实世界数据_：在高容量、丰富的数据集上训练大型、高容量模型，在有效解决下游应用方面表现出了显著的能力和显著的成功。例如，Chat GPT、GPT-4 和 LLaMA 等 LLM 不仅在 NLP 领域表现出色，而且为下游任务提供了出色的问题解决能力。因此，是否有可能在机器人领域训练一个具身大型模型，该模型通过训练具有很强的泛化能力，可以适应新的场景和机器人任务。这需要大量的隐含数据集来为模型训练提供数据。Open X-Implementation 是来自 22 个不同机器人的具体数据集，包含 527 项技能和 160266 项任务。收集的数据包括来自机器人的真实演示数据，这些数据是通过记录执行操作的过程获得的。该数据集主要关注家庭和厨房环境，涉及家具、食品和餐具等物品。这些行动主要集中在拾取和放置任务上，一小部分涉及更复杂的机动。在该数据集上训练的高容量模型 RT-X 表现出了出色的传输能力。UMI 提出了一个数据收集和政策学习框架。他们设计了一个手持抓取器和一个优雅的数据收集界面，实现了便携式、低成本和信息丰富的数据收集，用于挑战双手和动态演示数据。通过简单地修改训练数据，机器人可以实现零样本可推广、双手动和精确的任务。移动 ALOHA 是一种低成本的全身移动操作系统。它可用于收集全身活动下双手操作的任务数据，如炸虾和上菜。使用该系统收集的数据和静态 ALOHA 训练代理(agents)可以提高移动操作任务的性能。这些代理(agents)人可以担任家庭助理或工作助理。在人类-代理(agents)协作中，人类和代理(agents)在数据收集过程中一起学习，减少了人类的工作量，加速了数据采集，提高了数据质量。具体而言，在具体场景中，在数据收集过程中，人类提供初始动作输入。随后，代理(agents)通过迭代扰动和去噪过程来细化这些动作，逐步优化它们，以产生精确、高质量的操作演示。整个过程可以总结如下：人类在操作中贡献直觉和多样性，而代理(agents)处理优化和稳定性，减少对操作员的依赖，执行更复杂的任务，并收集更高质量的数据。

2. _Simulated Data:_ The aforementioned data collection methods involve directly collecting demonstration data in the real world for agent training. Such collection methods often require significant manpower, material resources, and time, leading to inefficiency. Therefore, in most cases, researchers can choose to collect datasets in simulation environments for model training. Collecting data in simulation environments does not require extensive resources and can generally be auto­mated by programs, saving a lot of time. CLIPORT [[294]](#bookmark299) and Transporter Networks [[361]](#bookmark351) collected demonstration data from the Pybullet simulator for end-to-end network model training and successfully transferred the models from simulation to real world. GAPartNet [[362]](#bookmark353) constructed a large-scale part-centric interactive dataset GAPartNet, providing rich part-level anno­tations for perception and interaction tasks. They proposed a pipeline for domain-generalized 3D part segmentation and pose estimation, which can generalize well to unseen object categories in both simulators and the real world. SemGrasp [[289]](#bookmark294) built a large-scale grasping text-aligned dataset Cap-Grasp, which is a semantically rich dexterous hand grasping dataset from virtual environments.

> 2. _模拟数据_：上述数据收集方法涉及直接收集现实世界中的演示数据，用于代理(agents)训练。这种收集方法通常需要大量的人力、物力和时间，导致效率低下。因此，在大多数情况下，研究人员可以选择在模拟环境中收集数据集进行模型训练。在仿真环境中收集数据不需要大量资源，通常可以由程序自动完成，从而节省大量时间。CLIPORT 和 Transporter Networks 从 Pybullet 模拟器收集了端到端网络模型训练的演示数据，并成功地将模型从模拟转移到现实世界。GAPartNet 构建了一个大规模的以零件为中心的交互式数据集 GAPartNet，为感知和交互任务提供了丰富的零件级算法。他们提出了一种用于领域广义 3D 零件分割和姿态估计的管道，该管道可以很好地推广到模拟器和现实世界中看不见的物体类别。SemGrasp 构建了一个大规模的抓取文本对齐数据集 Cap Grasp，这是一个来自虚拟环境的语义丰富的灵巧手抓取数据集。

3. _Sim-to-Real Paradigms:_ Recently, several sim-to-real paradigms have been introduced, to mitigate the need for ex­tensive and costly real-world demonstration data by conduct­ing extensive learning in simulation environments, followed by migration to real-world settings. This section outlines five paradigms for sim-to-real transfer, as shown in Fig. [17.](#_bookmark23)

> 3. 模拟到现实范式：最近，引入了几种模拟到现实的范式，通过在模拟环境中进行广泛的学习，然后迁移到现实环境中，来减少对扩展和昂贵的现实世界演示数据的需求。本节概述了模拟到真实转换的五种范式，如图[17.]所示

Real2Sim2real [[363]](#bookmark356) enhanced imitation learning in real-world scenarios by leveraging reinforcement learning (RL) trained in a "digital twin" simulation environment. This method involves strengthening strategies through extensive RL within the simulation, followed by transferring these strategies to the real world to address data scarcity and achieve effective robotic operational imitation learning. Initially, NeRF and VR are used for scene scanning and reconstruction, and the constructed scene assets are imported into the simulator to achieve real-to-simulation fidelity. Subsequently, RL in the simulation fine-tunes the initial strategies derived from sparse expert demonstrations collected in the real world. Finally, the refined strategies are transferred to real-world settings.

> Real2Sim2real 通过利用在“数字孪生”仿真环境中训练的强化学习(RL)，增强了现实世界场景中的模仿学习。这种方法涉及通过模拟中的广泛强化学习来加强策略，然后将这些策略转移到现实世界中，以解决数据稀缺问题，实现有效的机器人操作模仿学习。最初，NeRF 和 VR 用于场景扫描和重建，并将构建的场景资产导入模拟器，以实现真实到模拟的保真度。随后，模拟中的 RL 对从现实世界中收集的稀疏专家演示中得出的初始策略进行了微调。最后，将改进后的策略转移到现实环境中。

TRANSIC [[364]](#bookmark358) narrowed the sim-to-real gap by enabling real-time human intervention to correct robot behaviors in real-world scenes. It enhances sim-to-real transfer performance through several steps: first, robots are trained using RL to establish foundational strategies within a simulation environ­ment. Then, these strategies are implemented on real robots, with humans intervening and correcting behaviors in real­time via remote control when errors occur. The data collected from these interventions are used to train a residual policy. Integrating both foundational and residual policies ensures smoother trajectories in real-world applications following sim-to-real transfer. This approach significantly reduces the need for real-world data collection, thereby mitigating the burden while achieving effective sim-to-real transfer.

> TRANSIC 通过实现实时人工干预来纠正现实场景中的机器人行为，从而缩小了模拟到真实的差距。它通过几个步骤提高了模拟到真实的传输性能：首先，使用 RL 训练机器人，在模拟环境中建立基础策略。然后，这些策略在真实的机器人上实施，当发生错误时，人类通过远程控制实时干预和纠正行为。从这些干预措施中收集的数据用于培训剩余政策。集成基础策略和残差策略可确保在模拟到真实的转换后，现实世界应用程序中的轨迹更加平稳。这种方法大大减少了对真实世界数据收集的需求，从而减轻了负担，同时实现了有效的模拟到真实的传输。

> Bipedal & Quadruped Robot Control
> Humanoid Robot Control
> Multi-Robot Control

Domain Randomization [[365]--[367]](#bookmark364) enhanced the gener­alization of models trained in simulated environments to real-world scenarios by introducing parameter randomization during simulation. While both simulated and real environments involve perception through camera-acquired visual images, differences such as object friction and gloss make accurate simulation challenging. Randomizing parameters during sim­ulation training covers a wide range of conditions, potentially encompassing variations that might occur in real-world set­tings. This approach boosts the robustness of trained models, enabling deployment from simulation to real environments.

> 域随机化通过在模拟过程中引入参数随机化，增强了在模拟环境中训练的模型到现实世界场景的通用性。虽然模拟和真实环境都涉及通过相机获取的视觉图像进行感知，但物体摩擦和光泽等差异使精确的模拟变得具有挑战性。在模拟训练期间随机化参数涵盖了广泛的条件，可能包括现实世界设置中可能发生的变化。这种方法增强了训练模型的鲁棒性，实现了从模拟到真实环境的部署。

System Identification [[368]](#bookmark366) [[369]](#bookmark369) constructed an accurate mathematical model of physical scenes in real-world environ­ments, encompassing parameters such as dynamics and visual rendering. It aims to make simulation environments closely resemble real-world settings, facilitating smooth transitions of models trained in simulation to real environments.

> 系统识别构建了一个真实世界环境中物理场景的精确数学模型，包括动力学和视觉渲染等参数。它的目的是使模拟环境与真实世界的设置非常相似，促进在模拟中训练的模型平滑过渡到真实环境。

Lang4sim2real [[370]](#bookmark371) used natural language as a bridge to address the sim-to-real gap by using textual descriptions of images as a cross-domain unified signal. This approach aids in learning domain-invariant image representations, thereby improving generalization performance across simulation and real environments. Initially, an encoder is pretrained on image data annotated with cross-domain language descriptions. Sub­sequently, using the domain-invariant representations, a multi-domain, multi-task language-conditioned behavioral cloning policy is trained. This method compensates for the scarcity of real-world data by additional information from abundant simulated data, thereby enhancing sim-to-real transfer.

> Lang4sim2real 使用自然语言作为桥梁，通过使用图像的文本描述作为跨域统一信号来解决 sim 到 real 的差距。这种方法有助于学习领域不变的图像表示，从而提高了模拟和真实环境中的泛化性能。最初，编码器在用跨域语言描述注释的图像数据上进行预训练。随后，使用域不变表示，训练了一个多域、多任务语言条件的行为克隆策略。该方法通过从大量模拟数据中获取额外信息来补偿现实世界数据的稀缺性，从而增强模拟到真实的传输。

C. _Embodied Control_

Embodied control learns through interaction with the envi­ronment and optimizes behavior using a reward mechanism to obtain the optimal policy, thereby avoiding the drawbacks of traditional physical modeling methods. Embodied control methods can be divided into two types:

> 体现控制通过与环境的交互进行学习，并使用奖励机制优化行为以获得最优策略，从而避免了传统物理建模方法的缺点。具体的控制方法可分为两类：

1.  Deep Reinforcement Learning (DRL). DRL can han­dle high-dimensional data and learn complex behavior pat­terns, making it suitable for decision-making and control. The hybrid and dynamic policy gradient (HDPG) [[371]](#bookmark373) is proposed for biped locomotion, allowing the control policy to be simultaneously optimized by multiple criteria dynamically. DeepGait [[372]](#bookmark374) is a neural network policies for terrain-aware locomotion, which combines methods for model-based motion planning and reinforcement learning. It includes a terrain-aware planner for generating gait sequences and base motions guiding the robot towards target directions, along with a gait and base motion controller for executing these sequences while maintaining balance. Both the planner and controller are parameterized using neural network function approximators and optimized using deep reinforcement learning algorithms.

> 1. 深度强化学习(DRL)。DRL 可以处理高维数据并学习复杂的行为模式，使其适用于决策和控制。提出了用于双足运动的混合和动态策略梯度(HDPG)，允许通过多个标准动态地同时优化控制策略。DeepGait 是一种用于地形感知运动的神经网络策略，它结合了基于模型的运动规划和强化学习方法。它包括一个地形感知规划器，用于生成步态序列和引导机器人朝向目标方向的基础运动，以及一个步态和基础运动控制器，用于在保持平衡的同时执行这些序列。规划器和控制器都使用神经网络函数逼近器进行参数化，并使用深度强化学习算法进行优化。

> Fig. 18. []{#bookmark24 .anchor}Examples of embodied control for various locomotion modes, demonstrating the robots' agile movement and interaction capabilities. The orange box on the left showcases the locomotion of quadruped robots (Unitree B1, ANYmal, Spot, Ghost Robotics Vision-60, MIT Cheetah) and one biped robot (ATRIAS). The red box on the middle displays motion scenes of the Atlas Robot climbing stairs, TELESAR V robot writing, Unitree H1 high-fiving, and the Boston robot leaping. The green box shows multi-robot collaborative control, finding path and planning path.

2. Imitation Learning. The DRL has its drawbacks of requiring a large amount of data from numerous trials. To address this issue, imitation learning was introduced, which aims to minimize data usage by collecting high-quality demon­strations. To improve data efficiency, Offline RL + Online RL was proposed to reduce interaction costs and ensure safety. This method first employs offline RL to learn policies from static, pre-collected large datasets. These policies are then deployed in the real environment for real-time interaction and exploration, with adjustments made based on feedback. The representative imitation learning methods from human demonstrations are ALOHA [[373]](#bookmark375) and Mobile ALOHA [[359]](#bookmark347)

> 2. 模仿学习。日间行车灯有其缺点，即需要大量试验数据。为了解决这个问题，引入了模仿学习，旨在通过收集高质量的演示来最大限度地减少数据使用。为了提高数据效率，离线 RL+在线 RL 被提出以降低交互成本并确保安全。该方法首先采用离线 RL 从静态、预先收集的大型数据集中学习策略。然后将这些策略部署在真实环境中进行实时交互和探索，并根据反馈进行调整。来自人类演示的代表性模仿学习方法是 ALOHA 和移动 ALOHA

Although embodied AI encompasses high-level algorithms, models, and planning modules, its most fundamental and essential component is embodied control. Therefore, it is imperative to consider how to control physical entities and endow them with physical intelligence. Embodied control is closely related to hardware, such as controlling joint move­ments, end-effector positions, and walking speeds. For robotic arms, knowing the end-effector's position, how to plan joint trajectories to move the arm to the target? For humanoid robots, knowing the motion patterns, how to control the joints to achieve the target posture? These are critical issues that need to be addressed in control. Several works focus on robotic control, enhancing the flexibility of robotic actions. [[374]](#bookmark376) proposed a vision-based full-body control framework. By con­necting a robotic arm and a robotic dog, utilizing all degrees of freedom (12 joints in legs, 6 joints in the arm, and 1 in the gripper), it tracks the robot dog's speed and the robotic arm's end-effector position, achieving more flexible control. Some works [[375]](#bookmark377) [[376]](#bookmark378) employed traditional methods to control bipedal robot walking. MIT's Cheetah 3 [[377]](#bookmark379) ANYmal [[378]](#bookmark380) and Atlas [[379]](#bookmark381) used robust walking controllers to manage the robots. These robots can be used for more agile motion tasks, such as jumping or overcoming various obstacles [[380]--[384]](#bookmark383) Other works [[385]](#bookmark384) [[386]](#bookmark385) focused on the control of humanoid robots, to perform various actions like humans and mimic human behaviors. Fig [.18](#_bookmark24) illustrates some examples.

> 尽管嵌入式 AI 包含高级算法、模型和规划模块，但其最基本和最重要的组成部分是嵌入式控制。因此，必须考虑如何控制物理实体并赋予它们物理智能。实体控制与硬件密切相关，例如控制关节运动、末端执行器位置和行走速度。对于机器人手臂，知道末端执行器的位置，如何规划关节轨迹以将手臂移动到目标？对于类人机器人来说，知道运动模式，如何控制关节达到目标姿势？这些都是需要控制的关键问题。几项工作侧重于机器人控制，提高了机器人动作的灵活性。提出了一种基于视觉的全身控制框架。通过将机器人手臂和机器狗连接起来，利用所有自由度(腿上 12 个关节，手臂上 6 个关节，抓握器上 1 个关节)，它可以跟踪机器狗的速度和机器手臂的末端执行器位置，实现更灵活的控制。一些作品采用传统方法控制双足机器人行走。麻省理工学院的猎豹 3ANYmal 和阿特拉斯使用强大的行走控制器来管理机器人。这些机器人可用于更敏捷的运动任务，例如跳跃或克服各种障碍其他作品专注于控制类人机器人，执行类似人类的各种动作并模仿人类行为。图[18]显示了一些示例。

Embodied control integrates RL and sim-to-real techniques to optimize strategies through environmental interaction, en­abling the exploration of unknown domains, potentially sur­passing human capabilities, and adapting to unstructured en­vironments. While robots can mimic many human behaviors, effective task completion often requires RL training based on environmental feedback. The most challenging scenarios in­clude contact-intensive tasks, where manipulation necessitates real-time adjustments based on feedback such as the state, deformation, material, and force of manipulated objects. In such cases, RL is indispensable. In the era of MLMs, these models possess a generalized understanding of scene seman­tics, providing robust reward functions for RL. Additionally, RL is crucial for aligning large models with their intended tasks. In the future, after pre-training and fine-tuning, RL is still required to align with the physical world, ensuring effective deployment in real-world environments.

> 体现控制将 RL 和 sim 集成到真实技术中，通过环境交互优化策略，能够探索未知领域，潜在地超越人类能力，并适应非结构化环境。虽然机器人可以模仿许多人类行为，但有效的任务完成通常需要基于环境反馈的强化学习训练。最具挑战性的场景包括接触密集型任务，在这些任务中，操纵需要根据被操纵物体的状态、变形、材料和力等反馈进行实时调整。在这种情况下，RL 是必不可少的。在 MLM 时代，这些模型对场景语义有着普遍的理解，为 RL 提供了强大的奖励函数。此外，RL 对于将大型模型与其预期任务对齐至关重要。在未来，经过预训练和微调后，RL 仍然需要与物理世界保持一致，确保在现实世界环境中的有效部署。

## VIII. CHALLENGES AND FUTURE DIRECTIONS

Despite of the rapid progress of embodied AI, it faces several challenges and presents exciting future directions.

> 尽管具身人工智能发展迅速，但它面临着一些挑战，并提出了令人兴奋的未来方向。

**High-quality Robotic Datasets**: Obtaining sufficient real world robotic data remains a significant challenge. Collecting this data is both time-consuming and resource-intensive. Rely­ing solely on simulation data worsens the sim-to-real gap prob­lem. Creating diverse real world robotic datasets necessitates close and extensive collaboration among various institutions. Additionally, the development of more realistic and efficient simulators is essential for improving the quality of simulated data. Current work RT-1 [[11]](#bookmark34) used pre-trained models based on robot images and natural language commands. RT-1 has achieved good results in navigation and grasping tasks, but acquiring real world robot datasets is very challenging. For building generalizable embodied models capable of cross-scenario and cross-task applications in robotics, it is essen­tial to construct large-scale datasets, leveraging high-quality simulated environment data to assist real world data.

> **高质量的机器人数据集**：获取足够的现实世界机器人数据仍然是一个重大挑战。收集这些数据既费时又耗费资源。仅依赖模拟数据会加剧模拟到实际差距的问题。创建多样化的现实世界机器人数据集需要各机构之间密切而广泛的合作。此外，开发更逼真、更高效的模拟器对于提高模拟数据的质量至关重要。当前的工作 RT-1 使用了基于机器人图像和自然语言命令的预训练模型。RT-1 在导航和抓取任务中取得了良好的效果，但获取现实世界的机器人数据集非常具有挑战性。为了构建能够在机器人技术中跨场景和跨任务应用的通用化实体模型，构建大规模数据集至关重要，利用高质量的模拟环境数据来辅助现实世界的数据。

**Efficient Utilization of Human Demonstration Data**: Efficient utilization of human demonstration data involves leveraging the actions and behaviors demonstrated by humans to train and improve robotic systems. This process includes collecting, processing, and learning from large-scale, high-quality datasets where humans perform tasks that robots are intended to learn. Current work R3M [[387]](#bookmark386) used action labels and human demonstration data to learn generalizable represen­tations with high success rates, but the efficiency for complex tasks still needs improvement. Therefore, it is important to effectively utilize large amounts of unstructured, multi-label, and multi-modal human demonstration data combined with ac­tion label data to train embodied models that can learn various tasks in relatively short periods. By efficiently utilizing human demonstration data, robotic systems can achieve higher levels of performance and adaptability, making them more capable of performing complex tasks in dynamic environments.

> **高效利用人类演示数据**：高效利用人类展示数据涉及利用人类演示的动作和行为来训练和改进机器人系统。这个过程包括从大规模、高质量的数据集中收集、处理和学习，在这些数据集中，人类执行机器人想要学习的任务。当前的工作 R3M 使用动作标签和人类演示数据来学习可概括的表示，成功率很高，但复杂任务的效率仍有待提高。因此，重要的是有效地利用大量非结构化、多标签和多模式的人类演示数据，结合交流标签数据来训练能够在相对较短的时间内学习各种任务的实体模型。通过有效地利用人类演示数据，机器人系统可以实现更高水平的性能和适应性，使其能够在动态环境中执行复杂的任务。

**Cognition of Complex Environment**: Cognition of com­plex environment refers to the ability of embodied agents in physical or virtual environments, to perceive, understand, and navigate complex real world environments. Based on extensive commonsense knowledge, the Say-Can [[299]](#bookmark304) utilized pre­trained LLM models' task decomposition mechanism, which relies heavily on large amounts of commonsense knowledge for simple task planning but lacking understanding of long­term tasks in complex environments. For unstructured open environments, current works usually rely on pre-trained LLMs' task decomposition mechanism using extensive common-sense knowledge for simple task planning, while lacking specific scene understanding. It is vital to enhance the ability of knowl­edge transfer and generalization in complex environments. A truly versatile robotics system should be capable of compre­hending and executing natural language instructions across diverse and unseen scenes. This necessitates the development of adaptable and scalable embodied agent architectures.

> **复杂环境认知**：复杂环境认知是指物理或虚拟环境中具身主体感知、理解和导航复杂现实世界环境的能力。基于广泛的常识知识，Say-Can 利用了预训练的 LLM 模型的任务分解机制，该机制严重依赖大量常识知识进行简单的任务规划，但缺乏对复杂环境中长期任务的理解。对于非结构化的开放环境，目前的工作通常依赖于预训练的 LLM 的任务分解机制，使用广泛的常识知识进行简单的任务规划，同时缺乏对特定场景的理解。提高复杂环境中知识转移和泛化的能力至关重要。一个真正通用的机器人系统应该能够在各种看不见的场景中理解和执行自然语言指令。这需要开发适应性强、可扩展的嵌入式代理(agents)架构。

**Long-Horizon Task Execution**: Executing single instruc­tions can often entail long-horizon tasks for robots, exempli­fied by commands like "clean the kitchen", which involve activities such as rearranging objects, sweeping floors, wip­ing tables, and more. Accomplishing such tasks successfully necessitates the robot's ability to plan and execute a sequence of low-level actions over extended time spans. While current high-level task planners have shown initial success, they often prove inadequate in diverse scenarios due to their lack of tuning for embodied tasks. Addressing this challenge requires the development of efficient planners equipped with robust perception capabilities and much commonsense knowledge.

> **长期任务执行**：执行单个指令通常需要机器人执行长期任务，例如“清洁厨房”等命令，其中涉及重新排列物体、扫地、擦桌子等活动。成功完成这些任务需要机器人能够在较长的时间跨度内计划和执行一系列低级动作。虽然目前的高级任务规划人员已经取得了初步的成功，但由于缺乏对具体任务的调整，他们在各种情况下往往被证明是不够的。应对这一挑战需要培养具备强大感知能力和大量常识知识的高效规划者。

**Causal Relation Discovery**: Existing data-driven embodied agents make decisions based on the intrinsic correlations within the data. However, this modeling approach does not allow the models to truly understand the causal relations between knowledge, behavior, and environment, resulting in biased strategies. This makes it difficult to ensure that they can operate in real-world environments in an interpretable, robust, and reliable manner. Therefore, it is important for embodied agents to be driven by world knowledge, capable of autonomous causal reasoning. By understanding the world through interaction and learning its workings via abductive reasoning, we can further enhance the adaptability, decision reliability, and generalization capabilities of multimodal em­bodied agents in complex real-world environments. For em­bodied tasks, it is necessary to establish spatial-temporal causal relations across modalities through interactive instructions and state predictions [[388]](#bookmark387) Moreover, agents need to understand the affordances of objects to achieve adaptive task planning and long-distance autonomous navigation in dynamic scenes. To optimize decision-making, it is necessary to combine counterfactual and causal intervention strategies [[389]](#bookmark388) trace causality from counterfactual and causal intervention perspec­tives, reduce exploration iterations, and optimize decisions. Constructing a causal graph based on world knowledge and driving sim-to-real transfer of agents through active causal reasoning will form a unified framework for embodied AI.

> **因果关系发现**：现有的数据驱动实体代理(agents)根据数据中的内在相关性做出决策。然而，这种建模方法不允许模型真正理解知识、行为和环境之间的因果关系，从而导致有偏见的策略。这使得很难确保它们能够以可解释、稳健和可靠的方式在现实环境中运行。因此，具身代理(agents)由世界知识驱动，能够自主进行因果推理，这一点很重要。通过交互理解世界，并通过溯因推理学习其工作原理，我们可以进一步提高多模态实体代理(agents)在复杂现实环境中的适应性、决策可靠性和泛化能力。对于实体任务，有必要通过交互式指令和状态预测建立跨模态的时空因果关系。此外，代理(agents)需要了解对象的启示，以实现动态场景中的自适应任务规划和远程自主导航。为了优化决策，有必要结合反事实和因果干预策略从反事实和原因干预的角度追踪因果关系，减少探索迭代，优化决策。基于世界知识构建因果图，通过主动因果推理驱动智能体从模拟到真实的转移，将形成一个统一的具身人工智能框架。

**Continual Learning**: In robotics applications, continual learning [[390]](#bookmark389) is crucial for deploying robot learning policies in diverse environments, yet it remains a largely unexplored domain. While some recent studies have examined sub-topics of continual learning---such as incremental learning, rapid motor adaptation, and human-in-the-loop learning---these so­lutions are often designed for a single task or platform and do not yet consider foundational models. Open research problems and viable approaches include: 1) mixing different proportions of prior data distribution when fine-tuning on the latest data to alleviate catastrophic forgetting [[391]](#bookmark390) 2) developing efficient prototypes from prior distributions or curricula for task infer­ence in learning new tasks, 3) improving training stability and sample efficiency of online learning algorithms, 4) identifying principled ways to seamlessly incorporate large-capacity mod­els into control frameworks, potentially through hierarchical learning or slow-fast control, for real-time inference.

> **持续学习**：在机器人应用中，持续学习对于在不同环境中部署机器人学习策略至关重要，但它仍然是一个很大程度上未被探索的领域。虽然最近的一些研究考察了持续学习的子主题，如增量学习、快速运动适应和人类在环学习，但这些解决方案通常是为单一任务或平台设计的，还没有考虑基础模型。开放研究问题和可行的方法包括：1)在微调最新数据时混合不同比例的先前数据分布，以减轻灾难性遗忘 2)从先前分布或课程中开发高效的原型，用于学习新任务时的任务推断，3)提高在线学习算法的训练稳定性和样本效率，4)确定将大容量模型无缝整合到控制框架中的原则性方法，可能通过分层学习或慢速控制，实现实时推理。

**Unified Evaluation Benchmark**: While numerous bench­marks exist for evaluating low-level control policies, they often vary significantly in the skills they assess. Furthermore, the objects and scenes included in these benchmarks are typically limited by simulator constraints. To comprehensively evaluate embodied models, there is a need for benchmarks that encompass a diverse range of skills using realistic simulators. Regarding high-level task planners, many benchmarks focus on assessing planning capability through question-answering tasks. However, a more desirable approach involves evaluating both the high-level task planner and the low-level control policy together for executing long-horizon tasks and measuring success rates, rather than relying solely on isolated assessments of the planner. This integrated approach offers a more holistic assessment of the capabilities of embodied AI systems.

> **统一评估基准**：虽然存在许多用于评估低级控制策略的基准，但它们在评估的技能方面往往存在很大差异。此外，这些基准测试中包含的对象和场景通常受到模拟器约束的限制。为了全面评估实体模型，需要使用逼真的模拟器进行包含各种技能的基准测试。关于高级任务规划者，许多基准侧重于通过问答任务评估规划能力。然而，一种更可取的方法是同时评估高级任务规划器和低级控制策略，以执行长期任务并衡量成功率，而不是仅仅依赖于对规划器的孤立评估。这种综合方法对嵌入式人工智能系统的能力进行了更全面的评估。

## IX. CONCLUSION

Embodied AI allows agents to sense, perceive, and interact with various objects from both cyber space and physical world, which exhibits its vital significance toward achiev­ing AGI. This survey extensively reviews embodied robots, simulators, four representative embodied tasks: visual active perception, embodied interaction, embodied agents and sim-to-real robotic control, and future research directions. The comparative summary of the embodied robots, simulators, datasets, and approaches provides a clear picture of the recent development in embodied AI, which greatly benefits the future research along this emerging and promising research direction.

> 物化人工智能使智能体能够感知、感知和交互来自网络空间和物理世界的各种物体，这对实现 AGI 具有重要意义。本次调查广泛回顾了具身机器人、模拟器、四种具有代表性的具身任务：视觉主动感知、具身交互、具身代理(agents)和模拟现实机器人控制，以及未来的研究方向。对**具身机器人、模拟器、数据集**和方法的比较总结清晰地展示了具身人工智能的最新发展，这极大地有利于沿着这一新兴和有前景的研究方向进行未来的研究。
